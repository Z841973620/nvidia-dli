{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a86ebd5-58aa-40c5-8102-06f888503753",
   "metadata": {},
   "source": [
    "<center><img src=\"/files/images/DLI_Header.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e27fba5-6948-42c9-9ac3-bf8a67244619",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 迭代提示开发"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e13d039-9ec5-4617-912f-4bfc84f9324d",
   "metadata": {},
   "source": [
    "在这个 notebook 里，我们通过迭代一组简单的提示来热个身，让我们熟悉一下整个课程都在用的 `transformers` 工作流和 LLaMA-2 模型。\n",
    "\n",
    "通过对看似简单的提示进行迭代，我们将看到创建**具体**且包含**提示**的提示词的重要性，还将了解如何在面临有挑战性的任务时，为模型提供一点**“思考时间”**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83db67bb-a953-4210-a032-bf9f7957ebad",
   "metadata": {},
   "source": [
    "## 学习目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8646c89b-a172-4773-8b93-1a213df8d746",
   "metadata": {},
   "source": [
    "完成此 notebook 后，您将能够：\n",
    "* 使用 `transformers` 工作流从 LLaMA-2 LLM 获得响应。\n",
    "* 创建**具体**（specific）的提示词。\n",
    "* 创建为模型提供**“思考时间”**（time to think）的提示词。\n",
    "* 为模型提供**提示**以指导其响应。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c0fd90-b2c4-4432-a6f0-7d378325d0ed",
   "metadata": {},
   "source": [
    "## 视频教程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeab60a-a06a-43ff-8ddf-51d0d8f4a32f",
   "metadata": {},
   "source": [
    "执行以下单元以加载此 notebook 的视频教程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5f1c59-ec1f-44c3-8628-41a240f610fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-12-v1/v2/02-prompting.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e2324-6ed6-4658-910f-d15c68e26248",
   "metadata": {},
   "source": [
    "## 创建 LLaMA-2 工作流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f9aa8-b9d0-4c87-9b54-bcb9d59f9880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "model = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "\n",
    "llama_pipe = pipeline(\"text-generation\", model=model, device_map=\"auto\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc37a21-9dd5-46b0-a99a-243082a12981",
   "metadata": {},
   "source": [
    "## 辅助函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af26cb7-f326-4aec-91b8-128d87b8a7d3",
   "metadata": {},
   "source": [
    "在此 notebook 中，我们将使用以下函数来支持与 LLM 的交互。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b99117-4086-4737-ba05-f576ba8e9875",
   "metadata": {},
   "source": [
    "### 生成模型响应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca80c8-610c-4f46-bb11-65d1ad3dbc59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(prompt, max_length=1024, pipe=llama_pipe, **kwargs):\n",
    "    \"\"\"\n",
    "    Generates a response to the given prompt using a specified language model pipeline.\n",
    "\n",
    "    This function takes a prompt and passes it to a language model pipeline, such as LLaMA, \n",
    "    to generate a text response. The function is designed to allow customization of the \n",
    "    generation process through various parameters and keyword arguments.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The input text prompt to generate a response for.\n",
    "    - max_length (int): The maximum length of the generated response. Default is 1024 tokens.\n",
    "    - pipe (callable): The language model pipeline function used for generation. Default is llama_pipe.\n",
    "    - **kwargs: Additional keyword arguments that are passed to the pipeline function.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated text response from the model, trimmed of leading and trailing whitespace.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    prompt_text = \"Explain the theory of relativity.\"\n",
    "    response = generate(prompt_text, max_length=512, pipe=my_custom_pipeline, temperature=0.7)\n",
    "    print(response)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def_kwargs = dict(return_full_text=False, return_dict=False)\n",
    "    response = pipe(prompt.strip(), max_length=max_length, **kwargs, **def_kwargs)\n",
    "    return response[0]['generated_text'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2999e0dc-b67a-436d-a6df-ff86b8e5c0d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "加利福尼亚州首都\n",
    "--------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaf04e1-e084-4d05-a1a8-e51331f3c6eb",
   "metadata": {},
   "source": [
    "我们先从一个非常简单的提示词开始，我们会将其传递给 `generate` 函数，以便从 LLaMA-2 模型中获取响应。在这个提示词的迭代过程中，我们希望模型回复我们加利福尼亚州的首府 Sacramento。\n",
    "\n",
    "本实验中，我们希望模型仅回复 `\"Sacramento\"` 这个词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01702d62-b226-45b6-8e20-ed7f410b62e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of California?\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7c70c6-e7e8-46f0-8d28-5033460263a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbac8c9-319e-43f3-9859-b2f269bb50ff",
   "metadata": {},
   "source": [
    "模型不明白我们只想要首府城市的名称，而不需要任何其它上下文。这样的话，我们来写一个更**具体**的提示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69727a1-e364-4a17-93d1-721be30b63e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of California? Only answer this question and do so in as few a words as possible.\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855424dc-6d97-41c4-8347-d98aba83a787",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de56c7ff-2627-4fca-9c97-4177340358c5",
   "metadata": {},
   "source": [
    "有了点改进，但回复的开头仍然有个 `Answer:`。让我们尝试通过向模型提供**提示** `Answer:` 来规避模型的这种行为。这样做有可能就会让模型不再自己输出 `Answer`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4cbbf1-7b45-481f-8a2b-a4b6589aa800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of California? Only answer this question and do so in as few a words as possible. Answer: \"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b27326d-a109-4483-a820-73cd66c63fbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sacramento 的元音"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeec1c9-d7bc-4a56-8938-8292f2b50331",
   "metadata": {},
   "source": [
    "这节，我们尝试让模型做点更复杂的事：告诉我们加利福尼亚州首府名字中的所有元音。\n",
    "\n",
    "正确的答案是 S**a**cr**a**m**e**nt**o** -> **aaeo** -> **aeo**。为了让我自己（和您）更轻松的完成这个任务，我用了以下几个步骤来得出答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b97c2-b2a1-4251-adbc-864c271f2e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Tell me the vowels in the capital of California.\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e13825-1418-432b-b5ea-167111451243",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf59eea-2643-43c8-be7f-7a1bb5f658a8",
   "metadata": {},
   "source": [
    "当模型需要做多步骤推理时，让它执行多个中间步骤通常会很有帮助，就像要求模型展示其工作过程一样。这种技巧通常被称为给模型“思考时间”。\n",
    "\n",
    "下面的提示词旨在实现同样的效果，但要求模型先在中间步骤中回复加利福尼亚州首府是什么，然后再给出其中的元音。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91edcd68-c6ea-4dce-8f3a-1ae4a326ac56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Tell me the capital of California, and then tell me all the vowels in it.\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8682e8d0-fb74-4b1e-8046-7b1037e83cb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33833fe-bd3f-482f-a4d8-068b65ec4639",
   "metadata": {},
   "source": [
    "现在，我们看到了让模型有**“思考时间”**的效果，下面再次尝试一个稍复杂点的任务：按反向字母顺序给出加利福尼亚州首府的元音。\n",
    "\n",
    "正确答案是：S**a**cr**a**m**e**nt**o** -> **aaeo** -> **aeo** -> **oea**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3413ea-8224-4a30-8ab5-b89f31f7f254",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Tell me the vowels in the capital of California in reverse alphabetical order?\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e66050-4449-4c32-8bfa-ed0fa015c776",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7acd4f-1c53-47b5-961f-3f5fbf62311a",
   "metadata": {},
   "source": [
    "为了帮助模型，我们再次提示模型将任务分解为中间步骤，从而为其提供**“思考时间”**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f3756b-b976-43fb-b082-334bdecbadfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Tell me the capital of California, and then tell me all the vowels in it, then tell me the vowels in reverse-alphabetical order.\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74e501e-75d9-46e0-8fef-84a9cfd33166",
   "metadata": {},
   "source": [
    "## 练习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb4e198-d274-43cd-ba2c-620b8fbdd45b",
   "metadata": {},
   "source": [
    "虽然 LLM 不一定是进行数学计算的最佳工具，但作为一项练习，试试用下面的提示词计算 23 和 34 的乘积，然后迭代地开发提示词来获得正确答案。请务必考虑如何让提示词尽可能**具体**，同时为模型提供**“思考时间”**。\n",
    "\n",
    "如果遇到问题，可以看看下面的参考答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14541792-ee4e-44e9-8d13-a99c3a8d8d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "23*34 # Show the actual answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24769517-7106-44dd-9e9b-a286a5f15483",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"23x34\" # While you and I understand the intention of this prompt, to the model it is not at all **precise**\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897ea6f6-1259-4b1f-977c-e26cc2bdb489",
   "metadata": {},
   "source": [
    "### 您的代码\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a7ab61-dc0e-4ec2-b79d-7f7cc843dd63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65c1181a-21c7-41bd-acbd-817aa20ae510",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 参考答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02322095-a320-4a0a-81b7-b3a62bc6ff18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Calculate the product of 23 and 34. Use the steps typical of long multiplication and show your work.\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9253ae-933e-4bd0-8227-466b55526928",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 关键概念回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42769ed5-4b97-4475-8fd8-dddf6890e906",
   "metadata": {},
   "source": [
    "此 notebook 中介绍了这几个关键概念：\n",
    "* **具体**：尽可能明确的指导 LLM。\n",
    "* **提示**：用于指导响应，通常是为了防止在响应中包含提示本身。\n",
    "* **“思考时间”**：一种要求模型执行多步骤并展示其工作来提升 LLM 响应质量的方式（通常计算任务会涉及到）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e05bfc-8d95-4f64-8aca-32a9d5322bfc",
   "metadata": {
    "tags": []
   },
   "source": [
    "重启内核\n",
    "----\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726c6cc-27e7-4f70-98a0-8bee153d1b38",
   "metadata": {},
   "source": [
    "为下一个 notebook 释放 GPU 显存，请运行以下单元重启内核。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f288ecb-54dc-4f4e-8cc9-54230e5f5ab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
