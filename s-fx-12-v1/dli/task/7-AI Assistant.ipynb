{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29023c7c-e5ce-43b2-af76-b6432f5e16ec",
   "metadata": {},
   "source": [
    "<center><img src=\"/files/images/DLI_Header.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c220782-b82e-48ea-873b-92a5530db930",
   "metadata": {},
   "source": [
    "# Star Bikes AI 助手"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4713e8c2-05c4-4815-a01f-c22340e868d3",
   "metadata": {},
   "source": [
    "在此 notebook 中，您将创建一个 AI 助手帮助客户做出购买 Star Bikes 自行车的最佳决策。您还将了解到正在使用模型的 **token 限制**，及其对保留对话历史记录的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17942686-f99d-4077-be2e-8d40977391a8",
   "metadata": {},
   "source": [
    "## 学习目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c3ef1-24e2-4545-bfcf-1cf7871d7d95",
   "metadata": {},
   "source": [
    "完成此 notebook 后，您将能够：\n",
    "* 解释 **token 限制**及其对 LLM 行为的影响。\n",
    "* 构建一个具有（优先）对话记忆，不会超出 **token 限制**的 AI 助手。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049a06a3-0eca-43c9-811c-c49677cd7ac5",
   "metadata": {},
   "source": [
    "## 视频教程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94853247-1450-47a8-81f0-3ae7c46e0b20",
   "metadata": {},
   "source": [
    "执行以下单元以加载此 notebook 的视频教程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cc9eab-80fa-48e2-891f-dcebf4c0c458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-12-v1/v2/07-assistant.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f19484-acf2-4669-bbca-298c1c512bd7",
   "metadata": {},
   "source": [
    "## 创建 LLaMA-2 工作流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab62455a-b3bf-41eb-8499-b85dc053c31f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "model = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "# model = \"TheBloke/Llama-2-7B-chat-GPTQ\"\n",
    "\n",
    "llama_pipe = pipeline(\"text-generation\", model=model, device_map=\"auto\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3faff0-3616-4186-8436-6365d4ebcd55",
   "metadata": {},
   "source": [
    "## 获取 LLaMA-2 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe3f005-1da2-4ae5-bf39-832aedf33a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf1de0-1019-4f34-a1ba-a44943ea4b48",
   "metadata": {},
   "source": [
    "## 辅助函数和类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc39d8e-4667-45a9-a4c7-ae0bba32ebe3",
   "metadata": {},
   "source": [
    "在此 notebook 中，我们将使用以下函数和类来支持我们与 LLM 的交互。请随时浏览这些函数和类，因为它们在下面的使用中将详细介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ceca2-1a25-4ede-8cf8-11f98dcd14e6",
   "metadata": {},
   "source": [
    "### 生成模型响应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42343e1f-40d8-4b9a-a56c-53f60774dcd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(prompt, max_length=4096, pipe=llama_pipe, **kwargs):\n",
    "    \"\"\"\n",
    "    Generates a response to the given prompt using a specified language model pipeline.\n",
    "\n",
    "    This function takes a prompt and passes it to a language model pipeline, such as LLaMA, \n",
    "    to generate a text response. The function is designed to allow customization of the \n",
    "    generation process through various parameters and keyword arguments.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The input text prompt to generate a response for.\n",
    "    - max_length (int): The maximum length of the generated response. Default is 1024 tokens.\n",
    "    - pipe (callable): The language model pipeline function used for generation. Default is llama_pipe.\n",
    "    - **kwargs: Additional keyword arguments that are passed to the pipeline function.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated text response from the model, trimmed of leading and trailing whitespace.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    prompt_text = \"Explain the theory of relativity.\"\n",
    "    response = generate(prompt_text, max_length=512, pipe=my_custom_pipeline, temperature=0.7)\n",
    "    print(response)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def_kwargs = dict(return_full_text=False, return_dict=False)\n",
    "    response = pipe(prompt.strip(), max_length=max_length, **kwargs, **def_kwargs)\n",
    "    return response[0]['generated_text'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeec8244-ebf1-4ffd-af1d-85224c9e3927",
   "metadata": {},
   "source": [
    "### 构造提示词，包括可选的系统上下文和/或示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40abda7d-fffc-4afb-bbb9-922dea42057b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_prompt_with_context(main_prompt, system_context=\"\", conversation_examples=[]):\n",
    "    \"\"\"\n",
    "    Constructs a complete structured prompt for a language model, including optional system context and conversation examples.\n",
    "\n",
    "    This function compiles a prompt that can be directly used for generating responses from a language model. \n",
    "    It creates a structured format that begins with an optional system context message, appends a series of conversational \n",
    "    examples as prior interactions, and ends with the main user prompt. If no system context or conversation examples are provided,\n",
    "    it will return only the main prompt.\n",
    "\n",
    "    Parameters:\n",
    "    - main_prompt (str): The core question or statement for the language model to respond to.\n",
    "    - system_context (str, optional): Additional context or information about the scenario or environment. Defaults to an empty string.\n",
    "    - conversation_examples (list of tuples, optional): Prior exchanges provided as context, where each tuple contains a user message \n",
    "      and a corresponding agent response. Defaults to an empty list.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string formatted as a complete prompt ready for language model input. If no system context or examples are provided, returns the main prompt.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    main_prompt = \"I'm looking to improve my dialogue writing skills for my next short story. Any suggestions?\"\n",
    "    system_context = \"User is an aspiring author seeking to enhance dialogue writing techniques.\"\n",
    "    conversation_examples = [\n",
    "        (\"How can dialogue contribute to character development?\", \"Dialogue should reveal character traits and show personal growth over the story arc.\"),\n",
    "        (\"What are some common pitfalls in writing dialogue?\", \"Avoid exposition dumps in dialogue and make sure each character's voice is distinct.\")\n",
    "    ]\n",
    "\n",
    "    full_prompt = construct_prompt_with_context(main_prompt, system_context, conversation_examples)\n",
    "    print(full_prompt)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    # Return the main prompt if no system context or conversation examples are provided\n",
    "    if not system_context and not conversation_examples:\n",
    "        return main_prompt\n",
    "\n",
    "    # Start with the initial part of the prompt including the system context, if provided\n",
    "    full_prompt = f\"<s>[INST] <<SYS>>{system_context}<</SYS>>\\n\" if system_context else \"<s>[INST]\\n\"\n",
    "\n",
    "    # Add each example from the conversation_examples to the prompt\n",
    "    for user_msg, agent_response in conversation_examples:\n",
    "        full_prompt += f\"{user_msg} [/INST] {agent_response} </s><s>[INST]\"\n",
    "\n",
    "    # Add the main user prompt at the end\n",
    "    full_prompt += f\"{main_prompt} [/INST]\"\n",
    "\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d3b17d-2b8b-49a3-ba12-d6b1a6b20cc9",
   "metadata": {},
   "source": [
    "### LlamaChatbot 类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e18ccd-72e4-40aa-a2fb-d8359fb5fda8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LlamaChatbot:\n",
    "    \"\"\"\n",
    "    A chatbot interface for generating conversational responses using the LLaMA language model.\n",
    "\n",
    "    Attributes:\n",
    "    - system_context (str): Contextual information to provide to the language model for all conversations.\n",
    "    - conversation_history (list of tuples): Stores the history of the conversation, where each\n",
    "      tuple contains a user message and the corresponding agent response.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, system_context):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the LlamaChatbot class.\n",
    "\n",
    "        Parameters:\n",
    "        - system_context (str): A string that sets the initial context for the language model.\n",
    "        \"\"\"\n",
    "        self.system_context = system_context\n",
    "        self.conversation_history = []  # Initializes the conversation history\n",
    "\n",
    "    def chat(self, user_msg):\n",
    "        \"\"\"\n",
    "        Generates a response from the chatbot based on the user's message.\n",
    "\n",
    "        This method constructs a prompt with the current system context and conversation history,\n",
    "        sends it to the language model, and then stores the new user message and model's response\n",
    "        in the conversation history.\n",
    "\n",
    "        Parameters:\n",
    "        - user_msg (str): The user's message to which the chatbot will respond.\n",
    "\n",
    "        Returns:\n",
    "        - str: The generated response from the chatbot.\n",
    "        \"\"\"\n",
    "        # Generate the prompt using the conversation history and the new user message\n",
    "        prompt = construct_prompt_with_context(user_msg, self.system_context, self.conversation_history)\n",
    "        \n",
    "        # Get the model's response\n",
    "        agent_response = generate(prompt)\n",
    "\n",
    "        # Store this interaction in the conversation history\n",
    "        self.conversation_history.append((user_msg, agent_response))\n",
    "\n",
    "        return agent_response\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the conversation history of the chatbot.\n",
    "\n",
    "        This method clears the existing conversation history, effectively restarting the conversation.\n",
    "        \"\"\"\n",
    "        # Clear conversation history\n",
    "        self.conversation_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c5fd5-0c47-40e6-894b-b673e4957de1",
   "metadata": {},
   "source": [
    "### LlamaChatBotWithHistoryLimit 类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36917bf9-73e2-4580-a30e-7315e40b9a43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LlamaChatbotWithHistoryLimit:\n",
    "    \"\"\"\n",
    "    A chatbot interface for generating conversational responses using the LLaMA language model.\n",
    "\n",
    "    Attributes:\n",
    "        - system_context (str): Contextual information to provide to the language model for all conversations.\n",
    "        - conversation_history (list of tuples): Stores the history of the conversation, where each\n",
    "          tuple contains a user message and the corresponding agent response.\n",
    "        - tokenizer: The tokenizer used to tokenize the conversation for maintaining the history limit.\n",
    "        - max_tokens (int): The maximum number of tokens allowed in the conversation history.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, system_context, tokenizer, max_tokens=2048):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the LlamaChatbot class with a tokenizer and token limit.\n",
    "\n",
    "        Parameters:\n",
    "            - system_context (str): A string that sets the initial context for the language model.\n",
    "            - tokenizer: The tokenizer used to process the input and output for the language model.\n",
    "            - max_tokens (int): The maximum number of tokens to retain in the conversation history.\n",
    "        \"\"\"\n",
    "        self.system_context = system_context\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_tokens = max_tokens\n",
    "        self.conversation_history = []  # Initializes the conversation history\n",
    "\n",
    "    def chat(self, user_msg):\n",
    "        \"\"\"\n",
    "        Generates a response from the chatbot based on the user's message.\n",
    "\n",
    "        This method constructs a prompt with the current system context and conversation history,\n",
    "        sends it to the language model, and then stores the new user message and model's response\n",
    "        in the conversation history, ensuring that the history does not exceed the specified token limit.\n",
    "\n",
    "        Parameters:\n",
    "            - user_msg (str): The user's message to which the chatbot will respond.\n",
    "\n",
    "        Returns:\n",
    "            - str: The generated response from the chatbot.\n",
    "        \"\"\"\n",
    "        # Generate the prompt using the conversation history and the new user message\n",
    "        prompt = construct_prompt_with_context(user_msg, self.system_context, self.conversation_history)\n",
    "        \n",
    "        # Get the model's response\n",
    "        agent_response = generate(prompt)\n",
    "\n",
    "        # Store this interaction in the conversation history\n",
    "        self.conversation_history.append((user_msg, agent_response))\n",
    "\n",
    "        # Check and maintain the conversation history within the token limit\n",
    "        self._trim_conversation_history()\n",
    "\n",
    "        return agent_response\n",
    "\n",
    "    def _trim_conversation_history(self):\n",
    "        \"\"\"\n",
    "        Trims the conversation history to maintain the number of tokens below the specified limit.\n",
    "        \"\"\"\n",
    "        # Concatenate the conversation history into a single string\n",
    "        history_string = ''.join(user + agent for user, agent in self.conversation_history)\n",
    "        \n",
    "        # Calculate the number of tokens in the conversation history\n",
    "        history_tokens = len(self.tokenizer.encode(history_string))\n",
    "\n",
    "        # While the history exceeds the maximum token limit, remove the oldest items\n",
    "        while history_tokens > self.max_tokens:\n",
    "            # Always check if there's at least one item to pop\n",
    "            if self.conversation_history:\n",
    "                # Remove the oldest conversation tuple\n",
    "                self.conversation_history.pop(0)\n",
    "                # Recalculate the history string and its tokens\n",
    "                history_string = ''.join(user + agent for user, agent in self.conversation_history)\n",
    "                history_tokens = len(self.tokenizer.encode(history_string))\n",
    "            else:\n",
    "                # If the conversation history is empty, break out of the loop\n",
    "                break\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the conversation history of the chatbot.\n",
    "\n",
    "        This method clears the existing conversation history, effectively restarting the conversation.\n",
    "        \"\"\"\n",
    "        # Clear conversation history\n",
    "        self.conversation_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd12556-fb8e-4768-9044-811622cec89b",
   "metadata": {},
   "source": [
    "### 打印给定字符串的 token 数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2f0379-07aa-4583-9b37-2995aa882ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_token_count(text, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculate and return the number of tokens in a given text using a specified tokenizer.\n",
    "\n",
    "    This function takes a string of text and a tokenizer. It uses the tokenizer to encode the text\n",
    "    into tokens and then returns the count of these tokens.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input string to be tokenized.\n",
    "    - tokenizer: A tokenizer instance capable of encoding text into tokens.\n",
    "\n",
    "    Returns:\n",
    "    - int: The number of tokens in the input text as determined by the tokenizer.\n",
    "    \"\"\"\n",
    "    return len(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2821401d-ac98-4b30-9f6f-70ce07b731ea",
   "metadata": {},
   "source": [
    "### 拼接对话历史记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea30bb2-94e3-4f6d-94fd-d46e43645c9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def concat_history(tuples_list):\n",
    "    \"\"\"\n",
    "    Concatenates texts from a list of 2-tuples.\n",
    "\n",
    "    Each tuple in the list is expected to contain two strings. The function\n",
    "    will concatenate all the first elements followed by all the second elements\n",
    "    in their respective order of appearance in the list.\n",
    "\n",
    "    Parameters:\n",
    "    - tuples_list (list of 2-tuples): A list where each element is a tuple of two strings.\n",
    "\n",
    "    Returns:\n",
    "    - str: A single string that is the result of concatenating all the texts from the tuples.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    conversation_tuples = [\n",
    "        ('Question 1', 'Answer 1'),\n",
    "        ('Question 2', 'Answer 2'),\n",
    "        ('Question 3', 'Answer 3')\n",
    "    ]\n",
    "\n",
    "    concatenated_text = concatenate_texts_from_tuples(conversation_tuples)\n",
    "    print(concatenated_text)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # Concatenate all the first and second elements of the tuples\n",
    "    return ''.join(question + response for question, response in tuples_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6781a436-aec7-47d6-83a3-3cccf373c978",
   "metadata": {},
   "source": [
    "## 数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef88361d-a326-4a8d-bd75-31742be6378b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Star Bikes 详细信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea1e03-4438-4bea-af72-e5a5fe2783ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bikes = [\n",
    "    {\n",
    "        \"model\": \"Galaxy Rider\",\n",
    "        \"type\": \"Mountain\",\n",
    "        \"features\": {\n",
    "            \"frame\": \"Aluminum alloy\",\n",
    "            \"gears\": \"21-speed Shimano\",\n",
    "            \"brakes\": \"Hydraulic disc\",\n",
    "            \"tires\": \"27.5-inch all-terrain\",\n",
    "            \"suspension\": \"Full, adjustable\",\n",
    "            \"color\": \"Matte black with green accents\"\n",
    "        },\n",
    "        \"usps\": [\"Lightweight frame\", \"Quick gear shift\", \"Durable tires\"],\n",
    "        \"price\": 799.95,\n",
    "        \"internal_id\": \"GR2321\",\n",
    "        \"weight\": \"15.3 kg\",\n",
    "        \"manufacturer_location\": \"Taiwan\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Nebula Navigator\",\n",
    "        \"type\": \"Hybrid\",\n",
    "        \"features\": {\n",
    "            \"frame\": \"Carbon fiber\",\n",
    "            \"gears\": \"18-speed Nexus\",\n",
    "            \"brakes\": \"Mechanical disc\",\n",
    "            \"tires\": \"26-inch city slick\",\n",
    "            \"suspension\": \"Front only\",\n",
    "            \"color\": \"Glossy white\"\n",
    "        },\n",
    "        \"usps\": [\"Sleek design\", \"Efficient on both roads and trails\", \"Ultra-lightweight\"],\n",
    "        \"price\": 649.99,\n",
    "        \"internal_id\": \"NN4120\",\n",
    "        \"weight\": \"13.5 kg\",\n",
    "        \"manufacturer_location\": \"Germany\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Cosmic Comet\",\n",
    "        \"type\": \"Road\",\n",
    "        \"features\": {\n",
    "            \"frame\": \"Titanium\",\n",
    "            \"gears\": \"24-speed Campagnolo\",\n",
    "            \"brakes\": \"Rim brakes\",\n",
    "            \"tires\": \"700C road\",\n",
    "            \"suspension\": \"None\",\n",
    "            \"color\": \"Metallic blue\"\n",
    "        },\n",
    "        \"usps\": [\"Super aerodynamic\", \"High-speed performance\", \"Professional-grade components\"],\n",
    "        \"price\": 1199.50,\n",
    "        \"internal_id\": \"CC5678\",\n",
    "        \"weight\": \"11 kg\",\n",
    "        \"manufacturer_location\": \"Italy\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb9670-9447-43bd-9f42-1c36adc9fe00",
   "metadata": {},
   "source": [
    "## 自行车 AI 助手"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5da85-b6d6-4c45-a9dd-117caa121a89",
   "metadata": {},
   "source": [
    "在本节中，我们将创建一个 AI 客户支持助手，帮助潜在客户购买一辆 Star Bike。\n",
    "\n",
    "我们先设置一个恰当的**系统上下文**并实例化一个聊天机器人实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f807d3b9-00a0-428d-beb4-07e061fcc06d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = \"\"\"\n",
    "You are a friendly chatbot knowledgeable about bicycles. \\\n",
    "When asked about specific bike models or features, you try to provide accurate and helpful answers. \\\n",
    "Your goal is to assist and inform potential customers to the best of your ability in 50 words or less.\n",
    "\"\"\"\n",
    "\n",
    "chatbot = LlamaChatbot(system_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a340a315-10c6-465c-a027-1359ae45dfcc",
   "metadata": {},
   "source": [
    "让模型告诉我们最新的自行车。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de7b39-d9e0-43db-906a-96fb33ce149f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Can you tell me about the latest models?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb5049d-7a9c-450c-85d9-cad7795c40e2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6963c6-fa77-4875-a2a9-6cf09dfc3755",
   "metadata": {},
   "source": [
    "还不错，但我们当然希望助手向我们介绍的是 Star Bikes 的车型！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549fc0ac-9e17-477b-9f88-2cc49607d515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chatbot.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f46541-c7c8-4f44-993f-21df834135b1",
   "metadata": {},
   "source": [
    "## Star Bikes AI 助手"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f31c9b-d1d5-4dbc-81c4-c2999a007433",
   "metadata": {},
   "source": [
    "我们来创建一个新的聊天机器人，包含上述 `bikes` 数据供其参考。在下面的**系统上下文**中，我们为模型提供了一个**提示**，让它在每轮对话后都询问还有什么可以帮助的。这不仅对于 AI 助手来说是一个好方法，还可以在实践中防止模型无限期的继续下去，或者在只需要一轮对话时增加多次交互。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb3496-72a5-4f62-a170-0e246e62f101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = f\"\"\"\n",
    "You are a friendly chatbot knowledgeable about these bicycles from Star Bikes {bikes}. \\\n",
    "When asked about specific bike models or features, you try to provide accurate and helpful answers. \\\n",
    "Your goal is to assist and inform potential customers to the best of your ability in 50 words or less. \\\n",
    "You always end by asking what else you can help with.\n",
    "\"\"\"\n",
    "\n",
    "chatbot = LlamaChatbot(system_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291ed1b-caff-416f-a964-19ee773affe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Can you tell me about the latest models?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f4ca3-3dc5-453a-b6f7-b94c27981c07",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd4d51-3e6d-40e9-84b1-ff5cfd575134",
   "metadata": {},
   "source": [
    "看起来很好。来看看被问到自行车的详细信息时它会怎么回复？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9f4c4a-294c-4c2c-bbe0-263fde0c5eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"How much do each of the models cost?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d44016a-21e8-4bb5-bb69-a49b18e952cd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a0a2ca-0519-428f-9f78-bba5338e3c56",
   "metadata": {},
   "source": [
    "非常好。再看看它如何响应更模糊的查询。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4deafd-9b62-4406-b7f4-8d9bf773dbd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"I am more intersted in biking around town.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0e08ee-70ce-40bc-be89-b80efaf7d3da",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f966feaf-d013-4d2b-a53e-457ecbc10b65",
   "metadata": {},
   "source": [
    "总而言之，我们的助手似乎已经表现的很好了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83c788c-e833-48a5-9696-4b9137cd3ed5",
   "metadata": {},
   "source": [
    "## 关于 Token 数量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51933a1-e07c-4fa9-b836-60f27523da47",
   "metadata": {},
   "source": [
    "当我们将文本传给 LLaMA-2 等语言模型时，文本已经被转为 **token** 了，它们是语言模型能够处理并据此生成内容的文本单位，像是单词或标点符号。\n",
    "\n",
    "像 LLaMA-2 这样的语言模型天生带有 **token 限制**，是模型在单个提示响应周期中可以处理的 token 数量固定上限。该限制取决于它们的设计和处理 token 所需的计算资源。LLaMA-2 模型的 **token 限制**是 `4096`。模型的 token 限制可以从它的文档查到，也可以在其限制范围内进行调控。使用 `transformers` 工作流时，可以通过 `max_length` 参数来调控。\n",
    "\n",
    "其自身的 token 限制，或 `max_length` 参数（以较少者为准）*同时决定了输入和输出的 token 总数*。\n",
    "\n",
    "由于我们没清除上面对话的聊天记录，可以直接看看 `chatbot` 实例现在的 `conversation_history` 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97255f11-7dd8-4fb8-bc63-32b7f5175e20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.conversation_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43394f98-71bd-4d45-8590-89220bda0809",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22e3162-8e23-4725-b529-ac1b887b1873",
   "metadata": {},
   "source": [
    "为了算出来所有这些字符串所包含的 token 总量，我们用上面定义的 `concat_history` 辅助函数来将对话历史的所有字符串拼接起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b974926-4b7c-46ca-b291-a318fd3208f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conv_history = concat_history(chatbot.conversation_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ee2b0-e59e-4a6d-acc6-e06a4f60e4d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(conv_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456682a0-78fd-4e73-b228-0f44c2311197",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8fed90-be75-43c6-9ec1-c92cf75aa46d",
   "metadata": {},
   "source": [
    "现在，我们将使用上面定义的另一个辅助函数，`print_token_count`，来对对话历史字符串进行**分词**，用的就是已经导入的 LLaMA-2 **分词器**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d200a0fc-b1cc-4502-a18a-a4fdb235f0c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(conv_history, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448d19d2-433f-4444-bf0d-9d965e76f76a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c982e096-b692-4a85-8be0-d76b490749b9",
   "metadata": {},
   "source": [
    "我们来看看跟聊天机器人进行多次交流时，对话历史的 token 数是如何变化的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198b0214-25cc-4769-b66e-5223ad416913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"What kind of bike would be best if I'm on a budget?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7edb140-6106-45fe-8f29-f2a5f66b4ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfcca2a-854f-4dd6-86b2-3fefba87c734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"What's the next most expensive bike after the Galaxy Rider?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad11054-d79b-4bbd-9241-e0319a50e641",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a24973-314a-423b-b382-32d8dbe823cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Why is titanium so good for a frame?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c72ea7-2336-4320-a98e-bdd322ec51c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10615ebc-4ff2-4bd1-a381-30b502b7cf56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Do you remember where I said I was most interested in riding?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9348ee5a-8e5c-49d9-87fd-3f7a50874fae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed953f4-1cfb-42c0-90e2-99f913c8c90a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Can you please summarize our conversation for me?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95345a80-afd4-4187-8cdf-8fc8a774f84a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2108c8a9-d8cf-4c1c-a9d0-aced31bf9f3f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a46c457-4f67-4afb-a482-00386b630878",
   "metadata": {},
   "source": [
    "最后，我们来重置聊天机器人并再次打印 token 数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a5656e-fc22-4a7d-83e4-90bb8b41be04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chatbot.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d76126-2cbe-42e1-b800-b83243a36dd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715bde14-8613-49a7-aa11-9dd454a83eac",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65263216-9d3a-4556-8d5d-7ecfa1a20c32",
   "metadata": {},
   "source": [
    "鉴于我们的聊天机器人实现是通过将对话历史记录放到提示词里来存储对话的，因此我们与模型的每次交流都使得输入更加接近 **token 限制**。\n",
    "\n",
    "如上所述，我们使用的模型的 **token 限制**是 `4096`，如果您查看上面的 `generate` 函数，就会发现我们传给 `max_length` 参数的值就是 `4096`。因此，我们暂时还并未接近 **token 限制**，但我们依然应该考虑如何确保这个硬性限制不会影响我们的聊天机器人。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac55f853-6bdd-4204-9eaf-20cae86e942c",
   "metadata": {},
   "source": [
    "## 限制聊天记录"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d1801-499b-4866-a5df-d4007ea31053",
   "metadata": {},
   "source": [
    "下面是修改后的聊天类 `LlamaChatbotWithHistoryLimit`。它接受 `max_tokens` 参数，以及用于跟踪对话历史记录中 token 数量的 `tokenizer` 。\n",
    "\n",
    "当对话历史记录超过 `max_tokens` 时，就会调用 `_trim_conversation_history` 来弹出（pop off）最早的历史对话直到总长度小于 `max_tokens`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ee658-5985-48ff-8b87-f911c5f4b458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LlamaChatbotWithHistoryLimit:\n",
    "    \"\"\"\n",
    "    A chatbot interface for generating conversational responses using the LLaMA language model.\n",
    "\n",
    "    Attributes:\n",
    "        - system_context (str): Contextual information to provide to the language model for all conversations.\n",
    "        - conversation_history (list of tuples): Stores the history of the conversation, where each\n",
    "          tuple contains a user message and the corresponding agent response.\n",
    "        - tokenizer: The tokenizer used to tokenize the conversation for maintaining the history limit.\n",
    "        - max_tokens (int): The maximum number of tokens allowed in the conversation history.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, system_context, tokenizer, max_tokens=2048):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the LlamaChatbot class with a tokenizer and token limit.\n",
    "\n",
    "        Parameters:\n",
    "            - system_context (str): A string that sets the initial context for the language model.\n",
    "            - tokenizer: The tokenizer used to process the input and output for the language model.\n",
    "            - max_tokens (int): The maximum number of tokens to retain in the conversation history.\n",
    "        \"\"\"\n",
    "        self.system_context = system_context\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_tokens = max_tokens\n",
    "        self.conversation_history = []  # Initializes the conversation history\n",
    "\n",
    "    def chat(self, user_msg):\n",
    "        \"\"\"\n",
    "        Generates a response from the chatbot based on the user's message.\n",
    "\n",
    "        This method constructs a prompt with the current system context and conversation history,\n",
    "        sends it to the language model, and then stores the new user message and model's response\n",
    "        in the conversation history, ensuring that the history does not exceed the specified token limit.\n",
    "\n",
    "        Parameters:\n",
    "            - user_msg (str): The user's message to which the chatbot will respond.\n",
    "\n",
    "        Returns:\n",
    "            - str: The generated response from the chatbot.\n",
    "        \"\"\"\n",
    "        # Generate the prompt using the conversation history and the new user message\n",
    "        prompt = construct_prompt_with_context(user_msg, self.system_context, self.conversation_history)\n",
    "        \n",
    "        # Get the model's response\n",
    "        agent_response = generate(prompt)\n",
    "\n",
    "        # Store this interaction in the conversation history\n",
    "        self.conversation_history.append((user_msg, agent_response))\n",
    "\n",
    "        # Check and maintain the conversation history within the token limit\n",
    "        self._trim_conversation_history()\n",
    "\n",
    "        return agent_response\n",
    "\n",
    "    def _trim_conversation_history(self):\n",
    "        \"\"\"\n",
    "        Trims the conversation history to maintain the number of tokens below the specified limit.\n",
    "        \"\"\"\n",
    "        # Concatenate the conversation history into a single string\n",
    "        history_string = ''.join(user + agent for user, agent in self.conversation_history)\n",
    "        \n",
    "        # Calculate the number of tokens in the conversation history\n",
    "        history_tokens = len(self.tokenizer.encode(history_string))\n",
    "\n",
    "        # While the history exceeds the maximum token limit, remove the oldest items\n",
    "        while history_tokens > self.max_tokens:\n",
    "            # Always check if there's at least one item to pop\n",
    "            if self.conversation_history:\n",
    "                # Remove the oldest conversation tuple\n",
    "                self.conversation_history.pop(0)\n",
    "                # Recalculate the history string and its tokens\n",
    "                history_string = ''.join(user + agent for user, agent in self.conversation_history)\n",
    "                history_tokens = len(self.tokenizer.encode(history_string))\n",
    "            else:\n",
    "                # If the conversation history is empty, break out of the loop\n",
    "                break\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the conversation history of the chatbot.\n",
    "\n",
    "        This method clears the existing conversation history, effectively restarting the conversation.\n",
    "        \"\"\"\n",
    "        # Clear conversation history\n",
    "        self.conversation_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96740912-962e-46d5-8d1c-c6539b4e026d",
   "metadata": {},
   "source": [
    "我们来创建一个新的聊天机器人实例，这次 `max_tokens` 限制为 `200` 个 token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcdd33e-20ed-4a2c-b75e-a9683a0fffce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = f\"\"\"\n",
    "You are a friendly chatbot knowledgeable about these bicycles from Star Bikes {bikes}. \\\n",
    "When asked about specific bike models or features, you try to provide accurate and helpful answers. \\\n",
    "Your goal is to assist and inform potential customers to the best of your ability in 50 words or less. \\\n",
    "You always end by asking what else you can help with.\n",
    "\"\"\"\n",
    "\n",
    "chatbot = LlamaChatbotWithHistoryLimit(system_context, tokenizer=tokenizer, max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc914310-ea45-42f8-93c9-a5daa9c9d7d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Can you tell me about the latest models?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15746b3b-865d-42f0-83ed-a2ad99b27929",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b747ce-4c49-497d-96cd-2f1e57040a20",
   "metadata": {},
   "source": [
    "我们再多运行几轮，来跟踪对话历史记录中的 token 数量。请记住，我们已经将 `max_tokens` 设成了 `200`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce146ee1-5554-41d8-a9e9-2b28f687fe2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895965d9-469d-4aa8-a039-1daf29e426c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"How much do each of the models cost?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39882df-017f-43b6-9f82-fecd421e477b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c8fe2b-b4a9-430d-bfb3-295a05af8af4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63d4685-f4cb-4325-9f38-7ec21d0c1796",
   "metadata": {},
   "source": [
    "您可以看到 token 数量已减少到 `96` 来防止我们超出 `200` 的限制。我们再观察几轮对话。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b387aca-23d9-4ef9-8271-4b58cc7466a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"I am more intersted in biking around town.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243c322b-c52a-43f4-87e2-87716c90f073",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212308d0-07be-4ba0-9697-56511599037e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"What kind of bike would be best if I'm on a budget?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b687d7-efbf-4b44-b719-18ffda87cacc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87b6c15-bf5b-4b8d-a55d-ae9a0abfe611",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06765c5b-faef-4171-8b7b-66e847f0cf94",
   "metadata": {},
   "source": [
    "我们的聊天机器人已经成功的弹出了前几轮对话，以避免超出 token 限制。\n",
    "\n",
    "当然，这种防止故障的行为以不能完美的保留全部对话历史为代价。这里，我们可以看到与以往不同的是，当我们要求提供迄今为止的全部对话摘要时，我们只会得到最近几轮的交流总结。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965aa16-54d0-412f-9829-880967bc4557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Can you summarize our conversation?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f0382b-c259-495d-827b-abf508d44d21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592e4d1-97ea-4b61-9002-8e34f0ce026e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chatbot.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c137b-9817-49e0-a624-70d7020d5f47",
   "metadata": {},
   "source": [
    "## 最终练习：为您自己的虚构公司创建 AI 助手"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4d9001-947c-4c39-9d02-a2f463aaad41",
   "metadata": {},
   "source": [
    "运用您迄今为止所学的一切，为您自己虚构的公司创建一个 AI 助手。您的工作要包含几个主要步骤。\n",
    "1. 想一个公司，包括公司名和它销售的东西。\n",
    "2. 使用我们的 LLaMA-2 模型为您公司将要销售的产品生成合成数据。请参考上面的“Star Bikes 细节”部分，或 `bikes` 字典为例。如果您在生成合成 JSON 数据时遇到问题，请参阅 notebook *3-Review Analyst.ipynb*。\n",
    "3. 创建 AI 助手，为其提供您在上一步中生成的合成数据。欢迎您使用这个 notebook 提供的 `LlamaChatbotWithHistoryLimit` 类。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881eec26-11f0-4880-9c97-12c57bf29e39",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 关键概念回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e173642a-2e66-4194-9296-0b54e3c599e2",
   "metadata": {},
   "source": [
    "此 notebook 中介绍了以下关键概念：\n",
    "* **Token**：语言模型能够处理的一个文本块，比如单词或标点。\n",
    "* **Token 限制**：语言模型在单个提示词中可以处理的最大 token 数量。\n",
    "* **分词器**：一种将文本转换为 token 的工具，供语言模型理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dadf18-3fda-451b-a684-58510245cfb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 重启内核"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049cf48a-82c8-4010-a863-01dad76b8187",
   "metadata": {},
   "source": [
    "为下一个 notebook 释放 GPU 显存，请运行以下单元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e333754-31d1-4d45-8c8f-a8db78466599",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
