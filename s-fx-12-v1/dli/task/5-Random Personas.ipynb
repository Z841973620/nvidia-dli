{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ee00fa-b5ba-4887-943c-dfb93bf9a6e4",
   "metadata": {},
   "source": [
    "<center><img src=\"/files/images/DLI_Header.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706e8c2b-5f1b-4b7b-9ece-b6b341e8b49d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 随机人格"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e22814-a66d-4646-9966-9843a433ce70",
   "metadata": {},
   "source": [
    "在此 notebook 中，您将学习如何通过控制模型生成的**温度**（temperature）来控制其响应的随机性。利用**温度**，您将能够创造各种 AI 人格。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff739fa-f350-4b50-b3ad-d19fbefd4e6f",
   "metadata": {},
   "source": [
    "## 学习目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daefff31-716d-42f7-8362-26a10145727d",
   "metadata": {},
   "source": [
    "完成此 notebook 后，您将能够：\n",
    "* 解释**采样**是如何导致非确定性 LLM 响应的。\n",
    "* 通过调整**温度**来控制响应的随机度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a637834-47c5-47c6-8a5f-2e460c8c7401",
   "metadata": {},
   "source": [
    "## 视频教程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe924aa8-0cc7-4f28-be75-aab8da75f2e1",
   "metadata": {},
   "source": [
    "执行以下单元以加载此 notebook 的视频教程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ab4a4a-82bf-4eb2-95cb-cc37f06b7469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-12-v1/v2/05-random.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44272604-6cca-4cb4-b19d-2b8f415ebd74",
   "metadata": {},
   "source": [
    "## 创建 LLaMA-2 工作流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c28974-d576-4886-ab0b-061659187313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "model = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "# model = \"TheBloke/Llama-2-7B-chat-GPTQ\"\n",
    "\n",
    "llama_pipe = pipeline(\"text-generation\", model=model, device_map=\"auto\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b612f-5ce9-4e47-ab25-cbe946164506",
   "metadata": {},
   "source": [
    "## 辅助函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4540012d-1d17-44e2-af7e-a5ff0c587101",
   "metadata": {},
   "source": [
    "在此 notebook 中，我们将使用下面这个函数与 LLM 交互。可以先大概看一眼，后面会详细介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76923260-7338-4380-9fd7-d3c7756d651e",
   "metadata": {},
   "source": [
    "### 生成模型响应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5148dc93-f5bf-4242-b916-40087f2273c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(prompt, max_length=1024, pipe=llama_pipe, **kwargs):\n",
    "    \"\"\"\n",
    "    Generates a response to the given prompt using a specified language model pipeline.\n",
    "\n",
    "    This function takes a prompt and passes it to a language model pipeline, such as LLaMA, \n",
    "    to generate a text response. The function is designed to allow customization of the \n",
    "    generation process through various parameters and keyword arguments.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The input text prompt to generate a response for.\n",
    "    - max_length (int): The maximum length of the generated response. Default is 1024 tokens.\n",
    "    - pipe (callable): The language model pipeline function used for generation. Default is llama_pipe.\n",
    "    - **kwargs: Additional keyword arguments that are passed to the pipeline function.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated text response from the model, trimmed of leading and trailing whitespace.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    prompt_text = \"Explain the theory of relativity.\"\n",
    "    response = generate(prompt_text, max_length=512, pipe=my_custom_pipeline, temperature=0.7)\n",
    "    print(response)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def_kwargs = dict(return_full_text=False, return_dict=False)\n",
    "    response = pipe(prompt.strip(), max_length=max_length, **kwargs, **def_kwargs)\n",
    "    return response[0]['generated_text'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f120b024-f651-4889-891c-75994ee196e7",
   "metadata": {},
   "source": [
    "### 构造提示词，包括可选的系统上下文和/或示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d208aa65-1a47-4f96-be1a-7e98b4ba0050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_prompt_with_context(main_prompt, system_context=\"\", conversation_examples=[]):\n",
    "    \"\"\"\n",
    "    Constructs a complete structured prompt for a language model, including optional system context and conversation examples.\n",
    "\n",
    "    This function compiles a prompt that can be directly used for generating responses from a language model. \n",
    "    It creates a structured format that begins with an optional system context message, appends a series of conversational \n",
    "    examples as prior interactions, and ends with the main user prompt. If no system context or conversation examples are provided,\n",
    "    it will return only the main prompt.\n",
    "\n",
    "    Parameters:\n",
    "    - main_prompt (str): The core question or statement for the language model to respond to.\n",
    "    - system_context (str, optional): Additional context or information about the scenario or environment. Defaults to an empty string.\n",
    "    - conversation_examples (list of tuples, optional): Prior exchanges provided as context, where each tuple contains a user message \n",
    "      and a corresponding agent response. Defaults to an empty list.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string formatted as a complete prompt ready for language model input. If no system context or examples are provided, returns the main prompt.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    main_prompt = \"I'm looking to improve my dialogue writing skills for my next short story. Any suggestions?\"\n",
    "    system_context = \"User is an aspiring author seeking to enhance dialogue writing techniques.\"\n",
    "    conversation_examples = [\n",
    "        (\"How can dialogue contribute to character development?\", \"Dialogue should reveal character traits and show personal growth over the story arc.\"),\n",
    "        (\"What are some common pitfalls in writing dialogue?\", \"Avoid exposition dumps in dialogue and make sure each character's voice is distinct.\")\n",
    "    ]\n",
    "\n",
    "    full_prompt = construct_prompt_with_context(main_prompt, system_context, conversation_examples)\n",
    "    print(full_prompt)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    # Return the main prompt if no system context or conversation examples are provided\n",
    "    if not system_context and not conversation_examples:\n",
    "        return main_prompt\n",
    "\n",
    "    # Start with the initial part of the prompt including the system context, if provided\n",
    "    full_prompt = f\"<s>[INST] <<SYS>>{system_context}<</SYS>>\\n\" if system_context else \"<s>[INST]\\n\"\n",
    "\n",
    "    # Add each example from the conversation_examples to the prompt\n",
    "    for user_msg, agent_response in conversation_examples:\n",
    "        full_prompt += f\"{user_msg} [/INST] {agent_response} </s><s>[INST]\"\n",
    "\n",
    "    # Add the main user prompt at the end\n",
    "    full_prompt += f\"{main_prompt} [/INST]\"\n",
    "\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd78c2-d700-4a71-baea-b1e4b76ad80c",
   "metadata": {},
   "source": [
    "## 默认的非随机响应"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a535af-43af-4d04-b8d1-ba2196161302",
   "metadata": {},
   "source": [
    "您可能已在之前的 notebook 中注意到，用相同的提示词，LLaMA-2 模型就会返回相同的响应。下面我们来观察一下。\n",
    "\n",
    "这里，我们将使用模型执行另一项生成任务：生成 Galaxy Rider 山地自行车的虚拟客户体验。我们已经设置了恰当的系统上下文，提示模型去回忆一天难忘的骑行体验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ddb17-e619-4947-b0ba-0ade00a8c5ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = \"\"\"\n",
    "You are a perennially satisfied customer who loves to reminisce about personal experiences with products. \\\n",
    "You never delve into technical specifics, as you believe it's the emotion and the joy that matter most. \\\n",
    "You're excited for others to feel the same euphoria and happiness you do. Your aim isn't to advertise, \\\n",
    "but to share a genuine, heartfelt story of joy and contentment.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"Recall a memorable day out with your Galaxy Rider mountain bike in 50 words or less.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7983d5e7-ab19-445e-86c2-81fe48cce6d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate(construct_prompt_with_context(prompt, system_context)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5294b61-adcd-4a42-b813-533226809d70",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efbde1d-9ba0-4a27-8091-a185236043dd",
   "metadata": {},
   "source": [
    "让我们用完全相同的提示词再次生成一个响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faea14c-9bb3-41f0-90db-fda38da3c874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate(construct_prompt_with_context(prompt, system_context)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f0a1c2-39e5-48c2-9a03-8578ebf6afbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate(construct_prompt_with_context(prompt, system_context)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117165bb-fbd6-494d-9f52-8d5f2138e141",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef795d9-65d2-4be0-8d14-432cf7b6b128",
   "metadata": {},
   "source": [
    "如上所述，模型每次都会生成完全相同的响应。在许多场景下，这正是我们想要的行为，但对于其它场景，我们想让模型的响应有一定的随机性。为此，我们将修改模型响应的**温度**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bdc89d-281b-46f5-8f29-becd0359c0b0",
   "metadata": {},
   "source": [
    "## 采样和温度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a5b4e0-67a3-490f-aff7-1882ba4854f6",
   "metadata": {},
   "source": [
    "在语言模型领域，**采样**是指模型通过从潜在的下一个词（其实是 token，但我们可以把 token 先当成是单词）的概率分布中进行采样的过程。当我们在不启用**采样**的情况下与语言模型交互，模型的运行结果就会是确定的，也就是会稳定的选择可能性最大的下一个词。当您需要一致性和准确性时，这种默认行为很有用，但满足不了创造性和多样化的需求。\n",
    "\n",
    "用 `transformers` 工作流与 LLaMA-2 模型交互时，默认情况下**采样**是被*禁用*的。在调用 `generator` 的时候设置 `do_sample=True` 就可以开启采样。这样就让模型根据概率分布来选择词语，使得概率较低的词也能被选中，从而生成更多样化、更有趣的文本。\n",
    "\n",
    "启用采样后，我们还可以传入特定的**温度**值，您可以将其视为响应的随机程度。`temperature` 需要我们传一个 `0.0` 到 `1.0` 之间的值，值越大随机性就越强。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9d9a51-d5ac-4fe5-a736-5155dfa9a08d",
   "metadata": {},
   "source": [
    "## 练习：随机响应"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d6d682-07d0-4674-87a9-20cc840ec59d",
   "metadata": {},
   "source": [
    "复用与上面相同的**系统上下文**和提示词，启用**采样**（`do_sample=True`）并将**温度**设为最高值（`temperature=1.0`）。生成 3 种不同的响应，看看它们是不是各不相同。\n",
    "\n",
    "如果遇到问题，请查看下方的参考答案。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1eb2ab-b768-4a87-aeb3-8e1e468058a8",
   "metadata": {},
   "source": [
    "## 您的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608ac163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8244d459-7fdb-4737-8cd7-f2077de152c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 参考答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b65b2d-fdb6-4c65-8836-e8023cac56a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate(construct_prompt_with_context(prompt, system_context), do_sample=True, temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1fda59-8421-4abe-8865-1d20daa43723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate(construct_prompt_with_context(prompt, system_context), do_sample=True, temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ad85e2-13f3-40fc-91d6-e8106563661c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate(construct_prompt_with_context(prompt, system_context), do_sample=True, temperature=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e43770-6d77-4eb7-9288-1cafe2cc1d7b",
   "metadata": {},
   "source": [
    "## 创造力和准确性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc45442-ba4d-46ea-bf6b-b2ca4ec24270",
   "metadata": {},
   "source": [
    "最后，关于 `temperature`，值得一提的是随机生成虽然能满足对独特性和创意的需求，但不具备精确性。在对模型的精确性或事实准确性有要求的情况下，始终记得在增加温度时检查其输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d181a-1d5c-4c3a-87a6-fb454c2acf85",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 关键概念回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442ffaa9-9e52-4805-9273-858cb0904b01",
   "metadata": {},
   "source": [
    "此 notebook 中介绍了以下关键概念：\n",
    "* **采样**：文本生成过程中，语言模型根据词汇概率分布选择下一个词（token）的过程。\n",
    "* **温度**：用于控制采样随机性的超参数。温度越高，响应就越具多样性和创造性，温度越低，模型的输出就越能被预测、越保守。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193d3820-c0a8-497e-8dc0-fa741c1fba93",
   "metadata": {},
   "source": [
    "## 可选的进阶练习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a1b5a-4485-4f72-9c60-6257844367b3",
   "metadata": {},
   "source": [
    "如果您想超出本课程的内容进阶一下，可以试试下面的额外开放式练习。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa76f625-0299-4809-8d9d-c59896207cfe",
   "metadata": {},
   "source": [
    "### 使用 7B 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5016a362-dd54-413e-9c50-fd89a91d86ca",
   "metadata": {},
   "source": [
    "在 notebook 顶部，按照下面的代码重启内核后，取消注释以使用 7B 模型，而不是 13B 模型。试试通过提示工程在使用小（更弱）模型的情况下获得满意的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17909f3d-49de-42d4-861f-2a419ea404b8",
   "metadata": {},
   "source": [
    "### 制作交互角色"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b804fb-87f2-4448-b148-500b00158bda",
   "metadata": {},
   "source": [
    "现在，您可以生成虚拟人物的对话，尝试创建一个小型系统，创建多种不同的个性，并让他们产生交互。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32df389-b99f-4b57-ab15-dc17556d87ed",
   "metadata": {},
   "source": [
    "### 让可交互角色玩游戏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff82d544-edc3-4e3a-86bf-2cd180254aa2",
   "metadata": {},
   "source": [
    "扩展之前的练习，创建多个致力于实现某个“游戏”所定义目标的角色。可以是试图让一个角色说出某个词，或者是泄露一个秘密的藏宝点，甚至可以是某种让角色携手合作才能实现的目标。如果您真的想挑战一下自己，甚至可以考虑创建两个以上或是一组玩家进行互动。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b8885f-3035-44bc-9df7-501dcab4b705",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 重启内核"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ee978-eca8-49d8-b3e6-95d423f616b2",
   "metadata": {},
   "source": [
    "为下一个 notebook 释放 GPU 显存，请运行以下单元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8de2be-7716-4c23-a06f-7f8364e62bab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
