{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5101d226-325d-407a-82d7-5b221da9ce69",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"><img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53007a3d-38ff-4cd1-859b-0ced1e8500cd",
   "metadata": {},
   "source": [
    "## 评估：构建实时视频 AI 应用 ##\n",
    "在此 Notebook 中，您将利用在本课程中学到的内容完成评估。评估分为几个步骤，您需在各个步骤中生成用于评级的文本文件。评级的评量标准如下：\n",
    "<table border=\"1\" class=\"dataframe\" align='left'>  \n",
    "<thead>    \n",
    "<tr style=\"text-align: left;\">      <th>第n步</th>      <th>有几个 &lt;FIXME&gt;</th>      <th>分数</th>    \n",
    "</tr>  \n",
    "</thead>\n",
    "    \n",
    "<tbody>    <tr>      <td>0. 问题描述</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <td>1. 了解输入视频</td>      <td>5</td>      <td>10</td>    </tr>    <tr>      <td>2. 就 AI 推理开展头脑风暴并下载预训练模型</td>      <td>2</td>      <td>10</td>    </tr>    <tr>      <td>3. 编辑推理配置文件</td>      <td>10</td>      <td>10</td>    </tr>    <tr>      <td>4. 构建并运行 DeepStream 工作流</td>      <td>20</td>      <td>20</td>    </tr>    <tr>      <td>5. 分析结果</td>      <td>1</td>      <td>10</td>    </tr>    <tr>      <td>额外练习。可视化帧</td>      <td>0</td>      <td>0</td>    </tr>  \n",
    "</tbody>\n",
    "    \n",
    "</table>\n",
    "<br>\n",
    "<p><img src='images/iva_framework.png' width=600></p>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea28a7a-3536-4c75-8c84-5978f49db9a6",
   "metadata": {},
   "source": [
    "### 第 0 步：问题描述 ###\n",
    "您是车队管理公司的开发人员。最近公司在所有车辆上安装了行车记录仪，并准备实施 AI 来分析车队的驾驶行为。您注意到车队存在一个问题，当车辆在另一车辆后方行驶且未留够避免碰撞的停车间距时，如果前方车辆突然停车，就会造成[追尾](https://en.wikipedia.org/wiki/Tailgating)。因此您决定构建 DeepStream 应用来帮助监控此行为。在这种情况下，您希望能够记录发生追尾的次数，以便了解发生的频率。请注意，输入视频源是静态文件，但您可以轻松修改工作流，从而使用实时视频。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f1c9bd-488a-4b54-bbc0-80ab4ecd5c4e",
   "metadata": {},
   "source": [
    "**说明**：<br>\n",
    "0.1 执行此单元以将目标视频设置为环境变量。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f79d8c-9bc3-4f2b-929b-3baab61cbd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1\n",
    "# DO NOT CHANGE THIS CELL\n",
    "import os\n",
    "os.environ['TARGET_VIDEO_PATH']='data/assessment_stream.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d18bdd-6ee6-4241-aa2b-d057bd04309d",
   "metadata": {},
   "source": [
    "### 第 1 步：了解输入视频 ###\n",
    "第一步是先了解输入视频的属性，然后再设计一个系统来处理这些视频。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42feee71-f5d3-4f15-a120-5aaf12724b70",
   "metadata": {},
   "source": [
    "使用 `ffprobe`（[按需参阅文档](https://ffmpeg.org/ffprobe.html)）函数获取输入视频的 `height`、`width` 和 `frame rate`。此外，我们还会使用 `-hide_banner` 选项充分减少文本输出。\n",
    "\n",
    "**说明**：<br>\n",
    "1.1 执行此单元以预览视频。<br>\n",
    "1.2 执行此单元从输入视频流中收集信息。<br>\n",
    "1.3 *只将* `<FIXME>` 修改为正确的值，并执行此单元以标记您的答案。_您可以多次执行此单元，直到满意为止_ 。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4976ef4-5a16-4106-8454-48c07cfad8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1\n",
    "# DO NOT CHANGE THIS CELL\n",
    "from IPython.display import Video\n",
    "Video(os.environ['TARGET_VIDEO_PATH'], width=720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83672ee2-3e0f-47b1-bac3-f22df24669e9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1.2\n",
    "# DO NOT CHANGE THIS CELL\n",
    "!ffprobe -i $TARGET_VIDEO_PATH \\\n",
    "         -hide_banner \\\n",
    "         2>&1| tee my_assessment/video_profile.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d5c7b-7798-4920-800f-608d3752d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3\n",
    "FRAME_RATE=<<<<FIXME>>>>\n",
    "FRAME_HEIGHT=<<<<FIXME>>>>\n",
    "FRAME_WIDTH=<<<<FIXME>>>>\n",
    "FRAME_CODEC='<<<<FIXME>>>>'\n",
    "FRAME_COLOR_FORMAT='<<<<FIXME>>>>'\n",
    "\n",
    "# DO NOT CHANGE BELOW\n",
    "Answer=f\"\"\"\\\n",
    "FRAME RATE: {round(FRAME_RATE)} FPS \\\n",
    "HEIGHT: {FRAME_HEIGHT} \\\n",
    "WIDTH: {FRAME_WIDTH} \\\n",
    "FRAME_CODEC: {FRAME_CODEC} \\\n",
    "FRAME_COLOR_FORMAT: {FRAME_COLOR_FORMAT} \\\n",
    "\"\"\"\n",
    "\n",
    "!echo $Answer > my_assessment/answer_1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1998b0e1-2bb8-4922-af6a-710f2e47f9df",
   "metadata": {},
   "source": [
    "### 第 2 步：就 AI 推理开展头脑风暴并下载预训练模型 ###\n",
    "下一步是就实现目标所需的 AI 推理开展头脑风暴。对于此应用，我们需要检测视频帧中的汽车，并识别边界框低于阈值的情况（如下所示）。\n",
    "\n",
    "<p><img src='images/tailgating_logic.png' width=720></p>\n",
    "\n",
    "幸运的是，[DashCamNet](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/dashcamnet) 专用物体检测模型所用的训练数据与我们的视频相似。我们可以使用 [NGC CLI](https://ngc.nvidia.com/setup/installers/cli) 下载适合我们应用的 [DashCamNet](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/dashcamnet) 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ba02d6-91d7-467a-8063-4b161beda2cf",
   "metadata": {},
   "source": [
    "**说明**：<br>\n",
    "2.1 执行此单元以安装 NGC CLI。<br>\n",
    "2.2 执行此单元以使用可列出所有可用模型的 `ngc registry mode list` 命令。使用 `--column name`、`--column repository` 和 `--column application` 选项，以仅显示相关列。之后，查看 [DashCamNet](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/dashcamnet) 的模型卡，并确认此模型是否适用。<br>\n",
    "2.3 *只将* `<FIXME>` 修改为正确的值，并执行此单元以下载 [DashCamNet](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/dashcamnet) 模型。该命令的输出将生成一个文本文件，以供评级使用。_您可以多次执行此单元，直到满意为止_ 。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f0c649-280e-4ad9-a45f-4125c36afa13",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2.1\n",
    "# DO NOT CHANGE THIS CELL\n",
    "import os\n",
    "os.environ['NGC_DIR']='/dli/task/ngc_assets'\n",
    "\n",
    "%env CLI=ngccli_cat_linux.zip\n",
    "!mkdir -p $NGC_DIR/ngccli\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $NGC_DIR/ngccli\n",
    "!unzip -u \"$NGC_DIR/ngccli/$CLI\" \\\n",
    "       -d $NGC_DIR/ngccli/\n",
    "!rm $NGC_DIR/ngccli/*.zip \n",
    "os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"NGC_DIR\", \"\"), os.getenv(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf605e-fe17-48a8-b1b6-70f594a83b43",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2.2\n",
    "# DO NOT CHANGE THIS CELL\n",
    "!ngc registry model list nvidia/tao/* --column name --column repository --column application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e46b6d8-bc0a-4db6-b0b7-285b8d0a2b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3\n",
    "!ngc registry model <<<<FIXME>>>> nvidia/tao/<<<<FIXME>>>>:pruned_v1.0 --dest $NGC_DIR \\\n",
    "2>&1| tee my_assessment/answer_2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c7764a-2126-44a3-b4d7-b18885d923d4",
   "metadata": {},
   "source": [
    "### 第 3 步：编辑推理配置文件 ###\n",
    "下一步是修改用于配置 AI 推理插件的 Gst-nvinfer 配置文件。为此可以手动创建新文本文件，既可以从头开始创建文件，也可以使用[提供的模板](spec_files/pgie_config_dashcamnet.txt)。此外，还可以点击[此处](https://github.com/NVIDIA-AI-IOT/deepstream_python_apps)，参考示例应用和配置文件。创建配置文件时，要注意的字段如下：\n",
    "\n",
    "使用从 NGC 下载的 TAO 工具包模型时，需使用以下属性：\n",
    "* `tlt-encoded-model` - TAO 工具包编码模型的路径名。\n",
    "* `tlt-model-key` - TAO 工具包编码模型的模型加载密钥。\n",
    "* `labelfile-path` - 包含模型标签的文本文件的路径名。\n",
    "* `int8-calib-file` - INT8 校正文件的路径名，该文件可用于调整 FP32 模型的动态范围（仅支持 INT8）。\n",
    "* `uff-input-blob-name` - UFF 文件中输入 Blob 的名称。\n",
    "* `output-blob-names` - 输出层名称数组。\n",
    "* `input-dims` - 模型的维度，即[通道、高度、宽度、输入顺序]，如果输入顺序为 0，即为 NCHW。\n",
    "* `net-scale-factor` - 像素归一化因子_（默认值为 1）_。\n",
    "\n",
    "推荐属性：\n",
    "* `batch-size` - 一同接受批量推理操作的帧数_（默认值为 1）_。\n",
    "\n",
    "检测器的必要属性：\n",
    "* `num-detected-classes` - 网络检测到的类数量。\n",
    "\n",
    "检测器的可选属性：\n",
    "* `cluster-mode` - 使用的集群算法_（默认值为 0，即矩形分组）_。\n",
    "* `interval` - 跳过推理的连续批数_（仅针对主要模式，| 默认值为 0）_。\n",
    "\n",
    "其他可选属性：\n",
    "* `network-mode` - 用于推理的数据格式_（0 表示 FP32，1 表示 INT8，2 表示 FP16 模式，| 默认值为 0，即 FP32）_。\n",
    "* `process-mode` - 插件的运行模式_（主要或次要）__（默认值为 1，即主要模式）_。\n",
    "* `model-color-format` - 模型所需的色彩格式_（默认值为 0，即 RGB）_。\n",
    "* `gie-unique-id` - 要分配给 GIE 的唯一 ID，用于帮助应用和其他元件识别检测到的边界框和标签_（默认值为 0）_。\n",
    "* `model-engine-file` - 序列化模型引擎文件的路径名。\n",
    "* `gpu-id` - 执行预处理/推理所用 GPU 的设备 ID_（仅限 dGPU）_。\n",
    "\n",
    "**注意**：配置文件中的值会被通过 GObject 属性设置的值覆盖。务必谨记的另一点是：推荐的属性为主要检测器的专属属性，您需要为辅助设备或分类器配置其他属性。您可以在[模型卡](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/dashcamnet)上找到所需的大部分信息：\n",
    "\n",
    "<p><img src='images/model_card.png' width=720></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dbcf79-4e80-4312-b462-d4aac1ff87e2",
   "metadata": {},
   "source": [
    "**说明**：\n",
    "<br>\n",
    "3.1. 打开并查看[配置文件](spec_files/pgie_config_dashcamnet.txt)。<br>\n",
    "3.2. *只将* 配置文件中 `<FIXME>` 更新为正确的值，并**保存更改**。之后，确保在此单元中引用配置文件的正确路径，并执行此单元以标记您的答案。_您可以多次执行此单元，直到满意为止_ 。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e1e1e-0e46-49a3-9c34-e3f1bccee885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2\n",
    "os.environ['SPEC_FILE']='/dli/task/spec_files/pgie_config_dashcamnet.txt'\n",
    "\n",
    "# DO NOT CHANGE BELOW\n",
    "!cat $SPEC_FILE\n",
    "!cp -v $SPEC_FILE my_assessment/answer_3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a547a77-8d64-4f83-b26d-91c9ee91edf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 第 4 步：构建并运行 DeepStream 工作流 ###\n",
    "接下来，需要构建工作流。向特定函数添加工作流创建和启动过程，以便轻松调用。在运行工作流之前，还需要实施探针回调函数。我们已提供功能架构和框架，以供此应用使用。此工作流的体系架构如下。\n",
    "\n",
    "<p><img src='images/assessment_pipeline.png' width=1080></p>\n",
    "\n",
    "确定车辆是否追尾的逻辑基于下图所示的检测物体边界框的坐标：\n",
    "\n",
    "<p><img src='images/tailgate_metrics.png' width=720></p>\n",
    "\n",
    "将探针附加至 _nvdsosd_ 插件时，唯一的要求是必须将其置于 _nvinfer_ 插件之后，以便在其中加入 AI 推理元数据。如前文所述，我们需要对探针[回调函数](https://en.wikipedia.org/wiki/Callback_(computer_programming))进行编程，以使此函数在车辆可能发生追尾时向我们发送信号。探针回调函数通常按样板帮助您遍历批量数据、帧和物体。如需有关如何实现回调函数的更多信息，请参阅 [GStreamer 探针文档](https://gstreamer.freedesktop.org/documentation/additional/design/probes.html)。\n",
    "\n",
    "<p><img src='images/probe_boiler_plate.png' width=720></p>\n",
    "\n",
    "我们希望为每帧生成一个包含 0 和 1 的列表，以表示其中是否出现追尾。因此，最终应该有与帧数一样多的列表编号。单个编号_不_能与检测到的每个物体相关联，因为这将导致多个编号与每帧相关联。示例输出如下：\n",
    "\n",
    "<p><img src='images/sample_log.png' width=720></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a860bbe9-b937-47af-a305-95c00d36f580",
   "metadata": {},
   "source": [
    "**说明**：\n",
    "<br>\n",
    "4.1. 查看工作流架构。<br>\n",
    "4.2. *只将* 单元中 `<FIXME>` 修改为正确的代码，并执行此单元，以定义构建和运行工作流的函数。<br>\n",
    "4.3. *只将* 单元中 `<FIXME>` 修改为正确的代码，并执行此单元，以定义探针回调函数。<br>\n",
    "4.4. 执行此单元以运行工作流。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9d8b9c-b28b-4d8f-b226-70ca5b76e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2\n",
    "#Import necessary libraries\n",
    "import sys\n",
    "import gi\n",
    "gi.require_version('Gst', '1.0')\n",
    "from gi.repository import GObject, Gst, GLib\n",
    "from common.bus_call import bus_call\n",
    "import pyds\n",
    "\n",
    "def run(input_file_path):\n",
    "    global inference_output\n",
    "    inference_output=[]\n",
    "    Gst.init(None)\n",
    "\n",
    "    # Create element that will form a pipeline\n",
    "    print(\"Creating Pipeline\")\n",
    "    pipeline=Gst.Pipeline()\n",
    "    \n",
    "    source=Gst.ElementFactory.make(<<<<FIXME>>>>, \"file-source\")\n",
    "    source.set_property('location', <<<<FIXME>>>>)\n",
    "    h264parser=Gst.ElementFactory.make(<<<<FIXME>>>>, \"h264-parser\")\n",
    "    decoder=Gst.ElementFactory.make(\"nvv4l2decoder\", \"nvv4l2-decoder\")\n",
    "    \n",
    "    streammux=Gst.ElementFactory.make(<<<<FIXME>>>>, \"Stream-muxer\")    \n",
    "    streammux.set_property('width', <<<<FIXME>>>>)\n",
    "    streammux.set_property('height', <<<<FIXME>>>>)\n",
    "    streammux.set_property('batch-size', <<<<FIXME>>>>)\n",
    "    \n",
    "    pgie=Gst.ElementFactory.make(<<<<FIXME>>>>, \"primary-inference\")\n",
    "    pgie.set_property('config-file-path', os.environ['SPEC_FILE'])\n",
    "    \n",
    "    nvvidconv1=Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor\")\n",
    "    nvosd=Gst.ElementFactory.make(<<<<FIXME>>>>, \"onscreendisplay\")\n",
    "    nvvidconv2=Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor2\")\n",
    "    capsfilter=Gst.ElementFactory.make(\"capsfilter\", \"capsfilter\")\n",
    "    caps=Gst.Caps.from_string(\"video/x-raw, format=I420\")\n",
    "    capsfilter.set_property(\"caps\", caps)\n",
    "    \n",
    "    encoder=Gst.ElementFactory.make(\"avenc_mpeg4\", \"encoder\")\n",
    "    encoder.set_property(\"bitrate\", 2000000)\n",
    "    \n",
    "    sink=Gst.ElementFactory.make(<<<<FIXME>>>>, 'filesink')\n",
    "    sink.set_property('location', 'output.mpeg4')\n",
    "    sink.set_property(\"sync\", 1)\n",
    "    \n",
    "    # Add the elements to the pipeline\n",
    "    print(\"Adding elements to Pipeline\")\n",
    "    pipeline.add(source)\n",
    "    pipeline.add(h264parser)\n",
    "    pipeline.add(decoder)\n",
    "    pipeline.add(streammux)\n",
    "    pipeline.add(<<<<FIXME>>>>)\n",
    "    pipeline.add(nvvidconv1)\n",
    "    pipeline.add(nvosd)\n",
    "    pipeline.add(nvvidconv2)\n",
    "    pipeline.add(capsfilter)\n",
    "    pipeline.add(encoder)\n",
    "    pipeline.add(sink)\n",
    "\n",
    "    # Link the elements together\n",
    "    print(\"Linking elements in the Pipeline\")\n",
    "    source.link(h264parser)\n",
    "    h264parser.link(decoder)\n",
    "    decoder.get_static_pad('src').link(streammux.get_request_pad(\"sink_0\"))\n",
    "    streammux.link(<<<<FIXME>>>>)\n",
    "    <<<<FIXME>>>>.link(nvvidconv1)\n",
    "    nvvidconv1.link(<<<<FIXME>>>>)\n",
    "    <<<<FIXME>>>>.link(nvvidconv2)\n",
    "    nvvidconv2.link(capsfilter)\n",
    "    capsfilter.link(encoder)\n",
    "    encoder.link(sink)\n",
    "    \n",
    "    # Attach probe to OSD sink pad\n",
    "    osdsinkpad=nvosd.get_static_pad(\"sink\")\n",
    "    <<<<FIXME>>>>.add_probe(Gst.PadProbeType.BUFFER, <<<<FIXME>>>>, 0)\n",
    "\n",
    "    # Create an event loop and feed gstreamer bus mesages to it\n",
    "    loop=GLib.MainLoop()\n",
    "    bus=pipeline.get_bus()\n",
    "    bus.add_signal_watch()\n",
    "    bus.connect(\"message\", bus_call, loop)\n",
    "    \n",
    "    # Start play back and listen to events\n",
    "    print(\"Starting pipeline\")\n",
    "    \n",
    "    pipeline.set_state(Gst.State.PLAYING)\n",
    "    try:\n",
    "        loop.run()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    pipeline.set_state(Gst.State.NULL)\n",
    "    return inference_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d3a5e2-aa96-463a-b9e5-bdf69b48bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3\n",
    "# Define the Probe Function\n",
    "def osd_sink_pad_buffer_probe(pad, info, u_data):\n",
    "    gst_buffer=info.get_buffer()\n",
    "\n",
    "    # Retrieve batch metadata from the gst_buffer\n",
    "    batch_meta=pyds.gst_buffer_get_nvds_batch_meta(hash(gst_buffer))\n",
    "    l_frame=batch_meta.frame_meta_list\n",
    "    while l_frame is not None:\n",
    "        \n",
    "        # Initially set the tailgate indicator to False for each frame\n",
    "        tailgate=False\n",
    "        try:\n",
    "            frame_meta=pyds.NvDsFrameMeta.cast(l_frame.data)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        frame_number=frame_meta.frame_num\n",
    "        l_obj=frame_meta.obj_meta_list\n",
    "        \n",
    "        # Iterate through each object to check its dimension\n",
    "        while l_obj is not None:\n",
    "            try:\n",
    "                obj_meta=pyds.NvDsObjectMeta.cast(l_obj.data)\n",
    "                \n",
    "                # If the object meet the criteria then set tailgate indicator to True\n",
    "                obj_bottom=obj_meta.rect_params.top+obj_meta.rect_params.height\n",
    "                if (<<<<FIXME>>>> > FRAME_WIDTH*.3) & (<<<<FIXME>>>> > FRAME_HEIGHT*.9): \n",
    "                    tailgate=True\n",
    "                    \n",
    "            except StopIteration:\n",
    "                break\n",
    "            try: \n",
    "                l_obj=l_obj.next\n",
    "            except StopIteration:\n",
    "                break\n",
    "        \n",
    "        print(f'Analyzing frame {frame_number}', end='\\r')\n",
    "        inference_output.append(str(int(<<<<FIXME>>>>)))\n",
    "        try:\n",
    "            l_frame=l_frame.next\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return Gst.PadProbeReturn.OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fef2c16-93b6-4c59-abdc-cd4409cba488",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4.4\n",
    "tailgate_log=run(input_file_path='/dli/task/data/assessment_stream.h264')\n",
    "\n",
    "# DO NOT CHANGE BELOW\n",
    "with open('/dli/task/my_assessment/answer_4.txt', 'w') as f: \n",
    "    f.write('\\n'.join(tailgate_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92642fe5-6bbe-45eb-9ed7-5d4cce4a8a59",
   "metadata": {},
   "source": [
    "## 第 5 步：分析结果 ##\n",
    "最后，可以使用收集的日志分析驾驶行为。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11858951-c2bb-4ca1-a921-55f9287fbbd0",
   "metadata": {},
   "source": [
    "**说明**：<br>\n",
    "5.1. 执行此单元，以将追尾日志导入 Pandas DataFrame。<br>\n",
    "5.2. 执行此单元以标出发生追尾的次数。<br>\n",
    "5.3. 确保引用输出 `.mp4` 文件，并执行此单元，以查看在原始视频中绘制边界框的合成文件。<br>\n",
    "5.4. 执行此单元，以计算该车辆追尾的平均时间。<br>\n",
    "5.5. *只* 修改 `<FIXME>` ，并标记您的答案。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80436323-269b-4010-8482-167861e93d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1\n",
    "# DO NOT CHANGE THIS CELL\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('my_assessment/answer_4.txt', names=['inference'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da127db-2687-41c2-a026-8988a4bbe8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2\n",
    "# DO NOT CHANGE THIS CELL\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df.plot(kind='bar', figsize=(30, 5))\n",
    "plt.xticks(np.arange(0, len(df)+1, FRAME_RATE), np.arange(0, len(df)/FRAME_RATE))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d15a0-9374-419a-869b-b353fc35de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3\n",
    "!ffmpeg -i output.mpeg4 output_converted.mp4 \\\n",
    "        -y \\\n",
    "        -loglevel quiet\n",
    "\n",
    "# DO NOT CHANGE BELOW\n",
    "Video('output_converted.mp4', width=720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae0b43f-23e1-4ab9-8fb7-0fec6ac7caaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4\n",
    "# DO NOT CHANGE THIS CELL\n",
    "display(df['inference'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cda7d0-9fad-4687-864a-6845d3ea1a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: How much time (without the percentage sign, e.g. 5.0) did the vehicle tailgate? \n",
    "Answer=<<<<FIXME>>>>\n",
    "\n",
    "# EXAMPLE: \n",
    "# Answer='5.0'\n",
    "\n",
    "# DO NOT CHANGE BELOW\n",
    "!echo $Answer > my_assessment/answer_5.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c075a-6f39-4483-9035-af9ac9f65a5c",
   "metadata": {},
   "source": [
    "## 对代码评分 ##\n",
    "如果您已检查完这 5 个问题并确认工作流正常运行，请保存对 Notebook 的更改，并重新访问启动此交互环境所使用的网页。如下方屏幕截图所示，单击 \"**ASSESS TASK**\"（评估任务）按钮。这样，您可获得这部分实验的积分，为自己赢取整个课程的能力证书。\n",
    "\n",
    "<p><img src='images/credit.png' width=1080></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b3429-e152-4ffb-a09d-2945ab3582bc",
   "metadata": {},
   "source": [
    "### 额外练习。可视化帧 ###\n",
    "我们在下面加入了一些实用函数，可帮助您可视化显示追尾行为的帧。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e0ee86-e27c-48c5-9274-de08cd3aa6fe",
   "metadata": {},
   "source": [
    "**说明**：<br>\n",
    "B.1. 执行此单元以提取追尾帧。<br>\n",
    "B.2. 执行此单元，以随机显示所选追尾帧。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5967bf7f-4332-4965-b713-c424e62d0dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B.1\n",
    "# DO NOT CHANGE THIS CELL\n",
    "import cv2\n",
    "\n",
    "!mkdir output_images\n",
    "!rm -r output_images/*\n",
    "input_video=cv2.VideoCapture('output_converted.mp4')\n",
    "retVal, im=input_video.read()\n",
    "frameCount=0\n",
    "while retVal:\n",
    "    if frameCount in df[df['inference']==1].index:\n",
    "        cv2.imwrite(\"output_images/frame_%d.jpg\" % frameCount, im)     # save frame as JPEG file      \n",
    "    retVal, im=input_video.read()\n",
    "    print(f'Read a new frame: {frameCount}', end='\\r')\n",
    "    frameCount+=1\n",
    "input_video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a87a09-d228-4c79-b831-868ebeb58a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B.2\n",
    "# DO NOT CHANGE THIS CELL\n",
    "from PIL import Image, ImageFont, ImageDraw, ImageEnhance\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "\n",
    "def plot_random_samples(frames):\n",
    "    sample_frames = np.random.choice(frames,size=8)\n",
    "    fig=plt.figure(figsize=(30, 8))\n",
    "    columns = 4\n",
    "    rows = 2\n",
    "    i = 1 \n",
    "    for frame_num in sample_frames:\n",
    "        # im = Image.open('{}/images/{}/{}.jpg'.format(config[\"Base_Dest_Folder\"], config[\"Test_Video_ID\"], box[\"frame_no\"]))\n",
    "        im = Image.open(f'output_images/frame_{frame_num}.jpg')\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        i += 1\n",
    "        plt.imshow(np.asarray(im))\n",
    "    plt.show()\n",
    "    \n",
    "plot_random_samples(df[df['inference']==1].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2292f18c-1702-4a7c-9f30-f52768febc37",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"><img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
