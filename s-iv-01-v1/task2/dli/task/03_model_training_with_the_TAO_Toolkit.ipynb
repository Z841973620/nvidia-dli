{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8416bc8",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c19d14",
   "metadata": {},
   "source": [
    "# 使用 TAO 工具包训练模型 #\n",
    "作为回顾，此图说明了典型的模型开发工作流程。首先，准备数据和预训练模型。接着，使用新数据训练模型并评估其性能。在取得令人满意的模型性能后，将其导出。_请注意：本部分内容不包括推理优化，我们将在下个 Notebook 中介绍相关内容。_\n",
    "\n",
    "<p><img src='images/pre-trained_model_workflow.png' width=1080></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c84deb7-7cd8-49ca-8d46-2cf4fefd1122",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 学习目标 ##\n",
    "在此 Notebook 中，您将学习如何使用 TAO 工具包训练视频 AI 模型，学习内容包括：\n",
    "* 采用专用的 TrafficCamNet 模型并训练用于物体检测的自定义 ResNet18 DetectNet_v2 模型\n",
    "* 创建模型训练、评估和推理配置文件\n",
    "* 评估模型\n",
    "* 将模型部署到 DeepStream\n",
    "\n",
    "**目录** <br>\n",
    "本 Notebook 涵盖以下部分：\n",
    "1. [模型训练](#s1)\n",
    "    * [练习 #1 - 修改数据集配置](#e1) \n",
    "    * [练习 #2 - 修改数据增强的配置](#e2) \n",
    "    * [练习 #3 - 修改模型配置](#e3)\n",
    "    * [练习 #4 - 修改边界框栅格化的配置](#e4)\n",
    "    * [练习 #5 - 修改后处理配置](#e5)\n",
    "    * [练习 #6 - 修改训练配置](#e6)\n",
    "    * [代价函数配置](#s1.1)\n",
    "    * [练习 #7 - 修改评估配置](#e7)\n",
    "    * [启动模型训练](#s1.2)\n",
    "2. [评估模型](#s2)\n",
    "3. [模型推理](#s3)\n",
    "    * [练习 #8 - 修改推理器配置](#e8)\n",
    "    * [练习 #9 - 修改边界框处理器配置](#e9)\n",
    "    * [可视化推理](#s3.1)\n",
    "4. [模型导出](#s4)\n",
    "    * [将模型导出为 TensorRT 引擎](#s4.1)\n",
    "5. [部署到 DeepStream](#s5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2853ddb-59f5-443f-80b4-11458f1f0078",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s1'></a>\n",
    "## 模型训练 ##\n",
    "训练配置是通过训练规范文件完成的，这类文件中包括用于训练的数据集、用于验证的数据集、要使用的预训练模型架构、用于调优的超参数等训练选项。DetectNet_v2 实验的 `train` 和 `evaluate` 子任务共用同一个配置文件。可以从头开始创建配置文件，也可以参照 TAO 工具包的[示例应用](https://docs.nvidia.com/tao/tao-toolkit/#cv-applications)中提供的模板加以修改。\n",
    "\n",
    "训练配置是通过训练规范文件完成的，该文件包括选项，例如用于训练的数据集、用于验证的数据集、要使用的预训练模型架构、要调整的超参数以及其他训练选项。 DetectNet_v2 实验的训练和评估子任务共享相同的配置文件。 可以使用 TAO Toolkit 的示例应用程序中提供的模板从头开始创建或修改配置文件。\n",
    "\n",
    "<p><img src='images/rewind.png' width=720></p>\n",
    "\n",
    "_请注意：在使用 NGC 专用模型时，需有正确的**加密密钥**方可加载模型。用户通过通用模型开展训练时，将能够定义自己的导出加密密钥。此操作可保护专有 IP，并用于解密 DeepStream 应用中的 `.etlt` 模型。_\n",
    "\n",
    "<p><img src='images/encryption_key.png' width=540></p>\n",
    "\n",
    "训练配置文件包含八个部分：\n",
    "* `dataset_config`\n",
    "* `augmentation_config`\n",
    "* `model_config`\n",
    "* `bbox_rasterizer_config`\n",
    "* `postprocessing_config`\n",
    "* `training_config`\n",
    "* `cost_function_config`\n",
    "* `evaluation_config`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16a78b7-bbfb-45a3-b4f7-9c96f93a4829",
   "metadata": {},
   "source": [
    "<p><img src='images/important.png' width=720></p>\n",
    "我们将使用模板创建配置文件。具体而言，为便于讨论，我们已将配置文件分解为单独的部分，并会在最后将这些部分组合起来，以供 TAO 工具包使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d2b124-5b6e-425b-9233-31c180a34ae3",
   "metadata": {},
   "source": [
    "执行以下单元，预览将使用的组合训练/评估配置文件。该文件目前无法使用，因为我们有意作出了一些修改，需要加以更正。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386dc00-fc20-40ce-8bad-f3fc2f49fbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Set and create directories for the TAO Toolkit experiment\n",
    "import os\n",
    "\n",
    "os.environ['PROJECT_DIR']='/dli/task/tao_project'\n",
    "os.environ['SOURCE_DATA_DIR']='/dli/task/data'\n",
    "os.environ['DATA_DIR']='/dli/task/tao_project/data'\n",
    "os.environ['MODELS_DIR']='/dli/task/tao_project/models'\n",
    "os.environ['SPEC_FILES_DIR']='/dli/task/spec_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9d7503-4ac7-49e3-8ea0-3ee5d81caa65",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Combining configuration components in separate files and writing into one\n",
    "!cat $SPEC_FILES_DIR/dataset_config.txt \\\n",
    "     $SPEC_FILES_DIR/augmentation_config.txt \\\n",
    "     $SPEC_FILES_DIR/model_config.txt \\\n",
    "     $SPEC_FILES_DIR/bbox_rasterizer_config.txt \\\n",
    "     $SPEC_FILES_DIR/postprocessing_config.txt \\\n",
    "     $SPEC_FILES_DIR/training_config.txt \\\n",
    "     $SPEC_FILES_DIR/cost_function_config.txt \\\n",
    "     $SPEC_FILES_DIR/evaluation_config.txt \\\n",
    "     > $SPEC_FILES_DIR/combined_training_config.txt\n",
    "!cat $SPEC_FILES_DIR/combined_training_config.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e796308-689a-4db8-93b4-27bc79ad6838",
   "metadata": {},
   "source": [
    "<a name='e1'></a>\n",
    "#### 练习 #1 - 修改数据集的配置 ####\n",
    "数据加载器定义用于训练的数据的路径，以及数据集中类的类映射。我们之前为训练数据集生成了 TFRecord。要使用新生成的 TFRecord，请更新规格文件中的 `dataset_config` 参数，以引用正确目录。另一个要考虑的参数是 `validation_fold`，我们可以用 `0` 来表明 _随机拆分数据_ 。如要按序列拆分，我们可以使用从数据集转换工具生成的任何文件。\n",
    "* `data_sources (dict)`：捕获要用于训练的 TFRecord 的路径。\n",
    "    * `tfrecords_path (str)`：训练 TFRecord 根/TFRecords_name* 的路径，即 **/data/tfrecords/kitti_trainval/*** 。\n",
    "    * `image_directory_path (str)`：从中生成 TFRecord 的训练数据源的路径。\n",
    "* `image_extension (str)`：图像扩展名，无需 `.` 即可使用。\n",
    "* `target_class_mapping (dict)`：此参数会将 TFRecord 中的类名映射到要训练的目标类。该字段支持将相似的类对象结组到同一个类下。\n",
    "    * `key (str)`：TFRecord 文件中的类名称值。该值必须与数据集转换器日志中显示值的相同。\n",
    "    * `value (str)`：对应于网络的预期学习值。\n",
    "* `validation_fold (int)`：如果是 N 折的 TFRecord，您可以定义用于验证的折叠索引值。对于*随机拆分的*分区，请将验证折叠索引值强制设置为 0，因为 TFRecord 仅为 2 折。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69457b8-aa9e-472e-9bd2-f739a6d954ed",
   "metadata": {},
   "source": [
    "**练习说明**：<br>\n",
    "* 将 `<FIXME>` 更改为可接受值并**保存更改**，完成对训练配置文件 `dataset_config`[（此处）](spec_files/dataset_config.txt)部分的修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe63ca6-8489-43b4-b6f3-2e2f3d0c34c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/dataset_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4de3e7-6674-428b-8a14-5e892f87ee07",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_config: {\n",
    "#   data_sources: {\n",
    "#     tfrecords_path: \"/dli/task/tao_project/data/tfrecords/kitti_trainval/*\"\n",
    "#     image_directory_path: \"/dli/task/tao_project/data/training\"\n",
    "#   }\n",
    "#   image_extension: \"png\"\n",
    "#   target_class_mapping: {\n",
    "#       key: \"car\"\n",
    "#       value: \"car\"\n",
    "#   }\n",
    "#   validation_fold: 0\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d668e8-2f79-4a69-abe5-4ea29b7d87a8",
   "metadata": {},
   "source": [
    "点击 ... 以显示**答案**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c46b97a-ffdd-4e1e-990c-dac046ae2716",
   "metadata": {},
   "source": [
    "<a name='#e2'></a>\n",
    "#### 练习 #2 - 修改数据增强的配置 ####\n",
    "在使用自己的数据集训练和微调模型时，可以在训练时扩充数据集以引入数据的变化。这即是我们所说的**在线增强**。此操作对训练非常有用，因为数据的变化可以提高模型的整体质量，并防止[过度拟合](https://en.wikipedia.org/wiki/Overfitting)。训练深度神经网络需要大量的标注数据，这一过程可能需要手动操作，且成本高昂。此外，我们也难以估计网络可能经历的所有极端情况。TAO 工具包具备*空间增强*（旋转、调整大小、转换、剪切和翻转）、*色彩空间增强*（色调旋转、亮度偏移和对比度偏移）以及*图像模糊*功能，可用于创建合成的数据变体。\n",
    "\n",
    "如要了解其中的部分值，则需要查看[模型卡](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/trafficcamnet)。\n",
    "\n",
    "<p><img src='images/model_card_tao.png' width=720></p>\n",
    "\n",
    "* `preprocessing (dict)`：配置输入图像和真值标签预处理模组。\n",
    "    * `output_image_width (int)`：与网络输入的宽度相同。\n",
    "    * `output_image_height (int)`：与网络输入的高度相同。\n",
    "    * `output_image_channel (int)`：与网络输入 _(1, 3)_ 的通道深度相同。\n",
    "    * `min_bbox_width (float)`：意在用于训练的对象标签最小宽度。\n",
    "    * `min_bbox_height (float)`：意在用于训练的对象标签最小宽度。\n",
    "* `spatial_augmentation (dict)`：支持空间增强，例如翻转、缩放和转换。\n",
    "    * `hflip_probability (float)`：水平翻转输入图像的概率 _(0.0 - 1.0)_ 。\n",
    "    * `vflip_probability (float)`：垂直翻转输入图像的概率 _(0.0 - 1.0)_ 。\n",
    "    * `zoom_min (float)`：输入图像的最小缩放比例 _(> 0.0)_ 。\n",
    "    * `zoom_max (float)`：输入图像的最大缩放比例 _(> 0.0)_ 。\n",
    "    * `translate_max_x (int)`：在 x 轴中添加的最大转换值 _(0.0 - output_image_width)_ 。\n",
    "    * `translate_max_y (int)`：在 y 轴中添加的最大转换值 _(0.0 - output_image_height)_ 。\n",
    "    * `rotate_rad_max (float)`：应用于图像和训练标签的旋转角度 _(> 0.0)_ 。\n",
    "* `color_augmentation (dict)`：配置色彩空间转换。\n",
    "    * `color_shift_stddev (float)`：色移的标准偏差值 _(0.0 - 1.0)_ 。\n",
    "    * `hue_rotation_max (float)`：色调旋转矩阵的最大旋转角度 _(0.0 - 360.0)_ 。\n",
    "    * `saturation_shift_max (float)`：更改饱和度的最大偏移值 _(0.0 - 1.0)_ 。\n",
    "    * `contrast_scale_max (float)`：围绕给定中心旋转的对比度斜率 _(0.0 - 1.0)_ 。\n",
    "    * `contrast_center (float)`：对比度围绕其旋转的中心 _（设置为 0.5）_ 。\n",
    "\n",
    "**注意**：如果预处理块的输出图像高度和输出图像宽度与生成 TFRecord 时提到的输入图像的高度和宽度不符，系统会随机填充或裁剪图像以使之符合输入分辨率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1602409-fbc5-4498-b8e1-ecd38db747e5",
   "metadata": {},
   "source": [
    "**练习说明**：<br>\n",
    "* 将 `<FIXME>` 更改为可接受值并**保存更改**，完成对训练配置文件 `augmentation_config`[（此处）](spec_files/augmentation_config.txt)部分的修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c968b92-aad8-4617-bcd1-ccedd2943164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/augmentation_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449d4678-838f-4ba9-aa77-20d300de306c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# augmentation_config: {\n",
    "#   preprocessing: {\n",
    "#     output_image_width: 960\n",
    "#     output_image_height: 544\n",
    "#     output_image_channel: 3\n",
    "#     min_bbox_width: 1.0\n",
    "#     min_bbox_height: 1.0\n",
    "#   }\n",
    "#   spatial_augmentation: {\n",
    "#     hflip_probability: 0.5\n",
    "#     vflip_probability: 0.5\n",
    "#     zoom_min: 1.0\n",
    "#     zoom_max: 1.0\n",
    "#     translate_max_x: 8.0\n",
    "#     translate_max_y: 8.0\n",
    "#   }\n",
    "#   color_augmentation: {\n",
    "#     color_shift_stddev: 0.0\n",
    "#     hue_rotation_max: 25.0\n",
    "#     saturation_shift_max: 0.2\n",
    "#     contrast_scale_max: 0.1\n",
    "#     contrast_center: 0.5\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ce5c51-2573-4131-bb1c-0bd63fad1327",
   "metadata": {},
   "source": [
    "点击 ... 以显示**解决方案**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca5d5e-cac7-4be6-afc3-6f7a34511419",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='e3'></a>\n",
    "#### 练习 #3 - 修改模型的配置 ####\n",
    "可以使用规格文件中的 `model_config` 选项配置核心物体检测模型。\n",
    "* `arch (str)`：主干特征提取器的体系架构 _（默认值为\"resnet\"）_ 。\n",
    "* `pretrained_model_file (str)`：预训练 TAO 模型文件的路径。\n",
    "* `num_layers (int)`：可扩展模板特征提取器的深度。\n",
    "* `use_pooling (bool)`：在缩减取样时选择使用“跨步卷积”或“最大池化”。我们建议将此项设置为 `false` 并使用跨步卷积。\n",
    "* `objective_set (dict)`：网络训练目标。对于物体检测网络，将其设置为学习 `cov` 和 `bbox`。这些设置不应改变。\n",
    "    * `bbox`\n",
    "        * `scale`: `35.0`\n",
    "        * `offset`: `0.5`\n",
    "    * `cov`\n",
    "* `dropout_rate (float)`：丢弃概率 _(0.0 - 1.0)_ 。\n",
    "* `load_graph (bool)`：用以确定是否从预训练模型加载图形或是否仅加载权重的标记。对于已剪枝的模型，请将此参数设置为 `true`，因为需要同时导入模型图形和权重。\n",
    "* `freeze_blocks (float)`：定义或需从实例化特征提取器模板中冻结的块。在神经网络上下文中冻结层是为了控制权重是否更新。某个层（如特征提取器中的层）遭到冻结时，即表示无法进一步修改权重。此技术用于缩短训练的计算时间。如需了解有关冻结层的更多信息，请参阅 [TensorFlow 的迁移学习和微调指南](https://www.tensorflow.org/guide/keras/transfer_learning#freezing_layers_understanding_the_trainable_attribute)。\n",
    "* `use_batch_norm (bool)`：用以确定是否使用了批量归一化的标记 _（默认=false）_ 。批量归一化是一种用于训练深度较高的神经网络（如我们正在使用的此类网络）的技术，能够对每个小批量的输入层执行标准化处理。该技术旨在应对**内部协变量偏移**，当每个小批量的权重更新后，网络中各深度层的输入分布发生变化时，就会出现这种情况。批量归一化可以稳定学习过程，并显著减少训练深度网络所需的训练次数。您可以参阅 [TensorFlow 的 API 文档](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization)，了解更多有关分批标准化的信息。\n",
    "* `freeze_bn (bool)`：确定是否在训练期间冻结模组中的批量归一化层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa84e1dd-56bc-46b4-8326-8526061a1caf",
   "metadata": {},
   "source": [
    "**练习说明**：<br>\n",
    "* 将 `<FIXME>` 更改为可接受值并**保存更改**，完成对训练配置文件 `model_config`[（此处）](spec_files/model_config.txt)部分的修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df3b022-885a-4c4b-af91-397801039374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/model_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c356227-cf5e-4827-8aec-aeb259826198",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_config: {\n",
    "#   arch: \"resnet\"\n",
    "#   pretrained_model_file: \"/dli/task/tao_project/models/trafficcamnet_vunpruned_v1.0/resnet18_trafficcamnet.tlt\"\n",
    "#   freeze_blocks: 0\n",
    "#   freeze_blocks: 1\n",
    "#   num_layers: 18\n",
    "#   use_pooling: false\n",
    "#   use_batch_norm: true\n",
    "#   dropout_rate: 0.0\n",
    "#   objective_set: {\n",
    "#     cov: {}\n",
    "#     bbox: {\n",
    "#       scale: 35.0\n",
    "#       offset: 0.5\n",
    "#     }\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c502c-5d82-4670-9bf4-9bcb14d1022b",
   "metadata": {},
   "source": [
    "点击 ... 以显示**解决方案**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef44879-74a2-4e67-ae8c-3756e9afc860",
   "metadata": {},
   "source": [
    "<a name='e4'></a>\n",
    "#### 练习 #4 - 修改边界框格栅化的配置 ####\n",
    "DetectNet_v2 生成 2 个张量：`cov` 和 `bbox`。系统会将图像分成 16x16 网格单元。`cov` (_coverage_) 张量定义了单个物体所覆盖的网格单元数量。`bbox` 张量定义了物体左上角和右下角相对于网格单元的标准化图像坐标。\n",
    "* `deadzone radius (float)`：物体椭圆区域周围被视为静止状态的区域。这在物体重叠的情况下非常有用，可避免混淆前景物体和背景物体 _(0.0 - 1.0)_ 。\n",
    "* `target_class_config (dict)`：定义给定类物体的覆盖区域，并为每个类重复。\n",
    "    * `cov_center_x (float)`：物体中心的 x 坐标 _(0.0 - 1.0)_ 。\n",
    "    * `cov_center_y (float)`：物体中心的 y 坐标 _(0.0 - 1.0)_ 。\n",
    "    * `cov_radius_x (float)`：覆盖区域椭圆的 X 半径 _(0.0 - 1.0)_ 。\n",
    "    * `cov_radius_y (float)`：覆盖区域椭圆的 y 半径 _(0.0 - 1.0)_ 。\n",
    "    * `bbox_min_radius (float)`：要为框绘制的覆盖区域的最小半径 _(0.0 - 1.0)_ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a58821-138b-43f8-87e9-6091af651dc1",
   "metadata": {},
   "source": [
    "**练习说明**：<br>\n",
    "* 将 `<FIXME>` 更改为可接受值并**保存更改**，完成对训练配置文件 `bbox_rasterizer_config`[（此处）](spec_files/bbox_rasterizer_config.txt)部分的修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaeae88-3725-41b4-b71f-d77181b13ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/bbox_rasterizer_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc03c8e-57f5-4f19-b697-cfa2961ac3b5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bbox_rasterizer_config: {\n",
    "#   target_class_config: {\n",
    "#     key: \"car\"\n",
    "#     value: {\n",
    "#       cov_center_x: 0.5\n",
    "#       cov_center_y: 0.5\n",
    "#       cov_radius_x: 0.4\n",
    "#       cov_radius_y: 0.4\n",
    "#       bbox_min_radius: 1.0\n",
    "#     }\n",
    "#   }\n",
    "#   deadzone_radius: 0.4\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a5a15-47e9-429c-894e-a6170110c7fd",
   "metadata": {},
   "source": [
    "点击 ... 以显示**解决方案**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16faa94c-ca78-478e-99de-280806e658e0",
   "metadata": {},
   "source": [
    "<a name='e5'></a>\n",
    "#### 练习 #5 - 修改后处理配置 ####\n",
    "后处理器模组在原始检测输出中生成可渲染的边界框。此过程包括：\n",
    "1. 使用覆盖区域张量中的置信度值为物体进行阈值化处理，从而过滤出有效的检测结果。\n",
    "2. 使用 DBSCAN 聚类分析原始过滤预测结果，生成最终渲染的边界框。\n",
    "3. 根据聚类到同一个分类的候选框获得最终的置信度阈值，并根据这个阈值过滤掉置信度较低的聚类。\n",
    "* `target_class_config (dict)`：对于正在训练的每个类，`postprocessing_config` 具有 `target_class_config` 元素，用以定义该类的聚类参数。\n",
    "    * `key (str)`：分类名称。\n",
    "    * `value (dict)`：配置后处理器模组的聚类分析 config proto 参数。\n",
    "        * `cluster_config (dict)`: \n",
    "            * `coverage_threshold (float)`：将覆盖范围张量的输出视为聚类有效候选框所需到达到的最小阈值 _(0.0 - 1.0)_ 。\n",
    "            * `dbscan_eps (float)`：两个样本之间的最大距离，将其中一个样本视为在另一个样本的附近。dbscan_eps 越大，分组到一起的框越多 _(0.0 - 1.0)_ 。\n",
    "            * `dbscan_min_samples (float)`：一个点在邻域中被视为核心点所需达到的总权重 _(0.0 - 1.0)_ 。\n",
    "            * `minimum_bounding_box_height (int)`：视为有效检测后聚类的最小高度（以像素为单位）_（0 - 输入图像高度）_ 。\n",
    "            * `clustering_algorithm (enum)`：定义后处理算法（DBSCAN，即基于密度的聚类算法；NMS，即非极大值抑制；或混合方法），以将原始检测结果聚类到最终的边界框渲染（默认 = DBSCAN）。使用混合模式时，请确保定义 DBSCAN 和 NMS 配置参数。\n",
    "            * `DBSCAN` -`dbscan_confidence_threshold (float)`：用于从 DBSCAN 过滤出聚类边界框输出的置信阈值 _(> 0.0，默认=0.1)_ 。\n",
    "            * `NMS` - `nms_iou_threshold (float)`：交并比阈值，以过滤出原始检测中的冗余框，形成最终的聚类输出 _(0.0 - 1.0，默认=0.2)_ 。\n",
    "            * `NMS` -`nms_confidence_threshold (float)`：用于从 NMS 过滤出聚类边界框输出的置信阈值 _(0.0 - 1.0，默认=0)_ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f54985-f042-4b15-b9e9-6db84f53fc73",
   "metadata": {},
   "source": [
    "**练习说明**：<br>\n",
    "* 将 `<FIXME>` 更改为可接受值并**保存更改**，完成对训练配置文件 `postprocessing_config`[（此处）](spec_files/postprocessing_config.txt)部分的修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f53439-a7b7-4b98-8fcb-7255d3f0e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/postprocessing_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8389bba-5259-493a-af4d-2418c91682df",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# postprocessing_config: {\n",
    "#   target_class_config: {\n",
    "#     key: \"car\"\n",
    "#     value: {\n",
    "#       clustering_config: {\n",
    "#         coverage_threshold: 0.005\n",
    "#         dbscan_eps: 0.15\n",
    "#         dbscan_min_samples: 0.05\n",
    "#         minimum_bounding_box_height: 20\n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e476808c-457c-423a-a75e-167d37537a9c",
   "metadata": {},
   "source": [
    "点击 ... 以显示**解决方案**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab27d0b-71a8-422b-8e41-0f5c81789999",
   "metadata": {},
   "source": [
    "<a name='e6'></a>\n",
    "#### 练习 #6 - 修改训练配置 ####\n",
    "`training_config` 介绍训练和学习流程。\n",
    "* `batch_size_per_gpu (int)`：每个 GPU 每批的图像数量 _(> 1)_ 。\n",
    "* `num_epochs (int)`：运行实验的总迭代次数。\n",
    "* `learning_rate (dict)`：定义学习率时间表。目前 DetectNet_v2 只支持：\n",
    "    * `soft_start_annealing_schedule (dict)`，使用以下方式进行配置：\n",
    "        * `soft_start (float)`：将学习率从最低学习率提升至最高学习率的时间 _(0.0 - 1.0)_ 。\n",
    "        * `annealing (float`：将学习率从最高学习率冷却至最低学习率的时间 _(0.0 - 1.0)_ 。\n",
    "        * `minimum learning rate (float)`：学习率时间表中的最低学习率 _(0.0 - 1.0)_ 。\n",
    "        * `maximum learning rate (float)`：学习率时间表中的最高学习率 _(0.0 - 1.0)_ 。\n",
    "* `regularizer (dict)`：训练期间要使用的正则化的类型和权重。**建议从低正则化权重开始并逐渐对其进行微调，以缩小训练和验证准确度之间的差距。此外，根据我们的实验结果，L1 似乎为我们提供了更好的剪枝比。**\n",
    "    * `type (enum)`：支持的类型为 _（NO_REG、L1、L2）_ 。\n",
    "    * `weight (float)`：正则化器的权重。\n",
    "* `optimizer (dict)`：用于训练的优化器。\n",
    "    * `adam (dict)`\n",
    "        * `epsilon (float)`：避免在实现中被零除的小数字。\n",
    "        * `beta1 (float)`\n",
    "        * `beta2 (float)`\n",
    "* `cost_scaling (dict)`：在训练期间启用成本扩展。**目前请为 DetectNet_v2 训练通道保持此参数不变**。\n",
    "    * `enabled`: false。\n",
    "    * `initial_exponent`: 20.0。\n",
    "    * `increment`: 0.005。\n",
    "    * `derement`: 1.0。\n",
    "* `checkpoint_interval (int)`：`train` 保存中间模型的时间间隔（迭代次数） _(0 - num_epochs)_ 。\n",
    "* `enable_qat (bool)`：使用量化感知训练 (QAT) 启用模型训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6bb4eb-7b55-4e52-b890-509374e1b1cf",
   "metadata": {},
   "source": [
    "**练习说明**：<br>\n",
    "* 将 `<FIXME>` 更改为可接受值并保存更改，完成对训练配置文件 `training_config`[（此处）](spec_files/training_config.txt)部分的修改。先选择较低的 `num_epochs` 值（推荐使用 10）执行迭代，因为迭代次数越多，训练时间越长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0f839e-d23b-4337-a853-7565039612a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/training_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7c7af-8a93-46d7-8ea9-1429b0c6a095",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training_config: {\n",
    "#   batch_size_per_gpu: 16\n",
    "#   num_epochs: 10\n",
    "#   learning_rate: {\n",
    "#     soft_start_annealing_schedule: {\n",
    "#       min_learning_rate: 5e-6\n",
    "#       max_learning_rate: 5e-4\n",
    "#       soft_start: 0.1\n",
    "#       annealing: 0.7\n",
    "#     }\n",
    "#   }\n",
    "#   regularizer: {\n",
    "#     type: L1\n",
    "#     weight: 3e-9\n",
    "#   }\n",
    "#   optimizer: {\n",
    "#     adam: {\n",
    "#       epsilon: 1e-08\n",
    "#       beta1: 0.9\n",
    "#       beta2: 0.999\n",
    "#     }\n",
    "#   }\n",
    "#   cost_scaling: {\n",
    "#     enabled: false\n",
    "#     initial_exponent: 20.0\n",
    "#     increment: 0.005\n",
    "#     decrement: 1.0\n",
    "#   }\n",
    "#   checkpoint_interval: 5\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97e91b3-ccb9-4a13-ac27-66a003a7a738",
   "metadata": {},
   "source": [
    "点击 ... 以显示**解决方案**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70f23b4-fff4-406c-bfa5-eca8f9df5b41",
   "metadata": {},
   "source": [
    "<a name='s1.1'></a>\n",
    "### 代价函数的配置 ###\n",
    "代价函数描述每个类的训练方式。为获得最佳性能，除确保每个目标类都有一个条目外，我们不需要更改规格文件中的参数。此处的其他参数应保持不变。以下是示例配置文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5205d3cf-e1cf-408b-b5f7-631764131bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/cost_function_config.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033d398e-5474-4b3a-9a3b-a297973e8c22",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='e7'></a>\n",
    "#### 练习 #7 - 修改评估配置 ####\n",
    "可以使用 `evaluation_config` 参数配置评估器。\n",
    "* `average_precision_mode (enum)`：计算平均精度的模型 _（示例或集成）_ 。\n",
    "* `validation_period_during_training (int)`：训练期间执行评估的时间间隔 _（1 - 迭代总次数）_ 。\n",
    "* `first_validation_epoch (int)`：开始运行验证的首轮迭代 _（1 - 迭代总次数）_ 。\n",
    "* `minimum_detection_ground_truth_overlap (dict)`：执行聚类分析后，真值与预测框之间的最小交并比，用以调用有效检测。\n",
    "    * `key (str)`：类名称。\n",
    "    * `value (float)`：交并比值。\n",
    "* `evaluation_box_config (dict)`：将最小框和最大框维度配置为有效真值和预测值，以执行平均精度计算。\n",
    "    * `minimum_height (float)`：有效真值和预测边界框中的最低高度（以像素为单位）_（0.0 - 模型图像高度）_ 。\n",
    "    * `minimum_width (float)`：有效真值和预测边界框中的最低宽度（以像素为单位）_（0.0 - 模型图像宽度）_ 。\n",
    "    * `maximum_height (float)`：有效真值和预测边界框中的最高高度（以像素为单位）_（minimum_height - 模型图像高度）_ 。\n",
    "    * `maximum_width (float)`：有效真值和预测边界框中的最高宽度（以像素为单位）_（minimum_width - 模型图像宽度）_ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0204f62-8e6d-4bb4-ad39-c5f79b261dcc",
   "metadata": {},
   "source": [
    "**练习说明**：<br>\n",
    "* 将 `<FIXME>` 更改为可接受值并**保存更改**，完成对训练配置文件 `evaluation_config`[（此处）](spec_files/evaluation_config.txt)部分的修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209b1c26-fefe-4631-a0ad-767c8e55e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/evaluation_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb449f-f02d-4b34-a43f-8742eaa8afaf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluation_config: {\n",
    "#   average_precision_mode: INTEGRATE\n",
    "#   validation_period_during_training: 5\n",
    "#   first_validation_epoch: 1\n",
    "#   minimum_detection_ground_truth_overlap: {\n",
    "#     key: \"car\"\n",
    "#     value: 0.7\n",
    "#   }\n",
    "#   evaluation_box_config {\n",
    "#     key: \"car\"\n",
    "#     value: {\n",
    "#       minimum_height: 4\n",
    "#       maximum_height: 9999\n",
    "#       minimum_width: 4\n",
    "#       maximum_width: 9999\n",
    "#     }\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8c6c6e-5907-4aa4-9418-607fc763c374",
   "metadata": {},
   "source": [
    "点击 ... 以显示**解决方案**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c484eb73-a9b4-436b-9a21-6375a89d3a7c",
   "metadata": {},
   "source": [
    "<a name='s1.2'></a>\n",
    "### 启动模型训练 ###\n",
    "使用 `train` 子任务时，`-e` 参数表示规格文件的路径，`-r` 参数表示结果目录，`-k` 表示*加载*预训练权重的密钥，`-n` 表示已保存的最后一步模型的名称。\n",
    "\n",
    "**注意**：训练可能需要数小时才能完成。Detectnet_v2 支持从检查点重新启动，以防训练作业过早终止。只需重新运行**同**一命令行，即可从距离最近的检查点处恢复训练。\n",
    "\n",
    "_可以使用 `--gpus` 参数为具有硬件的训练启用多 GPU 支持。在使用多个 GPU 运行训练时，我们需要修改 `batch_size_per_gpu` 和 `learning_rate`，以获得与 1 个 GPU 训练类似的 mAP。在大多数情况下，将批量大小缩小 NUM_GPU 倍或将学习率扩大 NUM_GPU 倍不失为一个不错的方法。_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3383f257-f83b-47e2-b10a-699059786cef",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Combining configuration components in separate files and writing into one\n",
    "!cat $SPEC_FILES_DIR/dataset_config.txt \\\n",
    "     $SPEC_FILES_DIR/augmentation_config.txt \\\n",
    "     $SPEC_FILES_DIR/model_config.txt \\\n",
    "     $SPEC_FILES_DIR/bbox_rasterizer_config.txt \\\n",
    "     $SPEC_FILES_DIR/postprocessing_config.txt \\\n",
    "     $SPEC_FILES_DIR/training_config.txt \\\n",
    "     $SPEC_FILES_DIR/cost_function_config.txt \\\n",
    "     $SPEC_FILES_DIR/evaluation_config.txt \\\n",
    "     > $SPEC_FILES_DIR/combined_training_config.txt\n",
    "!cat $SPEC_FILES_DIR/combined_training_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a63a82-9e8b-42b1-a5ef-07dccb5e72e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View train usage\n",
    "!detectnet_v2 train --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b42c75-5f25-4c74-b4e5-2f4f4f558d2b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Initiate the training process\n",
    "!detectnet_v2 train -e $SPEC_FILES_DIR/combined_training_config.txt \\\n",
    "                    -r $MODELS_DIR/resnet18_detector \\\n",
    "                    -k tlt_encode \\\n",
    "                    -n resnet18_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f31fac3-a7fc-4d25-986e-e2d1a4210c17",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s2'></a>\n",
    "## 评估模型 ##\n",
    "我们会在训练结束时和按特定时间间隔评估模型性能，评估可使用 `evaluation_config` 进行配置。用户可通过评估配置选择用于评估的数据集以及评估指标。我们还可以使用 `evaluate` 子任务来评估模型。该 `evaluate` 子任务在训练期间使用的相同验证集上运行评估，但可以进行更新，以在 `dataset_config` 中包含测试数据集。我们还可以通过编辑规格文件以指向目标模型，进而评估早期模型。使用 `evluate` 子任务时，`-e` 参数表示规格文件的路径，`-m` 参数表示模型的路径，`-k` 参数表示_加载_模型的密钥。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615059b8-99b4-4286-9bfb-2466c3e22bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View saved weights\n",
    "print('Model for Each Epoch:')\n",
    "print('---------------------')\n",
    "\n",
    "!ls -lh $MODELS_DIR/resnet18_detector/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e220a7a5-828c-4245-8c28-27479d35d515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View evaluate usage\n",
    "!detectnet_v2 evaluate --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98673d92-7ebe-48cd-841e-323481cfccbb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Evaluate the model using the same validation set as training\n",
    "!detectnet_v2 evaluate -e $SPEC_FILES_DIR/combined_training_config.txt\\\n",
    "                       -m $MODELS_DIR/resnet18_detector/weights/resnet18_detector.tlt \\\n",
    "                       -k tlt_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc27f7f-67b0-41fe-ae8a-795863680ecd",
   "metadata": {},
   "source": [
    "**观察**：<br>\n",
    "请记下此模型的 mAP，我们在相当短的时间内训练了该模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c9188e-6929-41cc-a478-10076e17639c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s3'></a>\n",
    "## 模型推理 ##\n",
    "`infer` 子任务可用于对单张图像或图像目录上的边界框进行可视化处理。这是一项可选操作，但我们强烈建议您在部署模型之前执行此操作。`infer` 子任务会在 `output_path/images_annotated` 目录中生成边界框渲染图像，并在 `output_path/labels` 目录中以 KITTIE 格式生成边界框标签。推理可能需要花费一些时间，具体时长视验证数据集大小而定。\n",
    "\n",
    "推理规格文件用于选择运行推理的选项。这份文件由两个模块组成：\n",
    "* inferencer_config\n",
    "* bbox_handler_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f47762-2952-4f6e-a36f-158b6d6b3173",
   "metadata": {},
   "source": [
    "<a name='e8'></a>\n",
    "#### 练习 #8 - 修改推理器的配置 ####\n",
    "推理器可实例化模型对象和预处理管道。\n",
    "* `inferencer_config (dict)`: \n",
    "    * `target_classes (str)`：模型应输出的目标类的名称。对于多类模型，此参数会重复。顺序必须与训练配置文件 cost_function_config 中的类相同。\n",
    "    * `batch_size (int)`：每个推理批次的图像数。\n",
    "    * `image_height (int)`：模型将推理的图像高度（以像素为单位）。\n",
    "    * `image_width (int)`：模型将推理的图像宽度（以像素为单位）。\n",
    "    * `image_channel (int)`：每张图像的通道数。\n",
    "    * `tlt_config (dict)`：用以实例化模型对象的 Proto 配置。\n",
    "        * `model (str)`：`.tlt` 模型文件的路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758df62d-d4af-4c11-832b-7f86b3354c0b",
   "metadata": {},
   "source": [
    "**练习说明**：<br>\n",
    "* 将 `<FIXME>` 更改为可接受值并**保存更改**，完成对推理配置文件 `inferencer_config`[（此处）](spec_files/inferencer_config.txt)部分的修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a18c15-c679-4980-9313-302e36520e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/inferencer_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfdac52-b7a7-4611-b7d2-dca566fb89ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inferencer_config: {\n",
    "#   target_classes: \"car\"\n",
    "#   image_width: 882\n",
    "#   image_height: 692\n",
    "#   image_channels: 3\n",
    "#   batch_size: 16\n",
    "#   tlt_config: {\n",
    "#     model: \"/dli/task/tao_project/models/resnet18_detector/weights/resnet18_detector.tlt\"\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1271f58-4acb-44ab-a467-2b3335799b76",
   "metadata": {},
   "source": [
    "点击 ... 以显示**解决方案**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6052b2fe-85e7-4b35-863a-deb9891670a9",
   "metadata": {},
   "source": [
    "<a name='e9'></a>\n",
    "#### 练习 #9 - 修改边界框处理器的配置 ####\n",
    "边界框处理器负责处理后处理、边界框渲染，以及序列化到 KITTI 格式的输出标签。执行的处理步骤如下：\n",
    "1. 对原始输出进行阈值化，以定义每个类可能存在检测的网格单元。\n",
    "2. 根据推理器的原始坐标重建图像空间坐标。\n",
    "3. 聚类分析原始阈值预测。\n",
    "4. 按类别过滤各类经聚类分析的预测。\n",
    "5. 以输入维度渲染图像上的最终边界框，并将其序列化为 KITTI 格式的元数据。\n",
    "* `bbox_handler_config (dict)`: \n",
    "    * `kitti_dump (bool)`：启用以 KITTI 格式保存每张图像最终输出预测的标记。\n",
    "    * `disable_overlay (bool)`：禁用每个图像的边界框渲染的标记。\n",
    "    * `overlay_linewidth (int)`：边界框边界的厚度（以像素为单位）。\n",
    "    * `classwise_bbox_handler_config (dict)`：包含用于配置聚类算法和边界框渲染器参数的 Proto 对象。需为每个类重复这一操作。\n",
    "        * `key (str)`：分类名称。\n",
    "        * `value (dict)`：配置后处理器模组的聚类分析 config proto 参数。\n",
    "            * `confidence_model (str)`：用以计算聚类边界框最终置信度的算法 _（\"aggregate_cov\" 或 \"mean_cov\"）_ 。\n",
    "            * `bbox_color (dict)`：每个框的 RGB 通道色彩强度。\n",
    "                * `R (int)`: _(0 - 255)_ 。\n",
    "                * `G (int)`: _(0 - 255)_ 。\n",
    "                * `B (int)`: _(0 - 255)_ 。\n",
    "            * `cluster_config (dict)`:  \n",
    "                * `coverage_threshold (float)`：将覆盖范围张量输出视为聚类有效候选框所需到达到的最小阈值 _(0.0 - 1.0)_ 。\n",
    "                * `dbscan_eps (float)`：两个样本之间的最大距离，将其中一个样本视为在另一个样本的附近。dbscan_eps 越大，分组到一起的框越多 _(0.0 - 1.0)_ 。\n",
    "                * `dbscan_min_samples (float)`：一个点在邻域中被视为核心点所需达到的总权重 *(0.0 - 1.0)* 。\n",
    "                * `minimum_bounding_box_height (int)`：视为有效检测后聚类的最小高度（以像素为单位）_（0 - 输入图像高度）_ 。\n",
    "                * `clustering_algorithm (enum)`：定义后处理算法（DBSCAN，即基于密度的聚类算法；NMS，即非极大值抑制；或混合方法），以将原始检测结果集群到最终的边界框渲染（默认 = DBSCAN）。使用混合模式时，请确保定义 DBSCAN 和 NMS 配置参数。\n",
    "                * `DBSCAN` -`dbscan_confidence_threshold (float)`：用于从 DBSCAN 过滤出聚类边界框输出的置信阈值 _（> 0.0，默认=0.1）_ 。\n",
    "                * `NMS` - `nms_iou_threshold (float)`：交并比阈值，以过滤出原始检测中的冗余框，形成最终聚类输出 _（（(0.0 - 1.0)，默认=0.2）_ 。\n",
    "                * `NMS` -`nms_confidence_threshold (float)`：用于从 NMS 过滤出聚类边界框输出的置信阈值 _（0.0 - 1.0，默认=0）_ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfee89d0-91fb-4899-9673-c4e5b0c01b6f",
   "metadata": {},
   "source": [
    "**练习说明**：<br>\n",
    "* 将 `<FIXME>` 更改为可接受值并**保存更改**，完成对推理配置文件 `bbox_handler_config`[（此处）](spec_files/bbox_handler_config.txt)部分的修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e86325-ef8c-4e25-a5e2-5ebe1d7482f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/bbox_handler_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1362e7-938f-40eb-813d-a42efedb5b13",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bbox_handler_config: {\n",
    "#   kitti_dump: false\n",
    "#   disable_overlay: false\n",
    "#   overlay_linewidth: 2\n",
    "#   classwise_bbox_handler_config: {\n",
    "#     key:\"car\"\n",
    "#     value: {\n",
    "#       confidence_model: \"aggregate_cov\"\n",
    "#       bbox_color: {\n",
    "#         R: 0\n",
    "#         G: 255\n",
    "#         B: 0\n",
    "#       }\n",
    "#       clustering_config: {\n",
    "#         clustering_algorithm: DBSCAN\n",
    "#         coverage_threshold: 0.005\n",
    "#         dbscan_eps: 0.15\n",
    "#         dbscan_min_samples: 0.05\n",
    "#         minimum_bounding_box_height: 20        \n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b7eeb5-1bd7-4a29-844b-d9be389e3e89",
   "metadata": {},
   "source": [
    "点击 ... 以显示**解决方案**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae423643-2315-448c-9820-75e57fb62e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Combining configuration components in separate files and writing into one\n",
    "!cat $SPEC_FILES_DIR/inferencer_config.txt \\\n",
    "     $SPEC_FILES_DIR/bbox_handler_config.txt \\\n",
    "     > $SPEC_FILES_DIR/combined_inference_config.txt\n",
    "!cat $SPEC_FILES_DIR/combined_inference_config.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8f8258-d995-475d-9cad-c40b3b7da8c7",
   "metadata": {},
   "source": [
    "使用 `inference` 子任务时，`-e` 参数表示推理规格文件的路径，`i` 参数表示输入图像目录的路径，`-o` 参数表示输出图像目录的路径，`-k` 参数表示*加载*模型的密钥。如要对单张图像运行推理，只需将 `inference` 子任务 `-i` 参数的路径替换为图像的路径即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ada339-ec6e-4e52-8ba7-71729b86dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View inference usage\n",
    "!detectnet_v2 inference --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fe56a3-f1d2-4901-a29d-ed33633e25b3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Perform inference on the validation set\n",
    "!detectnet_v2 inference -e $SPEC_FILES_DIR/combined_inference_config.txt \\\n",
    "                        -o $PROJECT_DIR/tao_infer_testing \\\n",
    "                        -i $DATA_DIR/training/images \\\n",
    "                        -k tlt_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3ab2c1-8f90-4533-b0f0-aeb8d2917698",
   "metadata": {},
   "source": [
    "<a name='s3.1'></a>\n",
    "### 可视化推理 ###\n",
    "我们可以编写一个快速函数，帮助我们对随机推理进行抽样。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c4827c-c1b5-4103-99ec-94788791af3c",
   "metadata": {},
   "source": [
    "执行以下单元格以可视化推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea3479-e3de-4008-999d-4987e09eac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Simple grid visualizer\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from math import ceil\n",
    "import random\n",
    "\n",
    "def visualize_images(image_dir, num_cols=4, num_images=10):\n",
    "    output_path = os.path.join(os.environ['PROJECT_DIR'], image_dir)\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(output_path, image) for image in os.listdir(output_path)]\n",
    "    for idx, img_path in enumerate(random.sample(a, num_images)):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx // num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b827f734-3ac9-44fc-9c87-e4d28b2e4952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Visualizing the random 12 images\n",
    "OUTPUT_PATH = 'tao_infer_testing/images_annotated' # relative path from $USER_EXPERIMENT_DIR.\n",
    "COLS = 4 # number of columns in the visualizer grid.\n",
    "IMAGES = 12 # number of images to visualize.\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38513719-f295-44d1-825a-f994ca6fbcbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s4'></a>\n",
    "## 模型导出 ##\n",
    "可结合使用 `.tlt` 模型与 TAO 工具包以进行推理，但 DeepStream 不支持直接使用。相反，TAO 工具包支持使用 `export` 子任务导出并准备训练模型，以部署到 DeepStream。\n",
    "\n",
    "我们有两种方式可以在 DeepStream 上部署由 TAO 工具包训练的模型：\n",
    "1. 像以前一样，使用 `export` 并将其集成到 DeepStream 应用中，以生成 `.etlt` 模型或加密的 TAO 文件。DeepStream 将生成 TensorRT 引擎文件，然后运行推理。\n",
    "2. 生成特定于设备优化的 TensorRT 引擎，这可以是 `.trt` 或 `.engine` 文件，并将其集成到 DeepStream 中。\n",
    "\n",
    "<p><img src='images/important.png' width=720></p>\n",
    "\n",
    "_TensorRT 引擎使用针对特定的机器进行的优化，这个优化是每个硬件配置独有的，因此应针对每个推理环境生成。通常，模型训练环境将拥有比部署环境更多的计算资源。除非部署硬件与训练 GPU 相同，否则使用 TAO 工具包生成的 TensorRT 引擎文件无法用于部署。另一方面，该 `.etlt` 文件可广泛用于训练和部署硬件。_\n",
    "\n",
    "使用 `export` 子任务时，`-m` 参数表示要导出的 `.tlt` 模型文件的路径，`-o` 参数表示已导出 `.etlt` 模型的保存路径，`-k` 参数表示**加密密钥**，`-e` 参数表示实验规格文件。此外，TAO 工具包支持对模板配置文件进行序列化，以便 DeepStream 的 `Gst-nvinfer` 元素使用此模型。此配置文件包含解析 etlt 模型文件所需的网络特定预处理参数和网络图参数 — 非常方便参考。该文件还会生成一个标签文件 `labels.txt`，其中包含按照生成输出的顺序训练的模型类名称。要生成 DeepStream 模板文件，只需使用 `--gen_ds_config` 选项运行 `export` 子任务即可。<br> 执行以下单元以导出经过训练的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a603260-ada1-4534-a783-332be1ccf247",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View export usage\n",
    "!detectnet_v2 export --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e702a1dc-ab54-437e-8e57-abe854b18c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Removing a pre-existing copy if there has been any.\n",
    "!rm -rf $MODELS_DIR/resnet18_detector_unpruned\n",
    "!mkdir -p $MODELS_DIR/resnet18_detector_unpruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c6b3b7-2e3f-4e9d-888c-bb0abb781f14",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Exporting .tlt model\n",
    "!detectnet_v2 export -m $MODELS_DIR/resnet18_detector/weights/resnet18_detector.tlt \\\n",
    "                     -o $MODELS_DIR/resnet18_detector_unpruned/resnet18_detector.etlt \\\n",
    "                     -k tlt_encode \\\n",
    "                     -e $SPEC_FILES_DIR/combined_training_config.txt \\\n",
    "                     --gen_ds_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7d19d6-3da7-43ed-b613-568105013218",
   "metadata": {},
   "source": [
    "<a name='s5'></a>\n",
    "## 部署到 DeepStream ##\n",
    "经训练的模型已准备好部署到 DeepStream 工作流中。我们已有一个脚本，该脚本将根据以下 3 个参数启动 DeepStream 工作流：<br>\n",
    "`python sample_apps/app_03.py <input video file name> <path to nvinfer configuration file> <output video file name>` <br>\n",
    "它具有以下架构，与我们在本课程第一部分创建的架构非常相似。\n",
    "\n",
    "<p><img src='images/deepstream_pipeline.png' width=1080></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bda48cc-71f5-481d-be2c-f4c759c4d4ed",
   "metadata": {},
   "source": [
    "执行以下单元，通过这个简单的 DeepStream 工作流传输两次视频，一次使用专用的 TrafficCamNet 模型，另一次使用自定义模型。之后，比较结果。<br>\n",
    "**注意**：如果时间允许，请在课程结束时随意修改一些训练参数，以改进模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b22c2b8-30f1-4699-80eb-9fa7715bf9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the nvinfer config file using the TrafficCamNet as is\n",
    "!cat $SPEC_FILES_DIR/pgie_config_trafficcamnet.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcfff33-fdf0-4911-852d-d07d0866aa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Run the DeepStream Pipeline with the Purpose-built Model as is\n",
    "%run sample_apps/app_03.py data/sample_30.h264 spec_files/pgie_config_trafficcamnet.txt output.mp4\n",
    "\n",
    "# Convert the video to a format that is compatible with Jupyter Lab\n",
    "!ffmpeg -i output.mp4 output_conv.mp4 -y -loglevel quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907cffd4-0977-4cd5-97a7-17db3b50d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the nvinfer config file using the custom model\n",
    "!cat $SPEC_FILES_DIR/pgie_config_resnet18_detector_unpruned.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e0fe03-7fca-4685-9217-dcd0529468e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Run the DeepStream Pipeline with the Custom Model\n",
    "%run sample_apps/app_03.py /dli/task/data/sample_30.h264 spec_files/pgie_config_resnet18_detector_unpruned.txt output_resnet18_detector_unpruned.mp4\n",
    "\n",
    "# Convert the video to a format that is compatible with Jupyter Lab\n",
    "!ffmpeg -i output_resnet18_detector_unpruned.mp4 output_resnet18_detector_unpruned_conv.mp4 -y -loglevel quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8394bd49-378a-4ebb-b787-48e830b50176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Compare results\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<div>\n",
    "    <video alt=\"input\" width='49%' autoplay>\n",
    "        <source src=\"output_conv.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    <video alt=\"output\" width='49%' autoplay>\n",
    "        <source src=\"output_resnet18_detector_unpruned_conv.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "</div>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23d74a7-31b4-4674-8100-8b048741b9f8",
   "metadata": {},
   "source": [
    "**您做的非常好**！准备就绪后，我们开始学习[下一个 Notebook](./04_optimizing_a_video_AI_application.ipynb)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d88e494",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
