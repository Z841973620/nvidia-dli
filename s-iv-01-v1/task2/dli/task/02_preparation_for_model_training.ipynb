{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddca83a8",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b113e10-1b6d-449b-b712-f79f3f610ef5",
   "metadata": {},
   "source": [
    "# 模型训练准备工作 #\n",
    "我们注意到，视频 AI 应用的 TrafficCamNet 模型存在一些问题。该模型可能并未完全针对我们的停车场用例进行训练。在实验的余下部分，我们将使用 TAO 工具包微调模型，使其适应我们的环境。模型开发的一般工作流程如下。首先，准备预训练模型和数据。接着，准备配置文件，并开始使用新数据训练模型并评估其性能。如果模型性能令人满意，我们即将其导出。请注意，这一流程不包括推理优化步骤，这些步骤对部署于边缘设备上的视频 AI 应用非常重要。\n",
    "<p><img src='images/pre-trained_model_workflow.png' width=1080></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1934605-75a4-49fa-9a5d-214f4a001400",
   "metadata": {},
   "source": [
    "## 学习目标 ##\n",
    "在此 Notebook 中，您将学习如何使用 TAO 工具包准备训练视频 AI 模型，学习内容包括：\n",
    "* 了解模型规范\n",
    "* 准备数据以供 TAO 工具包使用\n",
    "* 编辑面向 TAO 工具包任务的规格文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3756103b-e433-42e4-a9ec-9245d6a64b51",
   "metadata": {},
   "source": [
    "**目录**<br>\n",
    "本 Notebook 涵盖以下部分：\n",
    "1. [Detectnet_v2 物体检测模型](#s1)\n",
    "    * [目录结构](#s1.1)\n",
    "    * [模型目标](#s1.2)\n",
    "2. [准备预训练模型](#s2)\n",
    "    * [练习 #1 - 查看模型卡](#e1)\n",
    "3. [准备数据集](#s3)\n",
    "    * [标记数据](#s3.1)\n",
    "    * [探索性数据分析](#s3.2)\n",
    "    * [将视频文件转换为帧图像](#s3.3)\n",
    "    * [生成标签](#s3.4)\n",
    "    * [转换为 TFRecord 文件](#s3.5)\n",
    "    * [练习 #2 数据集转换](#e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4fcd16-3d29-405d-b032-6b48226dad63",
   "metadata": {},
   "source": [
    "<a name='s1'></a>\n",
    "## Detectnet_v2 物体检测模型 ##\n",
    "如前所述，[TrafficCamNet](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/trafficcamnet) 专用模型依托于 NVIDIA DetectNet_v2 检测器构建而成，并使用 ResNet18 作为特征提取器。因此，我们使用支持以下子任务的 `detectnet_v2` 任务：\n",
    "* `dataset_convert`\n",
    "* `train`\n",
    "* `evaluate`\n",
    "* `inference`\n",
    "* `prune`\n",
    "* `calibration_tensorfile`\n",
    "* `export`\n",
    "\n",
    "<p><img src='images/rewind.png' width=720><p>\n",
    "    \n",
    "我们可在命令行中使用 `detectnet_v2 <subtask> <args_per_subtask>` 来调用这些子任务。此外，还可通过使用 `detector_v2 <subtask> --help` 详细了解这些子任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3259d244-84ef-44ff-a389-d4a6eeab7571",
   "metadata": {},
   "source": [
    "<a name='s1.1'></a>\n",
    "### 目录结构 ###\n",
    "我们将在项目中使用以下结构，其中 `tao_project` 目录将保留与模型训练和输出相关的大部分素材。\n",
    "\n",
    "<p><img src='images/project_structure.png' width=740></p>\n",
    "\n",
    "* 当前目录为 `/dli/task`。使用路径时，更为可靠的方式是使用以 `/dli/task` 为开头的绝对路径，这是因为如不这样做，某些函数将尝试引用其被调用位置的相对路径。\n",
    "* 更高级别的 `data` 目录表示原始视频数据，而较低级别的 `tao_project/data` 目录代表将用于模型训练的预处理数据。\n",
    "* 更高级别的 `images` 目录包含本课程中使用的图形，并与视频 AI 模型无关联。\n",
    "* `spec_files` 目录包含用于 TAO 工具包模型训练的规格文件以及 DeepStream `Gst-nvinfer` 插件配置文件。\n",
    "* 在我们努力确定经优化的最终产品过程中，`tao_project/models` 目录将保留模型的不同版本。每个文件夹都将保存相应的模型文件（例如 `.tlt` 或 `.etlt`）以及 `labels.txt` 等随附素材。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2370d9e-d605-46ab-b474-2db51ef29635",
   "metadata": {},
   "source": [
    "执行以下单元，为 TAO 工具包设置和创建目录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b5c30-81b7-4a34-9789-b59614f44d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Set and create directories for the TAO Toolkit experiment\n",
    "import os\n",
    "\n",
    "os.environ['PROJECT_DIR']='/dli/task/tao_project'\n",
    "os.environ['SOURCE_DATA_DIR']='/dli/task/data'\n",
    "os.environ['DATA_DIR']='/dli/task/tao_project/data'\n",
    "os.environ['MODELS_DIR']='/dli/task/tao_project/models'\n",
    "os.environ['SPEC_FILES_DIR']='/dli/task/spec_files'\n",
    "\n",
    "!mkdir $PROJECT_DIR\n",
    "!mkdir $DATA_DIR\n",
    "!mkdir $MODELS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4977de-9671-40ec-8324-d05803074c43",
   "metadata": {},
   "source": [
    "<a name='s1.2'></a>\n",
    "### 模型的训练目标 ###\n",
    "对于视频 AI 应用，我们希望训练一个以 TrafficCamNet 为起点的模型，并为其提供额外的（已标记）数据，以便其适应我们特定的摄像头角度、照明条件和其他环境条件。我们会将**未剪枝的预训练 TrafficCamNet 专用模型**用作起点，并训练一个符合我们用例的自定义**单类物体检测模型**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a15e30",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s2'></a>\n",
    "## 准备预训练模型和数据集 ##\n",
    "开发者通常首先会从 [NGC](https://ngc.nvidia.com/) 中选择并下载预训练模型，其中既有高度精确的专用模型，也有符合其所选预训练权重的架构。由于经常需要在训练时间、准确性和推理性能之间进行权衡，因此开发者很难立即确定何种模型/架构最适合某个特定用例。在选择最佳候选模型之前，通常要比较多个模型。\n",
    "\n",
    "以下是一些有助于选择恰当模型的提示：\n",
    "* 查看模型输入/输出，以确定模型是否适合您的用例。\n",
    "* 输入格式也是需考虑的一项重要因素。例如，TrafficCamNet 和其他 DetectNet_v2 模型预计输入为 0~1的取值，且输入通道按 RGB 顺序归一化。按 BGR 顺序的模型需要输入预处理/均值消减，而这可能会导致性能欠佳。\n",
    "\n",
    "我们可以使用 `ngc registry model list <model_glob_string>` 命令获取位于 NGC 模型注册表中的模型列表。例如，我们可以使用 `ngc registry model list nvidia/tao/*` 列出所有可用模型。`--column` 可用于识别兴趣列。NGC 注册 CLI 相关详情请参阅[用户指南](https://docs.nvidia.com/dgx/pdf/ngc-registry-cli-user-guide.pdf)。每个模型下都存在一个剪枝版本和未剪枝版本，前者可支持直接部署，后者可支持针对特定用例使用更多数据重新训练。如要开展可训练，可选用未剪枝版本。`ngc registry model download-version <org>/[<team>/]<model-name:version>` 命令可用于下载注册表中的模型，并可通过 `--dest` 指定下载目录的路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8730b58",
   "metadata": {},
   "source": [
    "<a name='e1'></a>\n",
    "#### 练习 #1 - 查看模型卡 ####\n",
    "下载预训练模型。\n",
    "\n",
    "**说明**：<br>\n",
    "* 查看 [TrafficCamNet](https://catalog.ngc.nvidia.com/orgs/nvidia/team/tao/models/trafficcamnet) 或 [DetectNet_v2](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/pretrained_detectnet_v2) 模型的模型卡，了解在重要模型规格的所在位置。\n",
    "* 执行以下单元，下载 NGC CLI。\n",
    "* 执行以下单元，列出所有可用模型。\n",
    "* 执行以下单元，下载 TrafficCamNet 模型。\n",
    "* 执行以下单元，检查是否已下载该模型。\n",
    "* 如果 `labels.txt` 不存在，执行以下单元完成创建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88994507",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Download the NGC CLI\n",
    "%env CLI=ngccli_cat_linux.zip\n",
    "!mkdir -p ngc_assets/ngccli\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P ngc_assets/ngccli\n",
    "!unzip -u \"ngc_assets/ngccli/$CLI\" \\\n",
    "       -d ngc_assets/ngccli/\n",
    "!rm ngc_assets/ngccli/*.zip \n",
    "os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(\"ngc_assets\", os.getenv(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d1394b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# List all available models\n",
    "!ngc registry model list nvidia/tao/* --column name --column repository --column application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab3f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Download the unpruned pre-trained model from NGC\n",
    "!ngc registry model download-version nvidia/tao/trafficcamnet:unpruned_v1.0 \\\n",
    "    --dest $MODELS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8a7ac2-cf8e-4536-98a0-066dea725ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Download the pruned pre-trained model from NGC\n",
    "!ngc registry model download-version nvidia/tao/trafficcamnet:pruned_v1.0 \\\n",
    "    --dest $MODELS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03eafe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Check if models have been downloaded into directory\n",
    "!ls -rlt $MODELS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb57ff2-2960-4dc2-80e0-7ebae65debc8",
   "metadata": {},
   "source": [
    "**观察**：<br>\n",
    "以下是一些需要注意的字段：\n",
    "\n",
    "<p><img src='images/model_card_tao.png' width=1080></p>\n",
    "\n",
    "<p><img src='images/encryption_key.png' width=540></p>\n",
    "\n",
    "<p><img src='images/important.png' width=720></p>\n",
    "\n",
    "_请注意：我们使用专用 TrafficCamNet 模型作为场景改编的起点。如果课程结束时还有多余时间，您可以随意尝试使用其他模型架构进行实验。在使用 NGC 的专用模型时，加载模型需要有正确的**加密密钥**。用户通过通用模型开展训练时，将能够定义自己的导出加密密钥。此操作可保护专有 IP，并用于解密 DeepStream 应用中的 `.etlt` 模型。_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8a21c3",
   "metadata": {},
   "source": [
    "<a name='s3'></a>\n",
    "## 准备数据集 ##\n",
    "我们将使用在 NVIDIA 总部停车场通过同一摄像头拍摄的带标记视频数据来训练模型。须明确的一点是，我们提供的数据有限且不足以从头开始训练模型。通过利用 TAO 工具包和迁移学习，我们可以使用 TrafficCamNet 模型作为起点并训练自定义模型。这是我们在利用 TAO 工具包的场景/域适应功能时的常规操作。\n",
    "\n",
    "TAO 工具包要求使用特定格式的数据开展训练和评估：\n",
    "* TAO 工具包中的物体检测任务需要 `KITTI format` 中的数据。\n",
    "    * `images` 目录包含要训练的图像。\n",
    "    * `labels` 目录包含对应图像的标签。\n",
    "    * `kitti_seq_to_map.json` 文件是*可选项*，包含为图像目录中的帧提供的从序列到帧 ID 的映射。如果需要按序列将数据拆分为不同的折叠，此文件很实用。\n",
    "\n",
    "<p><img src='images/detection_input.png' width=720></p>\n",
    "\n",
    "* 相比之下，分类任务需要具有以下结构的图像目录，其中每个类都有自己的目录和类名称。\n",
    "\n",
    "<p><img src='images/classification_input.png' width=720></p>\n",
    "\n",
    "_您可以参阅 [TAO 工具包用户指南](https://docs.nvidia.com/metropolis/TLT/tlt-user-guide/text/data_annotation_format.html#object-detection-kitti-format)_ ，详细了解数据标记格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9dc917",
   "metadata": {},
   "source": [
    "<a name='s3.1'></a>\n",
    "### 标记数据 ###\n",
    "先预览视频，然后再继续。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e1e8b9-ccf9-4285-aa2f-46135f658087",
   "metadata": {},
   "source": [
    "执行下方单元，观看视频。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600ad2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View the video\n",
    "from IPython.display import Video\n",
    "\n",
    "Video(\"data/126_206-A0-3_raw.mp4\", width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e79388",
   "metadata": {},
   "source": [
    "除视频源之外，我们还需添加标签来评估推理结果（将实际物体与深度学习模型检测到的物体进行比较），并扩展用于训练的真值。一般来说，整个过程很耗时，但迁移学习会大幅改善这一情况。公开可用的标记工具有很多。我们的数据集标记数据由我们使用 [Vatic](https://github.com/cvondrick/vatic) 手动生成，并以 JSON 格式提供。每个条目均以 `track_id` 为开头，代表记录中每辆车的唯一索引。`track_id` 包含一组边界框及其各自的边框位置。下方列出了标记数据格式的元素：\n",
    "\n",
    "<p><img src=\"images/vatic.jpg\" width=720></p>\n",
    "\n",
    "这是视频中 JSON 文件的快照。我们主要关注为物体检测模型捕获的边界框坐标：\n",
    "* **xbr**：[0, 帧宽] 之间的整数，表示相对于帧大小，边界框在坐标中的最右侧位置。<br />\n",
    "* **xtl**：[0, 帧宽] 之间的整数，表示相对于帧大小，边界框在坐标中的最左侧位置。<br />\n",
    "* **ybr**：[0, 帧高] 之间的整数，表示相对于帧大小，边界框在坐标中的最底部位置。<br />\n",
    "* **ytl**：[0, 帧高] 之间的整数，表示相对于帧大小，边界框在坐标中的最顶部位置。<br />\n",
    "<p><img src=\"images/json_structure.png\" width=720></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc20e908",
   "metadata": {},
   "source": [
    "执行以下单元，以 JSON 格式预览标记数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d42767e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Preview the annotation\n",
    "!cat $SOURCE_DATA_DIR/126_206-A0-3_json_sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e79a574",
   "metadata": {},
   "source": [
    "<a name='s3.2'></a>\n",
    "### 探索性数据分析 ###\n",
    "我们可以使用 [Pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) 分析和预处理数据。为此，我们继续将 JSON 文件转换为 .csv 文本文件，否则整个过程会非常耗时。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b2153b-d6dc-4328-bbbd-b2ec1afd1232",
   "metadata": {},
   "source": [
    "执行以下单元，分析标记数据文件中包含的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6184d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Load the .csv into a DataFrame\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "annotated_frames=pd.read_csv('data/annotation.csv', converters={2:ast.literal_eval})\n",
    "print(\"Length of the full DF object:\", len(annotated_frames))\n",
    "annotated_frames.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9e875a",
   "metadata": {},
   "source": [
    "我们可以执行 `DataFrame.groupby().size()`，查看每 `frame_no` 的行数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bad0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Check how many rows per frame_no\n",
    "annotated_frames.groupby('frame_no').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a6825",
   "metadata": {},
   "source": [
    "似乎每一帧的物体数量都一样，都是 130。在此处，我们发现标记数据存在问题，那就是即便汽车已出框，但边界框仍然存在，这个问题一直持续到视频结束。我们必须使用 `outside` 列将其过滤掉。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e74a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Filter out annotations that do not have car inside the bbox\n",
    "filtered_frames=annotated_frames[annotated_frames[\"outside\"] == 0]\n",
    "print(\"Length of the filtered DF object:\", len(filtered_frames))\n",
    "filtered_frames.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810b4ab7-832e-4985-9417-b5e49db7fa92",
   "metadata": {},
   "source": [
    "经过滤的 DataFrame 要小得多。我们可以绘制*包含行驶中车辆的帧索引*。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1871870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Plot frames that include moving cars\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "frames_list=list(filtered_frames['frame_no'].unique())\n",
    "frame_existance=np.zeros(annotated_frames['frame_no'].max()+1)\n",
    "for i in frames_list:\n",
    "    frame_existance[int(i)]=1\n",
    "y_pos=np.arange(len(frame_existance))\n",
    "fig, ax=plt.subplots(figsize=(18, 3))\n",
    "plt.bar(y_pos, frame_existance, align='center', alpha=0.5)\n",
    "plt.title('Frame Indices that Include Moving Cars')\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef45127",
   "metadata": {},
   "source": [
    "经过滤的标记数据看起来更合理。似乎很多帧都存在没有车辆的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0e403c",
   "metadata": {},
   "source": [
    "<a name='s3.3'></a>\n",
    "### 将视频文件转换为帧图像 ###\n",
    "由于物体检测模型按基于帧的数据运行，因此我们需要从原始影片文件中生成帧。为此，我们将使用 [OpenCV](https://opencv.org/) 打开视频文件，并为带有标记数据的每个帧编写一个 `.png` 图像文件。我们将以 10 FPS 的速度使用原始 `.mp4` 文件。除将视频帧转换为 `.png` 图像之外，我们还将创建一个视频，将标记数据显示为边界框。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03427fe3-bd0b-4c90-b1f1-6d38993a31db",
   "metadata": {},
   "source": [
    "执行以下单元以创建带标记数据的视频，并为 TAO 工具包提取带标记的图像。整个流程最多需要 5 分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa2d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Define function to extract images and generate an annotated video\n",
    "import cv2\n",
    "colors = [(255, 255, 0), (255, 0, 255), (0, 255, 255), (0, 0, 255), (255, 0, 0), (0, 255, 0), (0, 0, 0), (255, 100, 0), (100, 255, 0), (100, 0, 255), (255, 0, 100)]\n",
    "\n",
    "def save_images(video_path, image_folder, frames_list, annotated_frames,  video_out_folder, fps=10):\n",
    "    # Create image folder if it doesn't exist\n",
    "    if not os.path.exists(image_folder):\n",
    "        print(\"Creating images folder\")\n",
    "        os.makedirs(image_folder)\n",
    "    \n",
    "    # Create directory for output video\n",
    "    if not os.path.exists(video_out_folder):\n",
    "        print(\"Creating video out folder\")\n",
    "        os.makedirs(video_out_folder)\n",
    "    \n",
    "    # Start reading input video\n",
    "    input_video=cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # cv2.VideoCapture().read() returns true if it has a next frame\n",
    "    retVal, im=input_video.read()\n",
    "    size=im.shape[1], im.shape[0]\n",
    "    fourcc=cv2.VideoWriter_fourcc('h','2','6','4') \n",
    "    \n",
    "    # Start writing output video\n",
    "    output_video=cv2.VideoWriter('{}/annotated_video.mp4'.format(video_out_folder), fourcc, fps, size)\n",
    "\n",
    "    frameCount=0\n",
    "    i=1\n",
    "    \n",
    "    # While has next frame\n",
    "    while retVal:\n",
    "        print(\"\\rProcessing frame no: {}\".format(frameCount), end='', flush=True)\n",
    "        \n",
    "        # If current frame is in the list of annotated frames, draw bounding box(es) and include in the output video\n",
    "        if frameCount in frames_list:\n",
    "            print(\"\\rSaving frame no: {}, index: {} out of {}\".format(frameCount, i, len(frames_list)), end='')\n",
    "            cv2.imwrite(os.path.join(image_folder, '{}.png'.format(frameCount)), im)\n",
    "            i+=1\n",
    "            frame_items=annotated_frames[annotated_frames[\"frame_no\"]==int(frameCount)]\n",
    "            for index, box in frame_items.iterrows():\n",
    "                xmin, ymin, xmax, ymax = box[\"xmin\"], box[\"ymin\"], box[\"xmax\"], box[\"ymax\"]\n",
    "                xmin2, ymin2, xmax2, ymax2 = box[\"crop\"][0], box[\"crop\"][1], box[\"crop\"][2], box[\"crop\"][3]\n",
    "                cv2.rectangle(im, (xmin, ymin), (xmax, ymax), colors[0], 1)\n",
    "                cv2.rectangle(im, (int(xmin2), int(ymin2)), (int(xmax2), int(ymax2)), colors[1], 1)\n",
    "            output_video.write(im)\n",
    "\n",
    "        # Read next frame\n",
    "        retVal, im=input_video.read()\n",
    "        frameCount+=1\n",
    "\n",
    "    input_video.release()\n",
    "    output_video.release()\n",
    "    return size        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3187abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Extract images and generate an annotated video\n",
    "save_images('{}/126_206-A0-3_raw.mp4'.format(os.environ['SOURCE_DATA_DIR']), \n",
    "            '{}/{}'.format('{}/training'.format(os.environ['DATA_DIR']), 'images'),\n",
    "            frames_list,\n",
    "            filtered_frames,\n",
    "            '{}/{}'.format(os.environ['DATA_DIR'], 'video_out'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043185ff-923e-4fa4-ba55-346c2fe4ee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View the annotated output video\n",
    "Video('tao_project/data/video_out/annotated_video.mp4', width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8dc9ad",
   "metadata": {},
   "source": [
    "<a name='s3.4'></a>\n",
    "### 生成标签 ###\n",
    "我们还需要为每一帧生成 KITTI 格式标签，您也可参阅 [TAO 工具包用户指南](https://docs.nvidia.com/tao/tao-toolkit/text/data_annotation_format.html#label-files)了解相关说明。KITTI 格式标签文件是简单的文本文件，每个对象包含一行。每行具有多个字段。每个物体的元素总数为 15 个，具体如下所示：<br>\n",
    "`class name`, `truncation`, `occlusion`, `alpha`, `xmin`, `ymin`, `xmax`, `ymax`, `height`, `weight`, `length`, `x`, `y`, `z`, `rotation_y` <br>\n",
    "目前，如要使用 TAO 工具包执行检测，只需要填充分类名称和边界框坐标字段即可。这是因为 TAO 工具包训练管道仅支持对分类和边界框坐标进行训练。可将其余字段设置为 0 以将其用作占位符。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee5df11-9c18-4a91-82a5-fa64685686a6",
   "metadata": {},
   "source": [
    "执行以下单元以生成标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fee05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Generate labels in KITTI format\n",
    "label_folder='{}/training/labels'.format(os.environ['DATA_DIR'])\n",
    "if not os.path.exists(label_folder):\n",
    "    print(\"Creating labels folder\")\n",
    "    os.makedirs(label_folder)\n",
    "for frame in sorted(frames_list): \n",
    "    current_frame=filtered_frames[filtered_frames['frame_no']==frame]\n",
    "    with open('{}/{}.txt'.format(label_folder, frame), 'w') as f: \n",
    "        for i, box in current_frame.iterrows(): \n",
    "            print('Writing for frame {}'.format(frame), end='\\r')\n",
    "            f.write(\"Car 0 0 0 {} {} {} {} 0 0 0 0 0 0 0\\n\".format(box['xmin'], box['ymin'], box['xmax'], box['ymax']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANAGE THIS CELL\n",
    "# Preview sample KITTI format labels\n",
    "!cat $DATA_DIR/training/labels/20.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7df4f1",
   "metadata": {},
   "source": [
    "<a name='s3.5'></a>\n",
    "### 转换为 TFRecord ###\n",
    "TAO 工具包可将训练数据转换为 [**TFRecord**](https://www.tensorflow.org/tutorials/load_data/tfrecord) 格式，这是一种存储二进制记录序列的简单格式。按照 TFRecord 规范，将图像帧和与该帧关联的所有标记数据编码到单行内。这非常有助于加快数据迭代。使用 KITTI 格式后，TAO 工具包可帮助我们将数据轻松转换为 TFRecord 格式。我们可以使用 `dataset_convert` 子任务完成此操作。`dataset_convert` 工具需要使用配置文件作为输入，这类文件中包含以下参数：\n",
    "* `kittie_config`\n",
    "    * `root_directory_path (str)`：数据集根路径。\n",
    "    * `image_dir_name (str)`：包含图像的目录的相对路径。\n",
    "    * `label_dir_name (str)`：包含标签的目录的相对路径。\n",
    "    * `partition_mode (str)`：将数据划分为折叠时所采用的方法*（\"随机划分\"或\"按序列划分\"）*。\n",
    "    * `num_partitions (int)`：拆分数据的分区数（折叠数）*（默认值为 2）*。当将分区模式设置为\"随机\"模式时，系统会忽略此字段，因为默认情况下系统只生成两个分区：`train` 和 `val`。\n",
    "    * `image_extension (str)`：图像的扩展名*（\".png\"、\".jpg\" 或 \".jpeg\"）*。\n",
    "    * `val_split (float)`：要分离以进行验证的数据百分比 *(0-100)* 。\n",
    "    * `num_shards (int)`：每个折叠的分片数量 _(1-20)_ 。当您拥有大量样本时，将数据集拆分成多个文件有利于操作，因为这样便于系统按以下方式处理输入：1) 并行读取以提高吞吐量，2) 能够更好地打乱数据集，以提高模型性能。当数据集很大时，这一点尤为重要。您可以参阅 [TensorFlow 的 API 文档](https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset#raises_1)，详细了解数据分片。\n",
    "* `image_directory_path (str)`：数据集根路径。\n",
    "\n",
    "生成后，您可以在多个训练实验中使用 TFRecord。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a5bcfc",
   "metadata": {},
   "source": [
    "<a name='e2'></a>\n",
    "#### 练习 #2 - 数据集转换 ####\n",
    "使用 `dataset_convert` 子任务来生成 TFRecord 文件。\n",
    "\n",
    "**说明**：<br>\n",
    "* 将 `<FIXME>` 更改为正确值并**保存更改**，以此修改 [TFRecord 转换规格文件](spec_files/kitti_config.txt)。\n",
    "* 执行以下单元以创建 TFRecord 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e32483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View the spec file\n",
    "!cat $SPEC_FILES_DIR/kitti_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a7d55-f0ff-403d-923a-c1f877a8ba2d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kitti_config {\n",
    "#   root_directory_path: \"/dli/task/tao_project/data/training\"\n",
    "#   image_dir_name: \"images\"\n",
    "#   label_dir_name: \"labels\"\n",
    "#   image_extension: \".png\"\n",
    "#   partition_mode: \"random\"\n",
    "#   num_partitions: 2\n",
    "#   val_split: 20\n",
    "#   num_shards: 10\n",
    "# }\n",
    "# image_directory_path: \"/dli/task/tao_project/data/training\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c1c8d-d612-40aa-9e48-843aae048fcf",
   "metadata": {},
   "source": [
    "点击 ... 以显示**答案**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b268d7c-d554-4daf-8ce1-ee76da69e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View dataset_convert usage\n",
    "!detectnet_v2 dataset_convert --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477fce70-d634-48d6-94e1-72d5e366882a",
   "metadata": {},
   "source": [
    "使用 `dataset_convert` 子任务时，`-o` 参数表示输出文件名，`-d` 参数指向检测数据集规格文件的路径，其中包含用于导出 `.tfrecord` 文件的配置文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14481bf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Create directory for TFRecords and delete existing files if they exist\n",
    "!mkdir -p $DATA_DIR/tfrecords && rm -rf $DATA_DIR/tfrecords/*\n",
    "\n",
    "!detectnet_v2 dataset_convert -d $SPEC_FILES_DIR/kitti_config.txt \\\n",
    "                              -o $DATA_DIR/tfrecords/kitti_trainval/kitti_trainval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bd0daf",
   "metadata": {},
   "source": [
    "查看已创建的数据分片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762b2899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Check the shards that have been created\n",
    "!ls -rlt $DATA_DIR/tfrecords/kitti_trainval/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae1d3a0",
   "metadata": {},
   "source": [
    "**您做得很好**！准备就绪后，我们开始学习[下一个 Notebook](./03_model_training_with_the_TAO_Toolkit.ipynb)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779f613e",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
