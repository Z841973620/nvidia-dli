{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2-Px6LAIoz7"
   },
   "source": [
    "<center><img src=\"images/DLI_Header.png\" alt=\"标题\" style=\"width: 400px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比语言-图像预训练或 [CLIP](https://github.com/openai/CLIP/tree/main) 是一种文本和图像编码工具，可与许多流行的生成式 AI 模型（例如 [DALL-E](https://openai.com/dall-e-2) 和 [Stable Diffusion](https://github.com/Stability-AI/stablediffusion)）一起使用。\n",
    "\n",
    "CLIP 本身并不是生成式 AI 模型，而是用于将文本编码与图像编码对齐。如果存在完美的图像文本描述，那么 CLIP 的目标就是为图像和文本创建相同的向量嵌入。让我们看看这在实践中意味着什么。\n",
    "\n",
    "本笔记本的目标是：\n",
    "* 学习如何使用 CLIP 编码\n",
    "  * 获取图像编码\n",
    "  * 获取文本编码\n",
    "  * 计算它们之间的余弦相似度\n",
    "* 使用 CLIP 创建文本到图像的神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，让我们加载本练习所需的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWn2WgPaIoz8"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import clip\n",
    "\n",
    "# Visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from textwrap import wrap\n",
    "\n",
    "# User defined libraries\n",
    "from utils import other_utils\n",
    "from utils import ddpm_utils\n",
    "from utils import UNet_utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于流行的图像识别神经网络，CLIP 有几种不同的变体：\n",
    "* RN50\n",
    "* RN101\n",
    "* RN50x4\n",
    "* RN50x16\n",
    "* RN50x64\n",
    "* ViT-B/32\n",
    "* ViT-B/16\n",
    "* ViT-L/14\n",
    "* ViT-L/14@336px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip.available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于此笔记本，我们将使用基于 [Vision Transformer](https://huggingface.co/docs/transformers/main/model_doc/vit) 架构的 `ViT-B/32` 。它具有 `512` 个特征，我们稍后会将其输入到我们的扩散模型中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\")\n",
    "clip_model.eval()\n",
    "CLIP_FEATURES = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 图像编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们加载 CLIP 时，它还会附带一组图像转换，我们可以使用这些图像将图像输入 CLIP 模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以在一张花卉照片上测试一下。首先从一朵雏菊开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/cropped_flowers/\"\n",
    "img_path = DATA_DIR + \"daisy/2877860110_a842f8b14a_m.jpg\"\n",
    "img = Image.open(img_path)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以先使用 `clip_preprocess` 转换图像，然后将结果转换为张量，从而找到 CLIP 嵌入。由于 `clip_model` 需要一批图像，因此我们可以使用 [np.stack](https://numpy.org/doc/stable/reference/generated/numpy.stack.html) 将处理后的图像转换为单个元素批次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_imgs = torch.tensor(np.stack([clip_preprocess(img)])).to(device)\n",
    "clip_imgs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，我们可以将批处理传递给 `clip_model.encode_image` 以查找图像的嵌入。如果您想查看编码的样子，请取消注释 `clip_img_encoding` 。当我们打印尺寸时，它会为我们的 `1` 图像列出 `512` 个特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_img_encoding = clip_model.encode_image(clip_imgs)\n",
    "print(clip_img_encoding.size())\n",
    "#clip_img_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 文本编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们有了图像编码，让我们看看是否可以得到匹配的文本编码。下面是不同花卉描述的列表。与图像一样，文本需要经过预处理才能由 CLIP 编码。为此，CLIP 附带了一个 `tokenize` 函数，以便将每个单词转换为整数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\n",
    "    \"A white daisy with a yellow center\",\n",
    "    \"An orange sunflower with a big brown center\",\n",
    "    \"A red rose bud\"\n",
    "]\n",
    "text_tokens = clip.tokenize(text_list).to(device)\n",
    "text_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，我们可以将标记传递给 `encode_text` 以获取我们的文本编码。如果您想查看编码的样子，请取消注释 `clip_text_encodings` 。与我们的图像编码类似，我们的 `3` 幅图像中的每一幅都有 `512` 个特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_text_encodings = clip_model.encode_text(text_tokens).float()\n",
    "print(clip_text_encodings.size())\n",
    "#clip_text_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了了解我们的哪一个文本描述最能描述雏菊，我们可以计算文本编码和图像编码之间的[余弦相似度](https://medium.com/@milana.shxanukova15/cosine-distance-and-cosine-similarity-a5da0e4d9ded)。当余弦相似度为 `1` 时，它们是完美匹配的。当余弦相似度为 `-1` 时，这两个编码是相反的。\n",
    "\n",
    "余弦相似度相当于[点积](https://mathworld.wolfram.com/DotProduct.html)，每个向量都按其幅度归一化。换句话说，每个向量的幅度变为 `1` 。\n",
    "\n",
    "我们可以使用以下公式来计算点积：\n",
    "\n",
    "$X \\cdot Y = \\sum_{i=1}^{n} x_i y_i = x_1y_1 + x_2 y_2 + \\cdots + x_n y_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_img_encoding /= clip_img_encoding.norm(dim=-1, keepdim=True)\n",
    "clip_text_encodings /= clip_text_encodings.norm(dim=-1, keepdim=True)\n",
    "similarity = (clip_text_encodings * clip_img_encoding).sum(-1)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "觉得如何？描述性最强的文字能获得最高分吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, text in enumerate(text_list):\n",
    "    print(text, \" - \", similarity[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们再练习一下。下面，我们添加了一朵向日葵和一朵玫瑰的图片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = [\n",
    "    DATA_DIR + \"daisy/2877860110_a842f8b14a_m.jpg\",\n",
    "    DATA_DIR + \"sunflowers/2721638730_34a9b7a78b.jpg\",\n",
    "    DATA_DIR + \"roses/8032328803_30afac8b07_m.jpg\"\n",
    "]\n",
    "\n",
    "imgs = [Image.open(path) for path in img_paths]\n",
    "for img in imgs:\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**：下面的 `get_img_encodings` 函数充斥着 `FIXMEs` 。请用适当的代码替换每个 `FIXME` ，以便从 PIL 图像生成 CLIP 编码。\n",
    "\n",
    "单击 `...` 获取答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_encodings(imgs):\n",
    "    processed_imgs = [FIXME(img) for img in imgs]\n",
    "    clip_imgs = torch.tensor(np.stack(FIXME)).to(device)\n",
    "    clip_img_encodings = FIXME.encode_image(clip_imgs)\n",
    "    return clip_img_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_img_encodings(imgs):\n",
    "    processed_imgs = [clip_preprocess(img) for img in imgs]\n",
    "    clip_imgs = torch.tensor(np.stack(processed_imgs)).to(device)\n",
    "    clip_img_encodings = clip_model.encode_image(clip_imgs)\n",
    "    return clip_img_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_img_encodings = get_img_encodings(imgs)\n",
    "clip_img_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**：找到能够很好地描述上述图像并产生高相似度分数的文本。计算相似度分数后，请随意重复此练习并进行修改。我们稍后会再次使用此文本列表。\n",
    "\n",
    "单击 `...` 查看示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\n",
    "    \"A daisy\",\n",
    "    \"A sunflower\",\n",
    "    \"A rose\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "```python\n",
    "text_list = [\n",
    "    \"A white daisy with a yellow center\",\n",
    "    \"An orange sunflower with a big brown center\",\n",
    "    \"A deep red rose flower\"\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = clip.tokenize(text_list).to(device)\n",
    "clip_text_encodings = clip_model.encode_text(text_tokens).float()\n",
    "clip_text_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最好能比较文本和图像的每种组合。为此，我们可以对每种图像编码 [重复](https://pytorch.org/docs/stable/generated/torch.Tensor.repeat.html#torch.Tensor.repeat) 每种文本编码。同样，我们可以对每种文本编码 [重复交错](https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html) 每种图像编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_img_encodings /= clip_img_encodings.norm(dim=-1, keepdim=True)\n",
    "clip_text_encodings /= clip_text_encodings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "n_imgs = len(imgs)\n",
    "n_text = len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_clip_text_encodings = clip_text_encodings.repeat(n_imgs, 1)\n",
    "repeated_clip_text_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_clip_img_encoding = clip_img_encodings.repeat_interleave(n_text, dim=0)\n",
    "repeated_clip_img_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = (repeated_clip_text_encodings * repeated_clip_img_encoding).sum(-1)\n",
    "similarity = torch.unflatten(similarity, 0, (n_text, n_imgs))\n",
    "similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们比较一下。理想情况下，从左上角到右下角的对角线应该是亮黄色，与它们的高值相对应。其余的值应该是低值和蓝色。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "gs = fig.add_gridspec(2, 3, wspace=.1, hspace=0)\n",
    "\n",
    "for i, img in enumerate(imgs):\n",
    "    ax = fig.add_subplot(gs[0, i])\n",
    "    ax.axis(\"off\")\n",
    "    plt.imshow(img)\n",
    "\n",
    "ax = fig.add_subplot(gs[1, :])\n",
    "plt.imshow(similarity.detach().cpu().numpy().T, vmin=0.1, vmax=0.3)\n",
    "\n",
    "labels = [ '\\n'.join(wrap(text, 20)) for text in text_list ]\n",
    "plt.yticks(range(n_text), labels, fontsize=10)\n",
    "plt.xticks([])\n",
    "\n",
    "for x in range(similarity.shape[1]):\n",
    "    for y in range(similarity.shape[0]):\n",
    "        plt.text(x, y, f\"{similarity[x, y]:.2f}\", ha=\"center\", va=\"center\", size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 CLIP 数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在之前的笔记本中，我们使用花卉类别作为标签。这次，我们将使用 CLIP 编码作为标签。\n",
    "\n",
    "如果 CLIP 的目标是将文本编码与图像编码对齐，那么我们是否需要为数据集中的每个图像提供文本描述？假设：我们不需要文本描述，只需要图像 CLIP 编码来创建文本到图像的管道。\n",
    "\n",
    "为了测试这一点，让我们将 CLIP 编码作为“标签”添加到我们的数据集中。在每一批数据增强图像上运行 CLIP 会更准确，但速度也会更慢。我们可以通过预处理和提前存储编码来加快速度。\n",
    "\n",
    "我们可以使用 [glob](https://docs.python.org/3/library/glob.html) 列出我们所有的图像文件路径："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = glob.glob(DATA_DIR + '*/*.jpg', recursive=True)\n",
    "data_paths[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一个代码块针对每个文件路径运行以下循环：\n",
    "* 打开与路径关联的图像并将其存储在 `img` 中\n",
    "* 预处理图像，找到 CLIP 编码，并将其存储在 `clip_img` 中\n",
    "* 将 CLIP 编码从张量转换为 python 列表\n",
    "* 将文件路径和 CLIP 编码作为一行存储在 csv 文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'clip.csv'\n",
    "\n",
    "with open(csv_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    for idx, path in enumerate(data_paths):\n",
    "        img = Image.open(path)\n",
    "        clip_img = torch.tensor(np.stack([clip_preprocess(img)])).to(device)\n",
    "        label = clip_model.encode_image(clip_img)[0].tolist()\n",
    "        writer.writerow([path] + label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理完整数据集可能需要几秒钟。完成后，打开 [clip.csv](clip.csv) 查看结果。\n",
    "\n",
    "我们可以使用与其他笔记本相同的图像转换："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 32 # Due to stride and pooling, must be divisible by 2 multiple times\n",
    "IMG_CH = 3\n",
    "BATCH_SIZE = 128\n",
    "INPUT_SIZE = (IMG_CH, IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "pre_transforms = [\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),  # Scales data into [0,1]\n",
    "    transforms.Lambda(lambda t: (t * 2) - 1)  # Scale between [-1, 1]\n",
    "]\n",
    "pre_transforms = transforms.Compose(pre_transforms)\n",
    "random_transforms = [\n",
    "    transforms.RandomCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "]\n",
    "random_transforms = transforms.Compose(random_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是初始化新数据集的代码。由于我们已经 `preprocessed_clip` ，我们将使用 `__init__` 函数将其预加载到我们的 GPU 上。我们保留了“即时” CLIP 编码作为示例。它会产生稍好一些的结果，但速度要慢得多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, csv_path, preprocessed_clip=True):\n",
    "        self.imgs = []\n",
    "        self.preprocessed_clip = preprocessed_clip\n",
    "        if preprocessed_clip:\n",
    "            self.labels = torch.empty(\n",
    "                len(data_paths), CLIP_FEATURES, dtype=torch.float, device=device\n",
    "            )\n",
    "        \n",
    "        with open(csv_path, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=',')\n",
    "            for idx, row in enumerate(reader):\n",
    "                img = Image.open(row[0])\n",
    "                self.imgs.append(pre_transforms(img).to(device))\n",
    "                if preprocessed_clip:\n",
    "                    label = [float(x) for x in row[1:]]\n",
    "                    self.labels[idx, :] = torch.FloatTensor(label).to(device)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = random_transforms(self.imgs[idx])\n",
    "        if self.preprocessed_clip:\n",
    "            label = self.labels[idx]\n",
    "        else:\n",
    "            batch_img = img[None, :, :, :]\n",
    "            encoded_imgs = clip_model.encode_image(clip_preprocess(batch_img))\n",
    "            label = encoded_imgs.to(device).float()[0]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset(csv_path)\n",
    "dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net 模型的架构与上次相同，但有一个小区别。我们将使用 `CLIP_FEATURES` 的数量，而不是使用类的数量作为 `c_embed_dim` 。之前，`c` 可能代表“类”，但这次，它代表“上下文”。幸运的是，它们都以 `c` 开头，因此我们不需要重构代码来反映这种意图的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 400\n",
    "B_start = 0.0001\n",
    "B_end = 0.02\n",
    "B = torch.linspace(B_start, B_end, T).to(device)\n",
    "\n",
    "ddpm = ddpm_utils.DDPM(B, device)\n",
    "model = UNet_utils.UNet(\n",
    "    T, IMG_CH, IMG_SIZE, down_chs=(256, 256, 512), t_embed_dim=8, c_embed_dim=CLIP_FEATURES\n",
    ")\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
    "model_flowers = torch.compile(model.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_context_mask` 函数会略有变化。由于我们用 CLIP 嵌入替换了分类输入，因此我们不再需要对标签进行独热编码。我们仍会将编码中的值随机设置为 `0` ，以帮助模型在没有上下文的情况下学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_mask(c, drop_prob):\n",
    "    c_mask = torch.bernoulli(torch.ones_like(c).float() - drop_prob).to(device)\n",
    "    return c_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还重新构建了 `sample_flowers` 函数。这一次，它将以我们的 `text_list` 作为参数并将其转换为 CLIP 编码。`sample_w` 函数基本保持不变，并已移至 [ddpm_utils.py](utils/ddpm_utils.py) 的底部。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_flowers(text_list):\n",
    "    text_tokens = clip.tokenize(text_list).to(device)\n",
    "    c = clip_model.encode_text(text_tokens).float()\n",
    "    x_gen, x_gen_store = ddpm_utils.sample_w(model, ddpm, INPUT_SIZE, T, c, device)\n",
    "    return x_gen, x_gen_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练时间到了！经过大约 `50` 个 `epochs` 后，模型将开始生成一些可识别的内容，在 `100` 时它将达到最佳状态。觉得如何？生成的图像与您的描述相符吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100\n",
    "c_drop_prob = 0.1\n",
    "lrate = 1e-4\n",
    "save_dir = \"05_images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lrate)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        t = torch.randint(0, T, (BATCH_SIZE,), device=device).float()\n",
    "        x, c = batch\n",
    "        c_mask = get_context_mask(c, c_drop_prob)\n",
    "        loss = ddpm.get_loss(model_flowers, x, t, c, c_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch} | Step {step:03d} | Loss: {loss.item()}\")\n",
    "    if epoch % 5 == 0 or epoch == int(epochs - 1):\n",
    "        x_gen, x_gen_store = sample_flowers(text_list)\n",
    "        grid = make_grid(x_gen.cpu(), nrow=len(text_list))\n",
    "        save_image(grid, save_dir + f\"image_ep{epoch:02}.png\")\n",
    "        print(\"saved images in \" + save_dir + f\" for episode {epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在模型已经训练完毕，让我们来玩一玩吧！如果我们给它一个数据集中没有的东西作为提示，会发生什么？或者你能制作出完美的提示来生成你能想象到的图像吗？\n",
    "\n",
    "制作提示以获得所需结果的艺术称为**提示工程**，正如这里所示，这取决于模型所训练的数据类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change me\n",
    "text_list = [\n",
    "    \"A daisy\",\n",
    "    \"A sunflower\",\n",
    "    \"A rose\"\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x_gen, x_gen_store = sample_flowers(text_list)\n",
    "grid = make_grid(x_gen.cpu(), nrow=len(text_list))\n",
    "other_utils.show_tensor_image([grid])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "找到一组您喜欢的图像后，运行下面的单元格将其转换为动画。它将保存到 [05_images/flowers.gif](05_images/flowers.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = [other_utils.to_image(make_grid(x_gen.cpu(), nrow=len(text_list))) for x_gen in x_gen_store]\n",
    "other_utils.save_animation(grids, \"05_images/flowers.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 下一步"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恭喜您完成了课程！希望您能享受这段旅程，并能创造出值得与亲朋好友分享的东西。\n",
    "\n",
    "准备好测试您的技能了吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8hHZEaPIo0A"
   },
   "source": [
    "<center><img src=\"images/DLI_Header.png\" alt=\"标题\" style=\"width: 400px;\"/></center>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
