{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0dyhr7k_7DS"
   },
   "source": [
    "<center><img src=\"images/DLI_Header.png\" alt=\"标题\" style=\"width: 400px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBiybVKy_2zi"
   },
   "source": [
    "# 2. 扩散模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtLP1aneFBZp"
   },
   "source": [
    "在之前的笔记本中，我们学习了如何使用 U-Net 从图像中分离噪声，但它无法从噪声中生成可信的新图像。扩散模型在从头开始生成图像方面要好得多。\n",
    "\n",
    "好消息是，我们的神经网络模型不会有太大变化。我们将在 U-Net 架构的基础上进行一些细微的修改。\n",
    "\n",
    "相反，最大的区别在于我们如何使用我们的模型。我们不会一次性将噪声添加到图像中，而是多次添加少量噪声。然后，我们可以多次在噪声图像上使用我们的神经网络来生成新图像，如下所示：\n",
    "\n",
    "<center><img src=\"images/rev_diffusion.png\" /></center>\n",
    "\n",
    "#### 学习目标\n",
    "\n",
    "此笔记本的目标是：\n",
    "* 构建正向扩散方差计划\n",
    "* 定义正向扩散函数 `q`\n",
    "* 更新 U-Net 架构以适应时间步长 `t`\n",
    "* 训练模型以根据时间步长 `t` 检测添加到图像中的噪声\n",
    "* 定义反向扩散函数以模拟 `p`\n",
    "* 尝试生成服装（再次）\n",
    "\n",
    "我们已将上一个笔记本中的一些函数移至 [other_utils.py](utils/other_utils.py) 文件中。我们可以使用它来重新加载 fashionMNIST 数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "# User defined libraries\n",
    "from utils import other_utils\n",
    "\n",
    "IMG_SIZE = 16\n",
    "IMG_CH = 1\n",
    "BATCH_SIZE = 128\n",
    "data, dataloader = other_utils.load_transformed_fashionMNIST(IMG_SIZE, BATCH_SIZE)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 正向扩散\n",
    "\n",
    "让 `T` 成为我们将向图像添加噪声的次数。我们可以使用 `t` 来跟踪当前的 `timestep`。\n",
    "\n",
    "在之前的笔记本中，我们使用术语 `beta` 来表示新图像与原始图像相比的噪声百分比。默认值为 50% 噪声和 50% 原始图像。这次，我们将使用 `variance schedule`，表示为 $\\beta_t$，或代码中的 `B`。这将描述在每个时间步 `t` 中将向我们的图像添加多少噪声。\n",
    "\n",
    "在论文 [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239?ref=assemblyai.com) 的第 4 部分中，作者讨论了定义良好时间表的技巧。它应该足够大，以便模型能够识别已添加的噪声（特别是因为图像可能已经很嘈杂），但仍然尽可能小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-k7itgbnwZ4y"
   },
   "outputs": [],
   "source": [
    "nrows = 10\n",
    "ncols = 15\n",
    "\n",
    "T = nrows * ncols\n",
    "start = 0.0001\n",
    "end = 0.02\n",
    "B = torch.linspace(start, end, T).to(device)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zx7ajI-1Q1T_"
   },
   "source": [
    "[正态分布](https://mathworld.wolfram.com/NormalDistribution.html) 具有以下签名：\n",
    "\n",
    "$\\mathcal{N}(x;u,\\sigma^2)$ = $\\frac{1}{\\sigma\\sqrt{2\\pi}}\\mathcal{e}^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}}$\n",
    "\n",
    "其含义为“参数为 $u$（均值）和 $\\sigma^2$（方差）的 $x$ 正态分布”。当 $\\mu$ 为 0 且 $\\sigma$ 为 1 时，我们得到标准正态分布 $\\mathcal{N}(x;0,1)$，其概率密度如下图所示：\n",
    "\n",
    "<center><img src=\"images/normal.png\" /></center>\n",
    "\n",
    "如果我们改变图像由于噪声在多个时间步骤中多次出现，我们将 $\\mathbf{x}_{t}$ 描述为时间步骤 $t$ 处的图像。然后，$\\mathbf{x}_{t-1}$ 将是前一个时间步骤处的图像，而 $x_{0}$ 将是原始图像。\n",
    "\n",
    "在之前的笔记本中，我们使用以下公式向图像添加噪声：\n",
    "\n",
    "$q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})=\\mathcal{N}(\\mathbf{x}_{t};(1-\\beta_{t}) \\cdot \\mathbf{x}_{t-1},\\beta_{t}^{2} \\cdot \\mathbf{I})$\n",
    "\n",
    "其中 $q$ 表示 [正向扩散过程](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#forward-diffusion-process) ，$q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})$ 描述了基于 $\\mathbf{x}_{t-1}$ 的新噪声图像 $\\mathbf{x}_{t}$ 的概率分布。\n",
    "\n",
    "这次，我们将使用类似的方程式来改变图像：\n",
    "\n",
    "$q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})=\\mathcal{N}(\\mathbf{x}_{t};\\sqrt{1-\\beta_{t}} \\cdot \\mathbf{x}_{t-1},\\beta_{t} \\cdot \\mathbf{I})$\n",
    "\n",
    "我们可以从这个概率分布中抽样，首先使用 [torch.randn_like](https://pytorch.org/docs/stable/generated/torch.randn_like.html) 从标准正态分布 $\\mathcal{N}(x;0,1)$ 中抽样：\n",
    "\n",
    "`noise = torch.randn_like(x_t)`\n",
    "\n",
    "然后我们可以将噪声相乘并添加到 `q` 中以进行抽样：\n",
    "\n",
    "`x_t = torch.sqrt(1 - B[t]) * x_t + torch.sqrt(B[t]) * noise`\n",
    "\n",
    "让我们在实践中看看这一切。运行下面的代码单元，对我们数据集的第一个图像执行 `T` （或 `150` ）次正向扩散。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "executionInfo": {
     "elapsed": 41034,
     "status": "ok",
     "timestamp": 1690448390469,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "UzIMeK6YRmIt",
    "outputId": "5c90fddf-43bc-4f3f-8374-6a4609bdd79e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "x_0 = data[0][0].to(device)  # Initial image\n",
    "x_t = x_0  # Set up recursion\n",
    "xs = []  # Store x_t for each T to see change\n",
    "\n",
    "for t in range(T):\n",
    "    noise = torch.randn_like(x_t)\n",
    "    x_t = torch.sqrt(1 - B[t]) * x_t + torch.sqrt(B[t]) * noise  # sample from q(x_t|x_t-1)\n",
    "    img = torch.squeeze(x_t).cpu()\n",
    "    xs.append(img)\n",
    "    ax = plt.subplot(nrows, ncols, t + 1)\n",
    "    ax.axis(\"off\")\n",
    "    plt.imshow(img)\n",
    "plt.savefig(\"forward_diffusion.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72jh9eF33R12"
   },
   "source": [
    "或者以动画形式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 910,
     "output_embedded_package_id": "1RQgJ25OqX2eHWritA1w734Vn0JzygjtQ"
    },
    "executionInfo": {
     "elapsed": 18548,
     "status": "ok",
     "timestamp": 1690448409015,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "TMexBq_Px8si",
    "outputId": "6b34799c-d21c-4955-ca08-702524296bdd"
   },
   "outputs": [],
   "source": [
    "gif_name = \"forward_diffusion.gif\"\n",
    "other_utils.save_animation(xs, gif_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(open(gif_name,'rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHi36Uy2Rm7r"
   },
   "source": [
    "## 2.2 省略噪声处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdPPaco1wLLO"
   },
   "source": [
    "我们可以对数据集中的每个图像进行 `T` 次噪声处理，以创建 `T` 个新图像，但我们需要这样做吗？\n",
    "\n",
    "借助递归的力量，我们可以估算出给定我们的测试计划 $\\beta_t$ 后 $x_t$ 会是什么样子。完整的数学分解可以在 [Lilian Weng 的博客](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#speed-up-diffusion-model-sampling) 中找到。\n",
    "让我们带回 `a`lpha，它是 $\\beta$ 的补充。我们可以将 $\\alpha_t$ 定义为 $1 - \\beta_t$，我们可以将 $\\bar{\\alpha}_t$ 定义为 $\\alpha_t$ 的 [累积乘积](https://pytorch.org/docs/stable/generated/torch.cumprod.html)。\n",
    "\n",
    "例如，$\\bar{\\alpha}_3 = \\alpha_0 \\cdot \\alpha_1 \\cdot \\alpha_2 \\cdot \\alpha_3$\n",
    "\n",
    "由于符号为条形，我们将 $\\bar{\\alpha}_t$ 称为 `a_bar`。我们新的噪声图像分布变为：\n",
    "\n",
    "$q(\\mathbf{x}_{t}|\\mathbf{x}_{0})=\\mathcal{N}(\\mathbf{x}_{t};\\sqrt{\\bar{\\alpha}_{t}} \\cdot x_{0},(1 - \\bar{\\alpha}_t) \\cdot \\mathbf{I})$\n",
    "\n",
    "转换为代码如下：\n",
    "\n",
    "`x_t = sqrt_a_bar_t * x_0 + sqrt_one_minus_a_bar_t * noise`\n",
    "\n",
    "我们现在不再依赖 $\\mathbf{x}_{t-1}$，可以从 $x_0$ 估计 $\\mathbf{x}_t$。让我们在代码中定义这些变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myGGt3V7_Y1O"
   },
   "outputs": [],
   "source": [
    "a = 1. - B\n",
    "a_bar = torch.cumprod(a, dim=0)\n",
    "sqrt_a_bar = torch.sqrt(a_bar)  # Mean Coefficient\n",
    "sqrt_one_minus_a_bar = torch.sqrt(1 - a_bar) # St. Dev. Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GILry2GTF36w"
   },
   "source": [
    "我们已经准备好了所有要素，让我们来编写前向扩散采样函数 `q`：\n",
    "\n",
    "$q(\\mathbf{x}_{t}|\\mathbf{x}_{0})=\\mathcal{N}(\\mathbf{x}_{t};\\sqrt{\\bar{\\alpha}_{t}} \\cdot \\mathbf{x}_{0},(1 - \\bar{\\alpha}_t) \\cdot \\mathbf{I})$\n",
    "\n",
    "目前，`sqrt_a_bar` 和 `sqrt_one_minus_a_bar` 只有一个维度，如果我们用 `t` 索引它们，它们每个都只有一个值。如果我们想将此值与图像中的每个像素值相乘，我们需要匹配维度数才能[广播](https://numpy.org/doc/stable/user/basics.broadcasting.html)。\n",
    "\n",
    "<center><img src=\"images/broadcasting.png\" width=\"60%\" /></center>\n",
    "\n",
    "我们可以通过使用 `None` 进行索引来添加额外的维度。这是 PyTorch 的快捷方式，用于向结果张量添加额外的维度。作为参考，一批图像的尺寸为： `批次维度 x 图像通道 x 图像高度 x 图像宽度` 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ih6Hcg9_FJLZ"
   },
   "outputs": [],
   "source": [
    "def q(x_0, t):\n",
    "    \"\"\"\n",
    "    Samples a new image from q\n",
    "    Returns the noise applied to an image at timestep t\n",
    "    x_0: the original image\n",
    "    t: timestep\n",
    "    \"\"\"\n",
    "    t = t.int()\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_a_bar_t = sqrt_a_bar[t, None, None, None]\n",
    "    sqrt_one_minus_a_bar_t = sqrt_one_minus_a_bar[t, None, None, None]\n",
    "\n",
    "    x_t = sqrt_a_bar_t * x_0 + sqrt_one_minus_a_bar_t * noise\n",
    "    return x_t, noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyP6MdSfGStZ"
   },
   "source": [
    "让我们测试一下这种新方法，并将其与旧的递归生成图像的方法进行比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "executionInfo": {
     "elapsed": 20057,
     "status": "ok",
     "timestamp": 1690448429069,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "FJU4Fy8JG5f6",
    "outputId": "8e355b67-f974-45dc-d8dd-137763084d3b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "xs = []\n",
    "\n",
    "for t in range(T):\n",
    "    t_tenser = torch.Tensor([t]).type(torch.int64)\n",
    "    x_t, _ = q(x_0, t_tenser)\n",
    "    img = torch.squeeze(x_t).cpu()\n",
    "    xs.append(img)\n",
    "    ax = plt.subplot(nrows, ncols, t + 1)\n",
    "    ax.axis('off')\n",
    "    other_utils.show_tensor_image(x_t)\n",
    "plt.savefig(\"forward_diffusion_skip.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 910,
     "output_embedded_package_id": "1XMyoAR8iugnUieuJIBQyWG8a5ClNQJG_"
    },
    "executionInfo": {
     "elapsed": 21978,
     "status": "ok",
     "timestamp": 1690448451035,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "Okel8MIzJdNT",
    "outputId": "a8918c27-afa8-482a-c943-4d133dbe378a"
   },
   "outputs": [],
   "source": [
    "gif_name = \"forward_diffusion_skip.gif\"\n",
    "other_utils.save_animation(xs, gif_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(open(gif_name,'rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIQxClokPHVS"
   },
   "source": [
    "与之前的技术相比，你看出来有什么不同吗？当按顺序添加噪声时，连续时间步长的图像之间的差异较小。尽管如此，神经网络在反向扩散过程中仍能很好地将噪声与原始图像分离。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILsgVFmycEEM"
   },
   "source": [
    "## 2.3 预测噪声"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRBp3ogo6SFn"
   },
   "source": [
    "我们的神经网络架构与之前基本相同。但是，由于添加的噪声量会随着每个时间步骤而变化，因此我们需要一种方法来告诉模型我们的输入图像处于哪个时间步骤。\n",
    "\n",
    "为此，我们可以创建一个如下所示的嵌入块。\n",
    "* `input_dim` 是我们想要嵌入的值的维数。我们将嵌入 `t`，它是一个一维标量。\n",
    "* `emb_dim` 是我们希望使用 [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) 层将输入值转换为的维数。\n",
    "* [UnFlatten](https://pytorch.org/docs/stable/generated/torch.nn.Unflatten.html) 用于将向量重塑为多维空间。由于我们要将此嵌入的结果添加到多维特征图中，因此我们将添加一些额外的维度，类似于我们在上面的 `q` 函数中扩展维度的方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OjyvTEgIKmDr"
   },
   "outputs": [],
   "source": [
    "class EmbedBlock(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.Unflatten(1, (emb_dim, 1, 1))\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(-1, self.input_dim)\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_b_SGxJ-63sv"
   },
   "source": [
    "我们将把这个时间嵌入块添加到 U-Net 的每个 `UpBlock` 中，从而得到以下架构。\n",
    "\n",
    "<center><img src=\"images/time_nn.png\" width=\"80%\" /></center>\n",
    "\n",
    "**TODO**：我们的 `DownBlock` 与之前相同。使用上图作为参考，您能用正确的变量替换 `FIXME` 吗？每个 `FIXME` 可以是以下之一：\n",
    "* `in_chs`\n",
    "* `out_chs`\n",
    "* `kernel_size`\n",
    "* `stride`\n",
    "* `padding`\n",
    "\n",
    "单击下面的 `...` 获取正确答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-ACwMgFKYlB"
   },
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_chs, out_chs):\n",
    "        kernel_size = 3\n",
    "        stride = 1\n",
    "        padding = 1\n",
    "\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(FIXME, FIXME, FIXME, FIXME, FIXME),\n",
    "            nn.BatchNorm2d(FIXME),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(FIXME, FIXME, FIXME, FIXME, FIXME),\n",
    "            nn.BatchNorm2d(FIXME),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_chs, out_chs):\n",
    "        kernel_size = 3\n",
    "        stride = 1\n",
    "        padding = 1\n",
    "\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_chs, out_chs, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_chs),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_chs, out_chs, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_chs),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrDOcqlbQArT"
   },
   "source": [
    "`UpBlock` 遵循类似的逻辑，但使用的是 [转置卷积](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html)。\n",
    "\n",
    "**TODO**：您能用正确的变量替换 `FIXME` 吗？每个 `FIXME` 可以是以下之一：\n",
    "* `in_chs`\n",
    "* `out_chs`\n",
    "* `kernel_size`\n",
    "* `stride`\n",
    "* `padding`\n",
    "* `strideT`\n",
    "* `out_paddingT`\n",
    "* `x`\n",
    "* `skip`\n",
    "\n",
    "单击下面的 `...` 获取正确答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJFm-gnaKZY-"
   },
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_chs, out_chs):\n",
    "        # Convolution variables\n",
    "        kernel_size = 3\n",
    "        stride = 1\n",
    "        padding = 1\n",
    "\n",
    "        # Transpose variables\n",
    "        strideT = 2\n",
    "        out_paddingT = 1\n",
    "\n",
    "        super().__init__()\n",
    "        # 2 * in_chs for concatenated skip connection\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(FIXME, FIXME, FIXME, FIXME, FIXME, FIXME),\n",
    "            nn.BatchNorm2d(FIXME),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(FIXME, FIXME, FIXME, FIXME, FIXME),\n",
    "            nn.BatchNorm2d(FIXME),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, skip):\n",
    "        x = torch.cat((FIXME, FIXME), 1)\n",
    "        x = self.model(FIXME)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_chs, out_chs):\n",
    "        # Convolution variables\n",
    "        kernel_size = 3\n",
    "        stride = 1\n",
    "        padding = 1\n",
    "\n",
    "        # Transpose variables\n",
    "        strideT = 2\n",
    "        out_paddingT = 1\n",
    "\n",
    "        super().__init__()\n",
    "        # 2 * in_chs for concatenated skip connection\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(2 * in_chs, out_chs, kernel_size, strideT, padding, out_paddingT),\n",
    "            nn.BatchNorm2d(out_chs),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_chs, out_chs, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_chs),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, skip):\n",
    "        x = torch.cat((x, skip), 1)\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2m56jgvvU_OB"
   },
   "source": [
    "最终的 U-Net 与我们在上一个笔记本中使用的类似。不同之处在于我们现在有一个连接到 `UpBlock` 的时间嵌入。\n",
    "\n",
    "**TODO**：虽然时间嵌入已集成到模型中，但仍有许多 `FIXME` 需要替换。这次，图像通道、上行通道和下行通道需要修复。您能否上下移动 U-Net 以在每个步骤中设置正确的通道数？\n",
    "\n",
    "每个 `FIXME` 可以是：\n",
    "* `img_chs`\n",
    "* `down_chs` 中的值\n",
    "* `up_chs` 中的值\n",
    "\n",
    "单击下面的 `...` 获取正确答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        img_chs = IMG_CH\n",
    "        down_chs = (16, 32, 64)\n",
    "        up_chs = down_chs[::-1]  # Reverse of the down channels\n",
    "        latent_image_size = IMG_SIZE // 4 # 2 ** (len(down_chs) - 1)\n",
    "        t_dim = 1 # New\n",
    "\n",
    "        # Inital convolution\n",
    "        self.down0 = nn.Sequential(\n",
    "            nn.Conv2d(FIXME, down_chs[0], 3, padding=1),\n",
    "            nn.BatchNorm2d(FIXME),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Downsample\n",
    "        self.down1 = DownBlock(down_chs[0], down_chs[1])\n",
    "        self.down2 = DownBlock(FIXME, FIXME)\n",
    "        self.to_vec = nn.Sequential(nn.Flatten(), nn.ReLU())\n",
    "        \n",
    "        # Embeddings\n",
    "        self.dense_emb = nn.Sequential(\n",
    "            nn.Linear(FIXME*latent_image_size**2, down_chs[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(down_chs[1], FIXME),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(down_chs[1], down_chs[2]*latent_image_size**2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.temb_1 = EmbedBlock(t_dim, up_chs[0]) # New\n",
    "        self.temb_2 = EmbedBlock(t_dim, up_chs[1]) # New\n",
    "        \n",
    "        # Upsample\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.Unflatten(1, (FIXME, latent_image_size, latent_image_size)),\n",
    "            nn.Conv2d(FIXME, up_chs[0], 3, padding=1),\n",
    "            nn.BatchNorm2d(up_chs[0]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UpBlock(up_chs[0], up_chs[1])\n",
    "        self.up2 = UpBlock(FIXME, FIXME)\n",
    "\n",
    "        # Match output channels\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(FIXME, FIXME, 3, 1, 1),\n",
    "            nn.BatchNorm2d(up_chs[-1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(up_chs[-1], img_chs, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        down0 = self.down0(x)\n",
    "        down1 = self.down1(down0)\n",
    "        down2 = self.down2(down1)\n",
    "        latent_vec = self.to_vec(down2)\n",
    "        \n",
    "        latent_vec = self.dense_emb(latent_vec)\n",
    "        # New\n",
    "        t = t.float() / T  # Convert from [0, T] to [0, 1]\n",
    "        temb_1 = self.temb_1(t)\n",
    "        temb_2 = self.temb_2(t)\n",
    "\n",
    "        up0 = self.up0(latent_vec)\n",
    "        up1 = self.up1(up0+temb_1, down2)\n",
    "        up2 = self.up2(up1+temb_2, down1)\n",
    "        return self.out(up2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        img_chs = IMG_CH\n",
    "        down_chs = (16, 32, 64)\n",
    "        up_chs = down_chs[::-1]  # Reverse of the down channels\n",
    "        latent_image_size = IMG_SIZE // 4 # 2 ** (len(down_chs) - 1)\n",
    "        t_dim = 1 # New\n",
    "\n",
    "        # Inital convolution\n",
    "        self.down0 = nn.Sequential(\n",
    "            nn.Conv2d(img_chs, down_chs[0], 3, padding=1),\n",
    "            nn.BatchNorm2d(down_chs[0]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Downsample\n",
    "        self.down1 = DownBlock(down_chs[0], down_chs[1])\n",
    "        self.down2 = DownBlock(down_chs[1], down_chs[2])\n",
    "        self.to_vec = nn.Sequential(nn.Flatten(), nn.ReLU())\n",
    "        \n",
    "        # Embeddings\n",
    "        self.dense_emb = nn.Sequential(\n",
    "            nn.Linear(down_chs[2]*latent_image_size**2, down_chs[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(down_chs[1], down_chs[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(down_chs[1], down_chs[2]*latent_image_size**2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.temb_1 = EmbedBlock(t_dim, up_chs[0])  # New\n",
    "        self.temb_2 = EmbedBlock(t_dim, up_chs[1])  # New\n",
    "        \n",
    "        # Upsample\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.Unflatten(1, (up_chs[0], latent_image_size, latent_image_size)),\n",
    "            nn.Conv2d(up_chs[0], up_chs[0], 3, padding=1),\n",
    "            nn.BatchNorm2d(up_chs[0]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UpBlock(up_chs[0], up_chs[1])\n",
    "        self.up2 = UpBlock(up_chs[1], up_chs[2])\n",
    "\n",
    "        # Match output channels\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(up_chs[-1], up_chs[-1], 3, 1, 1),\n",
    "            nn.BatchNorm2d(up_chs[-1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(up_chs[-1], img_chs, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        down0 = self.down0(x)\n",
    "        down1 = self.down1(down0)\n",
    "        down2 = self.down2(down1)\n",
    "        latent_vec = self.to_vec(down2)\n",
    "        \n",
    "        # New\n",
    "        t = t.float() / T  # Convert from [0, T] to [0, 1]\n",
    "        latent_vec = self.dense_emb(latent_vec)\n",
    "        temb_1 = self.temb_1(t)\n",
    "        temb_2 = self.temb_2(t)\n",
    "\n",
    "        up0 = self.up0(latent_vec)\n",
    "        up1 = self.up1(up0+temb_1, down2)\n",
    "        up2 = self.up2(up1+temb_2, down1)\n",
    "        return self.out(up2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1690448451038,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "8buyYGqLOiNP",
    "outputId": "ef8537b0-b494-4570-ee17-3d8738c2f73c"
   },
   "outputs": [],
   "source": [
    "model = UNet()\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
    "model = torch.compile(UNet().to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xw5Pw1VfV2m7"
   },
   "source": [
    "### 2.3.1 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYNL_m_tV5yn"
   },
   "source": [
    "在上一个笔记本中，我们使用了 [均方误差](https://developers.google.com/machine-learning/glossary#mean-squared-error-mse) 损失函数，比较了原始图像和基于噪声的预测原始图像。\n",
    "\n",
    "这次，我们将比较添加到图像中的真实噪声和预测噪声。Lilian Weng 在这篇 [博客文章](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#parameterization-of-l_t-for-training-loss) 中介绍了数学知识。最初，损失函数基于 [证据下限 (ELBO)](https://en.wikipedia.org/wiki/Evidence_lower_bound) [对数似然](https://mathworld.wolfram.com/Log-LikelihoodFunction.html)，但在 [去噪扩散概率模型论文](https://arxiv.org/abs/2006.11239) 中发现，预测噪声与真实噪声之间的均方误差在实践中更优。如果感兴趣，Lilian Weng 会在 [这里](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#reverse-diffusion-process) 介绍推导过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxwt6WrnRqF4"
   },
   "outputs": [],
   "source": [
    "def get_loss(model, x_0, t):\n",
    "    x_noisy, noise = q(x_0, t)\n",
    "    noise_pred = model(x_noisy, t/T) # Normalize t to be from 0 to 1\n",
    "    return F.mse_loss(noise, noise_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 逆向扩散"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在有一个模型可以预测在时间步长 `t` 添加到图像中的噪声，但生成图像并不像反复减去和添加噪声那么简单。`q` 函数可以反转，这样我们就可以从$\\mathbf{x}_t$生成$\\mathbf{x}_(t-1)$。\n",
    "\n",
    "$q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_{t-1};{\\mathbf{\\tilde{\\mu}}}(\\mathbf{x_t},\\mathbf{x_0}), \\tilde{\\beta}_t \\cdot \\mathbf{I})$\n",
    "\n",
    "**注意**：$\\tilde{\\beta}_t$最初计算为$\\frac{1-\\overline{a}_{t-1}}{1-\\overline{a}_{t}}\\beta_t$，但实际中只使用$\\beta_t$更为有效。\n",
    "\n",
    "使用 [贝叶斯定理](https://en.wikipedia.org/wiki/Bayes%27_theorem)，我们可以得出时间步长 `t` 的模型平均值 `u_t` 的方程。\n",
    "\n",
    "${\\mathbf{\\tilde{\\mu}}}_t = \\frac{1}{\\sqrt{\\alpha_t}}(\\mathbf{x_t}-\\frac{1-\\alpha_t}{\\sqrt{1-\\overline{\\alpha_t}}}\\mathbf{\\epsilon}_t)$\n",
    "\n",
    "图像 $\\mathbf{x}_{t-1}$ 可以通过 ${\\mathbf{\\tilde{\\mu}}}_t + \\tilde{\\beta}_t \\cdot \\mathbf{I}$ 来估计，因此我们将使用此方程递归生成样本图像，直到达到 `t == 0`。让我们看看这在代码中意味着什么。首先，我们将预先计算计算 `u_t` 所需的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt_a_inv = torch.sqrt(1 / a)\n",
    "pred_noise_coeff = (1 - a) / torch.sqrt(1 - a_bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将创建反向扩散函数 `reverse_q` 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4Wt6P13St5W"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def reverse_q(x_t, t, e_t):\n",
    "    t = torch.squeeze(t[0].int())  # All t values should be the same\n",
    "    pred_noise_coeff_t = pred_noise_coeff[t]\n",
    "    sqrt_a_inv_t = sqrt_a_inv[t]\n",
    "    u_t = sqrt_a_inv_t * (x_t - pred_noise_coeff_t * e_t)\n",
    "    if t == 0:\n",
    "        return u_t  # Reverse diffusion complete!\n",
    "    else:\n",
    "        B_t = B[t-1]\n",
    "        new_noise = torch.randn_like(x_t)\n",
    "        return u_t + torch.sqrt(B_t) * new_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们创建一个函数来迭代地从图像中去除噪声，直到图像中没有噪声为止。我们还将显示这些图像，以便我们可以看到模型的改进情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJopDkbQR60S"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_images(ncols, figsize=(8,8)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.axis(\"off\")\n",
    "    hidden_rows = T / ncols\n",
    "\n",
    "    # Noise to generate images from\n",
    "    x_t = torch.randn((1, IMG_CH, IMG_SIZE, IMG_SIZE), device=device)\n",
    "\n",
    "    # Go from T to 0 removing and adding noise until t = 0\n",
    "    plot_number = 1\n",
    "    for i in range(0, T)[::-1]:\n",
    "        t = torch.full((1,), i, device=device)\n",
    "        e_t = model(x_t, t)  # Predicted noise\n",
    "        x_t = reverse_q(x_t, t, e_t)\n",
    "        if i % hidden_rows == 0:\n",
    "            ax = plt.subplot(1, ncols+1, plot_number)\n",
    "            ax.axis('off')\n",
    "            other_utils.show_tensor_image(x_t.detach().cpu())\n",
    "            plot_number += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "是时候训练模型了！怎么样？看起来模型正在学习吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 92063,
     "status": "ok",
     "timestamp": 1690448543091,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "BncOxHBrTDly",
    "outputId": "160b838d-afb8-45fc-f7c2-fb2232bbdb62"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "epochs = 3\n",
    "ncols = 15  # Should evenly divide T\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        t = torch.randint(0, T, (BATCH_SIZE,), device=device)\n",
    "        x = batch[0].to(device)\n",
    "        loss = get_loss(model, x, t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 1 == 0 and step % 100 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {step:03d} | Loss: {loss.item()} \")\n",
    "            sample_images(ncols)\n",
    "print(\"Final sample:\")\n",
    "sample_images(ncols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vq_hWtK2cI2K"
   },
   "source": [
    "如果你眯起眼睛，你能看出模型正在生成什么吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7384,
     "status": "ok",
     "timestamp": 1690448550463,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "Z-zupBmzcUuH",
    "outputId": "6602d55c-20fe-408d-8f43-d17c98a119d0"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "figsize=(8,8) # Change me\n",
    "ncols = 3 # Should evenly divide T\n",
    "for _ in range(10):\n",
    "    sample_images(ncols, figsize=figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 下一步"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型正在学习……一些东西。它看起来有点像素化。为什么会这样？继续阅读下一篇笔记本以了解更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/DLI_Header.png\" alt=\"标题\" style=\"width: 400px;\"/></center>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMPRcXdvypGa4ncx029KLSM",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
