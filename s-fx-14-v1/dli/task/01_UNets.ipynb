{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-a8GRH8DXKk"
   },
   "source": [
    "<center><img src=\"images/DLI_Header.png\" alt=\"标题\" style=\"width: 400px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTQNPPpeDaOk"
   },
   "source": [
    "# 1. 从 U-Net 到 Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81PfOMFwKBZG"
   },
   "source": [
    "U-Nets 是一种卷积神经网络，最初是为医学成像而设计的。例如，我们可以向网络输入心脏图像，它可以返回另一张突出显示潜在癌变区域的图片。\n",
    "\n",
    "我们可以使用这个过程来生成新图像吗？这里有一个想法：如果我们在图像中添加噪声，然后使用 U-Net 将图像与噪声分离，结果会怎样？然后我们可以向模型输入噪声并让它创建可识别的图像吗？让我们试一试吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Pkc4vj3csv3"
   },
   "source": [
    "#### 学习目标\n",
    "\n",
    "本笔记本的目标是：\n",
    "* 探索 FashionMNIST 数据集\n",
    "* 构建 U-Net 架构\n",
    "  * 构建 Down Block\n",
    "  * 构建 Up Block\n",
    "* 训练模型以消除图像中的噪音\n",
    "* 尝试生成服装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "executionInfo": {
     "elapsed": 6260,
     "status": "error",
     "timestamp": 1689940390619,
     "user": {
      "displayName": "Devesh Khandelwal US",
      "userId": "16954520040589783180"
     },
     "user_tz": 420
    },
    "id": "NNmWEhrB-uSm",
    "outputId": "6450de47-6418-4268-82b0-da2f8e3b9fe0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Visualization tools\n",
    "import graphviz\n",
    "from torchview import draw_graph\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAAgjE5K_ZFZ"
   },
   "source": [
    "在 PyTorch 中，我们可以通过将 [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device) 设置为 `cuda` 来在操作中使用 GPU。函数 `torch.cuda.is_available()` 将确认 PyTorch 可以识别 GPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1689675950620,
     "user": {
      "displayName": "Devesh Khandelwal US",
      "userId": "16954520040589783180"
     },
     "user_tz": 420
    },
    "id": "Dw5___FcYtBQ",
    "outputId": "f418f7d6-cf0c-4b23-87ad-140cc2b1501c"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1689675950620,
     "user": {
      "displayName": "Devesh Khandelwal US",
      "userId": "16954520040589783180"
     },
     "user_tz": 420
    },
    "id": "dfE6Lkmq_Tt4",
    "outputId": "b346ac80-72b6-4db0-e76b-f8cff013d312"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzHZsOWLCdmB"
   },
   "source": [
    "## 1.1 数据集\n",
    "\n",
    "为了练习生成图像，我们将使用 [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) 数据集。FashionMNIST 旨在成为图像分类问题的“Hello World”数据集。黑白图像的小尺寸（28 x 28 像素）也使其成为图像生成的一个很好的起点。\n",
    "\n",
    "FashionMNIST 包含在 [Torchvision](https://pytorch.org/vision/stable/index.html) 中，这是一个与 PyTorch 关联的计算机视觉库。下载数据集时，我们可以传递我们想要应用于图像的 [transformations](https://pytorch.org/vision/stable/transforms.html) 列表。现在，我们将使用 [ToTensor](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor) 将图像转换为张量，以便我们可以使用神经网络处理图像。这将自动将像素值从 [0, 255] 缩放到 [0, 1]。它还会将尺寸从 [高度 x 宽度 x 通道] 重新排列为 [通道 x 高度 x 宽度]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIRaYbZkDKQa"
   },
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    \"./data/\", download=True, transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以使用下面的代码对一些图像进行采样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "executionInfo": {
     "elapsed": 2859,
     "status": "ok",
     "timestamp": 1689675953471,
     "user": {
      "displayName": "Devesh Khandelwal US",
      "userId": "16954520040589783180"
     },
     "user_tz": 420
    },
    "id": "0ICXTYpQDyjv",
    "outputId": "bcb0235c-6d89-473d-997a-128ebc1ef0bb"
   },
   "outputs": [],
   "source": [
    "# Adjust for display; high w/h ratio recommended\n",
    "plt.figure(figsize=(16, 1))\n",
    "\n",
    "def show_images(dataset, num_samples=10):\n",
    "    for i, img in enumerate(dataset):\n",
    "        if i == num_samples:\n",
    "            return\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(torch.squeeze(img[0]))\n",
    "\n",
    "show_images(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8KfDZm7EqXA"
   },
   "source": [
    "让我们为我们的数据集设置一些导入常量。使用 U-Nets，通常通过 [Max Pooling](https://paperswithcode.com/method/max-pooling) 不断将特征图的大小减半。然后，使用 [Transposed Convolution](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html) 将特征图大小加倍。为了在 U-Net 上下移动时保持图像尺寸一致，如果图像大小可以被 `2` 整除多次，则会有所帮助。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIU9PzSJEdTp"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 16 # Due to stride and pooling, must be divisible by 2 multiple times\n",
    "IMG_CH = 1 # Black and white image, no color channels\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经定义了图像的目标大小，让我们创建一个函数来加载数据并将其转换为目标大小。我们将添加到图像中的随机噪声将从 [标准正态分布](https://mathworld.wolfram.com/NormalDistribution.html) 中采样，这意味着 68% 的噪声像素值将从 -1 到 1。我们将类似地将图像值缩放到从 -1 到 1。\n",
    "\n",
    "这也是应用随机图像增强的好地方。现在，我们将从 [RandomHorizontalFlip](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomHorizontalFlip.html#torchvision.transforms.RandomHorizontalFlip) 开始。我们不会使用 [RandomVericalFlip](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomVerticalFlip.html#torchvision.transforms.RandomVerticalFlip)，因为我们最终会生成颠倒的图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDeNBgkgGnKE"
   },
   "outputs": [],
   "source": [
    "def load_fashionMNIST(data_transform, train=True):\n",
    "    return torchvision.datasets.FashionMNIST(\n",
    "        \"./data/\",\n",
    "        download=True,\n",
    "        train=train,\n",
    "        transform=data_transform,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_transformed_fashionMNIST():\n",
    "    data_transforms = [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),  # Scales data into [0,1]\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Lambda(lambda t: (t * 2) - 1)  # Scale between [-1, 1]\n",
    "    ]\n",
    "\n",
    "    data_transform = transforms.Compose(data_transforms)\n",
    "    train_set = load_fashionMNIST(data_transform, train=True)\n",
    "    test_set = load_fashionMNIST(data_transform, train=False)\n",
    "    return torch.utils.data.ConcatDataset([train_set, test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNR_1j7iHYqt"
   },
   "outputs": [],
   "source": [
    "data = load_transformed_fashionMNIST()\n",
    "dataloader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vb_BfRbWEPyR"
   },
   "source": [
    "## 1.2 U-Net 架构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cjn5q_dtEe01"
   },
   "source": [
    "首先，让我们定义 U-Net 架构的不同组件。主要是 `DownBlock` 和 `UpBlock` 。\n",
    "\n",
    "### 1.2.1 Down Block\n",
    "\n",
    "`DownBlock` 是一种典型的卷积神经网络。如果您是 PyTorch 新手，并且具有 Keras/TensorFlow 背景，则以下内容更类似于 [函数式 API](https://keras.io/guides/functional_api/)，而不是 [顺序模型](https://keras.io/guides/sequation_model/)。我们稍后将使用 [残差](https://stats.stackexchange.com/questions/321054/what-are-residual-connections-in-rnns) 和跳过连接。顺序模型不具备支持这些类型连接的灵活性，但函数模型却具备。\n",
    "\n",
    "在下面的 `__init__` 函数中，我们将各种神经网络操作分配给类变量：\n",
    "* [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) 将卷积应用于输入。`in_ch` 是我们正在卷积的通道数，`out_ch` 是输出通道数，与用于卷积的内核过滤器数相同。通常在 U-Net 架构中，我们在模型中向下移动的通道数越多。\n",
    "* [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) 是卷积内核的激活函数。\n",
    "* [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html) 将 [批量归一化](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338) 应用于一层神经元。ReLu 没有可学习的参数，因此我们可以将同一函数应用于多个层，其效果与使用多个 ReLu 函数相同。批量归一化确实具有可学习的参数，重复使用此函数可能会产生意想不到的效果。\n",
    "* [MaxPool2D](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) 是我们将用来在特征图沿网络向下移动时减小其大小的方法。可以通过卷积实现此效果，但最大池化通常用于 U-Nets。\n",
    "\n",
    "在 `forward` 方法中，我们描述了如何将各种函数应用于输入。到目前为止，操作按以下顺序连续进行：\n",
    "* `Conv2d`\n",
    "* `BatchNorm2d`\n",
    "* `ReLU`\n",
    "* `Conv2d`\n",
    "* `BatchNorm2d`\n",
    "* `ReLU`\n",
    "* `MaxPool2d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zEV5ZSK0Hj6l"
   },
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        kernel_size = 3\n",
    "        stride = 1\n",
    "        padding = 1\n",
    "\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Up Block\n",
    "\n",
    "虽然 `DownBlock` 会减小特征图的大小，但 `UpBlock` 会将其加倍。这是通过 [ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html) 实现的。我们可以使用与 `DownBlock` 几乎相同的架构，但我们将用 convT 替换 conv2。转置的 `步幅` 为 2，将导致加倍，并带有适当数量的 `填充` 。\n",
    "\n",
    "让我们使用下面的代码块进行一些练习。我们设置了一个示例，通过创建 `1` 的测试图像来测试此功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch, h, w = 1, 3, 3\n",
    "x = torch.ones(1, ch, h, w)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以使用恒等 `内核` 来查看 `conv_transpose2d` 如何改变输入图像。恒等内核只有一个 `1` 值。当用于卷积时，输出将与输入相同。\n",
    "\n",
    "尝试更改下面的 `stride` 、 `padding` 和 `output_padding` 。结果符合您的预期吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = torch.tensor([[1.]])  # Identity kernel\n",
    "kernel = kernel.view(1, 1, 1, 1).repeat(1, ch, 1, 1) # Make into a batch\n",
    "\n",
    "output = F.conv_transpose2d(x, kernel, stride=1, padding=0, output_padding=0)[0]\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "内核大小也会影响输出特征图的大小。尝试更改下面的 `kernel_size` 。注意输出图像如何随着内核大小的增加而扩大？这与常规卷积相反，在常规卷积中，较大的内核大小会减小输出特征图的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 3\n",
    "kernel = torch.ones(1, 1, kernel_size, kernel_size)\n",
    "\n",
    "output = F.conv_transpose2d(x, kernel, stride=1, padding=0, output_padding=0)[0]\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHtzWNIBLdnX"
   },
   "source": [
    "另一个有趣的区别是：我们将输入通道乘以 2。这是为了适应跳过连接。我们将把 `UpBlock` 匹配的 `DownBlock` 的输出与 `UpBlock` 的输入连接起来。\n",
    "\n",
    "<center><img src=\"images/FMUNet.png\" width=\"600\" /></center>\n",
    "\n",
    "如果 x 是输入特征图的大小，则输出大小为：\n",
    "\n",
    "`new_x = (x - 1) * stride + kernel_size - 2 * padding + out_padding`\n",
    "\n",
    "如果 stride = 2 且 out_padding = 1，则为了将输入特征图的大小加倍：\n",
    "\n",
    "`kernel_size = 2 * padding + 1`\n",
    "\n",
    "操作与之前几乎相同，但有两点不同：\n",
    "* `ConvTranspose2d` - 卷积转置而不是卷积\n",
    "* `BatchNorm2d`\n",
    "* `ReLU`\n",
    "* `Conv2d`\n",
    "* `BatchNorm2d`\n",
    "* `ReLU`\n",
    "* ~~`MaxPool2d`~~ - 扩大而不是缩小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KV5lmZ4pH22N"
   },
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        # Convolution variables\n",
    "        kernel_size = 3\n",
    "        stride = 1\n",
    "        padding = 1\n",
    "\n",
    "        # Transpose variables\n",
    "        strideT = 2\n",
    "        out_paddingT = 1\n",
    "\n",
    "        super().__init__()\n",
    "        # 2 * in_chs for concatednated skip connection\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(2 * in_ch, out_ch, kernel_size, strideT, padding, out_paddingT),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, skip):\n",
    "        x = torch.cat((x, skip), 1)\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3NXiknvZYjm"
   },
   "source": [
    "### 1.2.3 完整的 U-Net\n",
    "\n",
    "终于到了把它们拼凑起来的时候了！下面，我们有完整的 `UNet` 模型。\n",
    "\n",
    "在 `__init__` 函数中，我们可以使用 `down_chs` 定义 U-Net 每一步的通道数。当前默认值为 `(16, 32, 64)`，这意味着数据在模型中移动时的当前维度为：\n",
    "\n",
    "* input: 1 x 16 x 16\n",
    "* down0: 16 x 16 x 16\n",
    "  * down1: 32 x 8 x 8\n",
    "    * down2: 64 x 4 x 4\n",
    "      * dense_emb: 1024\n",
    "    * up0: 64 x 4 x 4\n",
    "  * up1: 64 x 8 x 8\n",
    "* up2: 32 x 16 x 16\n",
    "* out: 1 x 16 x 16\n",
    "\n",
    "`forward` 类方法是我们最终添加跳过连接的地方。对于 U-Net 中的每一步，我们将跟踪每个 `DownBlock` 的输出。然后，当我们移动 `UpBlock` 时，我们将[连接](https://pytorch.org/docs/stable/generated/torch.cat.html)前一个 `UpBlock` 的输出与其对应的 `DownBlock`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2884,
     "status": "ok",
     "timestamp": 1688418501290,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "QS0SCfWPI0DY",
    "outputId": "40929df8-9702-4610-bc52-cb2fe119df53"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        img_ch = IMG_CH\n",
    "        down_chs = (16, 32, 64)\n",
    "        up_chs = down_chs[::-1]  # Reverse of the down channels\n",
    "        latent_image_size = IMG_SIZE // 4 # 2 ** (len(down_chs) - 1)\n",
    "\n",
    "        # Inital convolution\n",
    "        self.down0 = nn.Sequential(\n",
    "            nn.Conv2d(img_ch, down_chs[0], 3, padding=1),\n",
    "            nn.BatchNorm2d(down_chs[0]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Downsample\n",
    "        self.down1 = DownBlock(down_chs[0], down_chs[1])\n",
    "        self.down2 = DownBlock(down_chs[1], down_chs[2])\n",
    "        self.to_vec = nn.Sequential(nn.Flatten(), nn.ReLU())\n",
    "        \n",
    "        # Embeddings\n",
    "        self.dense_emb = nn.Sequential(\n",
    "            nn.Linear(down_chs[2]*latent_image_size**2, down_chs[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(down_chs[1], down_chs[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(down_chs[1], down_chs[2]*latent_image_size**2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Upsample\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.Unflatten(1, (up_chs[0], latent_image_size, latent_image_size)),\n",
    "            nn.Conv2d(up_chs[0], up_chs[0], 3, padding=1),\n",
    "            nn.BatchNorm2d(up_chs[0]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UpBlock(up_chs[0], up_chs[1])\n",
    "        self.up2 = UpBlock(up_chs[1], up_chs[2])\n",
    "\n",
    "        # Match output channels\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(up_chs[-1], up_chs[-1], 3, 1, 1),\n",
    "            nn.BatchNorm2d(up_chs[-1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(up_chs[-1], img_ch, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        down0 = self.down0(x)\n",
    "        down1 = self.down1(down0)\n",
    "        down2 = self.down2(down1)\n",
    "        latent_vec = self.to_vec(down2)\n",
    "\n",
    "        up0 = self.up0(latent_vec)\n",
    "        up1 = self.up1(up0, down2)\n",
    "        up2 = self.up2(up1, down1)\n",
    "        return self.out(up2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet()\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sxj6aHoqY1EH"
   },
   "source": [
    "让我们使用 [torchview](https://github.com/mert-kurttutan/torchview) 验证模型架构。如果我们有三个 `down_chs`，则应该有两个 `DownBlock`，每个转换一个。同样，应该有两个 `UpBlock`。我们还应该检查是否有一个跳跃连接。U-Net 的“底部”不需要跳跃连接，因此每个 `UpBlock` 减一都有一个跳跃连接。\n",
    "\n",
    "最后，输出尺寸是否与输入尺寸相同？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1688418501827,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "vL3tWakdYzgl",
    "outputId": "9482d43b-2c2a-46f1-ee5c-f04c489fe114"
   },
   "outputs": [],
   "source": [
    "graphviz.set_jupyter_format('png')\n",
    "model_graph = draw_graph(\n",
    "    model,\n",
    "    input_size=(BATCH_SIZE, IMG_CH, IMG_SIZE, IMG_SIZE),\n",
    "    device='meta',\n",
    "    expand_nested=True\n",
    ")\n",
    "model_graph.resize_graph(scale=1.5)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brm8m3XQJDIS"
   },
   "source": [
    "在 [PyTorch 2.0](https://pytorch.org/get-started/pytorch-2.0/) 中，我们可以编译模型以加快训练速度。它会将操作列表发送到我们的 GPU，以便它可以像装配线一样将这些操作应用于我们的输入。有关更多信息，请阅读 [此处](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ta63x_ZyI3o9"
   },
   "outputs": [],
   "source": [
    "model = torch.compile(UNet().to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chH-4PKtc0Sh"
   },
   "source": [
    "## 1.3 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFecpW0bn9CR"
   },
   "source": [
    "让我们尝试在图像中添加噪音，看看我们的 U-Net 模型是否可以将其过滤掉。我们可以定义一个参数 `beta` 来表示我们的图像中噪音占原始图像的百分比。我们可以使用 `alpha` 来表示 `beta` 的[补充](https://brilliant.org/wiki/probability-by-complement/)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n2R5dSecdtVU"
   },
   "outputs": [],
   "source": [
    "def add_noise(imgs):\n",
    "    dev = imgs.device\n",
    "    percent = .5 # Try changing from 0 to 1\n",
    "    beta = torch.tensor(percent, device=dev)\n",
    "    alpha = torch.tensor(1 - percent, device=dev)\n",
    "    noise = torch.randn_like(imgs)\n",
    "    return alpha * imgs + beta * noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "algaplyFowBa"
   },
   "source": [
    "接下来，我们将损失函数定义为原始图像和预测图像之间的[均方误差](https://developers.google.com/machine-learning/glossary#mean-squared-error-mse)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ued4xgWIcSwU"
   },
   "outputs": [],
   "source": [
    "def get_loss(model, imgs):\n",
    "    imgs_noisy = add_noise(imgs)\n",
    "    imgs_pred = model(imgs_noisy)\n",
    "    return F.mse_loss(imgs, imgs_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iY7Rm5UqTxzH"
   },
   "source": [
    "为了显示我们模型的输出，我们需要将其转换回 CPU 上的图像格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9X8l-xqiVTg"
   },
   "outputs": [],
   "source": [
    "def show_tensor_image(image):\n",
    "    reverse_transforms = transforms.Compose([\n",
    "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "        transforms.Lambda(lambda t: torch.minimum(torch.tensor([1]), t)),\n",
    "        transforms.Lambda(lambda t: torch.maximum(torch.tensor([0]), t)),\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "    plt.imshow(reverse_transforms(image[0].detach().cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3L_YXMiHWjYZ"
   },
   "source": [
    "为了在训练期间看到改进效果，我们可以使用 [子图](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplot.html) 比较 `原始` 图像、 `添加噪声` 图像和 `预测原始` 图像。\n",
    "\n",
    "[@torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html) 将跳过使用此函数在训练期间计算梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkYV78lGg7UH"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def plot_sample(imgs):\n",
    "    # Take first image of batch\n",
    "    imgs = imgs[[0], :, :, :]\n",
    "    imgs_noisy = add_noise(imgs[[0], :, :, :])\n",
    "    imgs_pred = model(imgs_noisy)\n",
    "\n",
    "    nrows = 1\n",
    "    ncols = 3\n",
    "    samples = {\n",
    "        \"Original\" : imgs,\n",
    "        \"Noise Added\" : imgs_noisy,\n",
    "        \"Predicted Original\" : imgs_pred\n",
    "    }\n",
    "    for i, (title, img) in enumerate(samples.items()):\n",
    "        ax = plt.subplot(nrows, ncols, i+1)\n",
    "        ax.set_title(title)\n",
    "        show_tensor_image(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsNiGofWXedR"
   },
   "source": [
    "最后，到了关键时刻！是时候训练我们的模型并观察它的改进了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 58363,
     "status": "ok",
     "timestamp": 1688418560355,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "bfx4UOBOdOuY",
    "outputId": "9e653657-3f0d-4d29-aa68-8e205b493a68"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "epochs = 2\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = batch[0].to(device)\n",
    "        loss = get_loss(model, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 1 == 0 and step % 100 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {step:03d} Loss: {loss.item()} \")\n",
    "            plot_sample(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmTLtFc6X25h"
   },
   "source": [
    "预测图像中有一些噪音，但它仍然可以很好地提取原始服装。\n",
    "\n",
    "现在，当给出纯噪音时，模型会如何表现？它能创建可信的新图像吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3436,
     "status": "ok",
     "timestamp": 1688418563782,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "-Y_m7Ku9vLym",
    "outputId": "c85c9995-809f-4cd2-95fd-5f97948dc1a4"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for _ in range(10):\n",
    "    noise = torch.randn((1, IMG_CH, IMG_SIZE, IMG_SIZE), device=device)\n",
    "    result = model(noise)\n",
    "    nrows = 1\n",
    "    ncols = 2\n",
    "    samples = {\n",
    "        \"Noise\" : noise,\n",
    "        \"Generated Image\" : result\n",
    "    }\n",
    "    for i, (title, img) in enumerate(samples.items()):\n",
    "        ax = plt.subplot(nrows, ncols, i+1)\n",
    "        ax.set_title(title)\n",
    "        show_tensor_image(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 下一步"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "嗯，这些图像看起来更像是墨迹图像而不是衣服。在下一个笔记本中，我们将改进这项技术以创建更易于识别的图像。\n",
    "\n",
    "在继续之前，请通过运行下面的代码单元重新启动 jupyter 内核。这将防止将来的笔记本出现内存问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/DLI_Header.png\" alt=\"标题\" style=\"width: 400px;\"/></center>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
